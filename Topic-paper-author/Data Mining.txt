KDD	Knowledge Discovery in Databases: Papers from the 1994 AAAI Workshop, Seattle, Washington, July 1994. Technical Report WS-94-03	Usama M. Fayyad,Ramasamy Uthurusamy	1994	Knowledge Discovery in Databases: Papers from the 1994 AAAI Workshop, Seattle, Washington, July 1994. Technical Report WS-94-03
KDD	STAR: A General Architecture for the Support of Distortion Oriented Displays.	Paul Anderson,Ray Smith,Zhongwei Zhang	1995	STAR: A General Architecture for the Support of Distortion Oriented Displays.
KDD	Learning First Order Logic Rules with a Genetic Algorithm.	Sébastien Augier,Gilles Venturini,Yves Kodratoff	1995	Learning First Order Logic Rules with a Genetic Algorithm.
KDD	Discovery and Maintenance of Functional Dependencies by Independencies.	Siegfried Bell	1995	Discovery and Maintenance of Functional Dependencies by Independencies.
KDD	Active Data Mining.	Rakesh Agrawal,Giuseppe Psaila	1995	Active Data Mining.
KDD	Capacity and Complexity Control in Predicting the Spread Between Borrowing and Lending Interest Rates.	Corinna Cortes,Harris Drucker,Dennis Hoover,Vladimir Vapnik	1995	Capacity and Complexity Control in Predicting the Spread Between Borrowing and Lending Interest Rates.
KDD	Limits on Learning Machine Accuracy Imposed by Data Quality.	Corinna Cortes,Lawrence D. Jackel,Wan-Ping Chiang	1995	Limits on Learning Machine Accuracy Imposed by Data Quality.
KDD	Intelligent Instruments: Discovering How to Turn Spectral Data into Information.	Wray L. Buntine,Tarang Patel	1995	Intelligent Instruments: Discovering How to Turn Spectral Data into Information.
KDD	Applying a Data Miner To Heterogeneous Schema Integration.	Son Dao,Brad Perry	1995	Applying a Data Miner To Heterogeneous Schema Integration.
KDD	Rough Sets Similarity-Based Learning from Databases.	Xiaohua Hu,Nick Cercone	1995	Rough Sets Similarity-Based Learning from Databases.
KDD	Learning Arbiter and Combiner Trees from Partitioned Data for Scaling Machine Learning.	Philip K. Chan,Salvatore J. Stolfo	1995	Learning Arbiter and Combiner Trees from Partitioned Data for Scaling Machine Learning.
KDD	Exploiting Upper Approximation in the Rough Set Methodology.	Jitender S. Deogun,Vijay V. Raghavan,Hayri Sever	1995	Exploiting Upper Approximation in the Rough Set Methodology.
KDD	Analyzing the Benefits of Domain Knowledge in Substructure Discovery.	Surnjani Djoko,Diane J. Cook,Lawrence B. Holder	1995	Analyzing the Benefits of Domain Knowledge in Substructure Discovery.
KDD	Designing Neural Networks from Statistical Models: A New Approach to Data Exploration.	Antonio Ciampi,Yves Lechevallier	1995	Designing Neural Networks from Statistical Models: A New Approach to Data Exploration.
KDD	Knowledge-Based Scientific Discovery in Geological Databases.	Cen Li,Gautam Biswas	1995	Knowledge-Based Scientific Discovery in Geological Databases.
KDD	Knowledge Discovery in a Water Quality Database.	Saso Dzeroski	1995	Knowledge Discovery in a Water Quality Database.
KDD	Structured and Unstructured Induction with EDAGs.	Brian R. Gaines	1995	Structured and Unstructured Induction with EDAGs.
KDD	A Statistical Perspective On Knowledge Discovery In Databases.	John F. Elder IV,Daryl Pregibon	1995	A Statistical Perspective On Knowledge Discovery In Databases.
KDD	A Database Interface for Clustering in Large Spatial Databases.	Martin Ester,Hans-Peter Kriegel,Xiaowei Xu	1995	A Database Interface for Clustering in Large Spatial Databases.
KDD	Knowledge Discovery in Telecommunication Services Data Using Bayesian Network Models.	Kazuo J. Ezawa,Steven W. Norton	1995	Knowledge Discovery in Telecommunication Services Data Using Bayesian Network Models.
KDD	Available Technology for Discovering Causal Models, Building Bayes Nets, and Selecting Predictors: The TETRAD II Program.	Clark Glymour	1995	Available Technology for Discovering Causal Models, Building Bayes Nets, and Selecting Predictors: The TETRAD II Program.
KDD	Restructuring Databases for Knowledge Discovery by Consolidation and Link Formation.	Henry G. Goldberg,Ted E. Senator	1995	Restructuring Databases for Knowledge Discovery by Consolidation and Link Formation.
KDD	Data Mining for Loan Evaluation at ABN AMRO: A Case Study.	"A. J. Feelders,A. J. F. le Loux,J. W. van't Zand"	1995	Data Mining for Loan Evaluation at ABN AMRO: A Case Study.
KDD	Knowledge Discovery in Textual Databases (KDT).	Ronen Feldman,Ido Dagan	1995	Knowledge Discovery in Textual Databases (KDT).
KDD	Optimization and Simplification of Hierarchical Clusterings.	Douglas Fisher	1995	Optimization and Simplification of Hierarchical Clusterings.
KDD	An Assessment of PrIL.	Özden Gür-Ali,William A. Wallace	1995	An Assessment of PrIL.
KDD	Discriminant Adaptive Nearest Neighbor Classification.	Trevor Hastie,Robert Tibshirani	1995	Nearest neighbor classification expects the class conditional probabilities to be locally constant, and suffers from bias in high dimensions. We propose a locally adaptive form of nearest neighbor classification to try to ameliorate this curse of dimensionality. We use a local linear discriminant analysis to estimate an effective metric for computing neighborhoods. We determine the local decision boundaries from centroid information, and then shrink neighborhoods in directions orthogonal to these local decision boundaries, and elongate them parallel to the boundaries. Thereafter, any neighborhood-based classifier can be employed, using the modified neighborhoods. The posterior probabilities tend to be more homogeneous in the modified neighborhoods. We also propose a method for global dimension reduction, that combines local dimension information. In a number of examples, the methods demonstrate the potential for substantial improvements over nearest neighbor classification.
KDD	A Perspective on Databases and Data Mining.	Marcel Holsheimer,Martin L. Kersten,Heikki Mannila,Hannu Toivonen	1995	We discuss the use of database methods for data mining. Recently impressive results have been achieved for some data mining problems using highly specialized and clever data structures. We study how well one can manage by using general purpose database management systems. We illustrate our ideas by investigating the use of a dbms for a well-researched area: the discovery of association rules. We present a simple algorithm, consisting of only union and intersection operations, and show that it achieves quite good performance on an efficient dbms. Our method can incorporate inheritance hierarchies to the association rule algorithm easily. We also present a technique that effectively reduces the number of database operations when searching large search spaces that contain only few interesting items. Our work shows that database techniques are promising for data mining: general architectures can achieve reasonable results.
KDD	Estimating the Robustness of Discovered Knowledge.	Chun-Nan Hsu,Craig A. Knoblock	1995	Estimating the Robustness of Discovered Knowledge.
KDD	Efficient Algorithms for Attribute-Oriented Induction.	Hoi-Yee Hwang,Ada Wai-Chee Fu	1995	Efficient Algorithms for Attribute-Oriented Induction.
KDD	Robust Decision Trees: Removing Outliers from Databases.	George H. John	1995	Robust Decision Trees: Removing Outliers from Databases.
KDD	Conceptual Clustering in Structured Databases: A Practical Approach.	A. Ketterlin,Pierre Gançarski,Jerzy J. Korczak	1995	Conceptual Clustering in Structured Databases: A Practical Approach.
KDD	Anonymization Techniques for Knowledge Discovery in Databases.	Willi Klösgen	1995	Anonymization Techniques for Knowledge Discovery in Databases.
KDD	Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology.	Ron Kohavi,Dan Sommerfield	1995	Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology.
KDD	Exploiting Visualization in Knowledge Discovery.	Hing-Yan Lee,Hwee-Leng Ong,Lee-Hian Quek	1995	Exploiting Visualization in Knowledge Discovery.
KDD	MDL-Based Decision Tree Pruning.	Manish Mehta,Jorma Rissanen,Rakesh Agrawal	1995	MDL-Based Decision Tree Pruning.
KDD	Discovering Frequent Episodes in Sequences.	Heikki Mannila,Hannu Toivonen,A. Inkeri Verkamo	1995	Discovering Frequent Episodes in Sequences.
KDD	Decision Tree Induction: How Effective is the Greedy Heuristic?	Sreerama K. Murthy,Steven Salzberg	1995	Decision Tree Induction: How Effective is the Greedy Heuristic?
KDD	An Iterative Improvement Approach for the Discretization of Numeric Attributes in Bayesian Classifiers.	Michael J. Pazzani	1995	An Iterative Improvement Approach for the Discretization of Numeric Attributes in Bayesian Classifiers.
KDD	Compression-Based Evaluation of Partial Determinations.	Bernhard Pfahringer,Stefan Kramer	1995	Compression-Based Evaluation of Partial Determinations.
KDD	Fuzzy Interpretation of Induction Results.	Xindong Wu,Petter Måhlén	1995	Fuzzy Interpretation of Induction Results.
KDD	Discovering Enrollment Knowledge in University Databases.	Arun P. Sanjeev,Jan M. Zytkow	1995	Discovering Enrollment Knowledge in University Databases.
KDD	Extracting Support Data for a Given Task.	Bernhard Schölkopf,Chris Burges,Vladimir Vapnik	1995	Extracting Support Data for a Given Task.
KDD	Knowledge Discovery from Multiple Databases.	James S. Ribeiro,Kenneth A. Kaufman,Larry Kerschberg	1995	Knowledge Discovery from Multiple Databases.
KDD	Feature Extraction for Massive Data Mining.	V. Seshadri,Raguram Sasisekharan,Sholom M. Weiss	1995	Feature Extraction for Massive Data Mining.
KDD	Using Rough Sets as Tools for Knowledge Discovery.	Ning Shan,Wojciech Ziarko,Howard J. Hamilton,Nick Cercone	1995	Using Rough Sets as Tools for Knowledge Discovery.
KDD	Data Surveying: Foundations of an Inductive Query Language.	Arno Siebes	1995	Data Surveying: Foundations of an Inductive Query Language.
KDD	On Subjective Measures of Interestingness in Knowledge Discovery.	Abraham Silberschatz,Alexander Tuzhilin	1995	On Subjective Measures of Interestingness in Knowledge Discovery.
KDD	Using Recon for Data Cleaning.	Evangelos Simoudis,Brian Livezey,Randy Kerber	1995	Using Recon for Data Cleaning.
KDD	Discovery of Concurrent Data Models from Experimental Tables: A Rough Set Approach.	Andrzej Skowron,Zbigniew Suraj	1995	Discovery of Concurrent Data Models from Experimental Tables: A Rough Set Approach.
KDD	Learning Bayesian Networks with Discrete Variables from Data.	Peter Spirtes,Christopher Meek	1995	Learning Bayesian Networks with Discrete Variables from Data.
KDD	Fast Spatio-Temporal Data Mining of Large Geophysical Datasets.	Paul E. Stolorz,Hisashi Nakamura,Edmond Mesrobian,Richard R. Muntz,Eddie C. Shek,Jose Renato Santos,J. Yi,Kenneth W. Ng,S.-Y. Chien,Carlos R. Mechoso,John D. Farrara	1995	Fast Spatio-Temporal Data Mining of Large Geophysical Datasets.
KDD	Accelerated Quantification of Bayesian Networks with Incomplete Data.	Bo Thiesson	1995	Accelerated Quantification of Bayesian Networks with Incomplete Data.
KDD	Automated Selection of Rule Induction Methods Based on Recursive Iteration of Resampling Methods and Multiple Statistical Testing.	Shusaku Tsumoto,Hiroshi Tanaka	1995	Automated Selection of Rule Induction Methods Based on Recursive Iteration of Resampling Methods and Multiple Statistical Testing.
KDD	Automated Discovery of Functional Components of Proteins from Amino-Acid Sequences Based on Rough Sets and Change of Representation.	Shusaku Tsumoto,Hiroshi Tanaka	1995	Automated Discovery of Functional Components of Proteins from Amino-Acid Sequences Based on Rough Sets and Change of Representation.
KDD	Toward a Multi-Strategy and Cooperative Discovery System.	Ning Zhong,Setsuo Ohsuga	1995	Toward a Multi-Strategy and Cooperative Discovery System.
KDD	Resource and Knowledge Discovery in Global Information Systems: A Preliminary Design and Experiment.	Osmar R. Zaïane,Jiawei Han	1995	Resource and Knowledge Discovery in Global Information Systems: A Preliminary Design and Experiment.
KDD	Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD-95), Montreal, Canada, August 20-21, 1995	Usama M. Fayyad,Ramasamy Uthurusamy	1995	Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD-95), Montreal, Canada, August 20-21, 1995
KDD	The Quest Data Mining System.	Rakesh Agrawal,Manish Mehta,John C. Shafer,Ramakrishnan Srikant,Andreas Arning,Toni Bollinger	1996	The Quest Data Mining System.
KDD	Developing Tightly-Coupled Data Mining Applications on a Relational Database System.	Rakesh Agrawal,Kyuseok Shim	1996	Developing Tightly-Coupled Data Mining Applications on a Relational Database System.
KDD	A Linear Method for Deviation Detection in Large Databases.	Andreas Arning,Rakesh Agrawal,Prabhakar Raghavan	1996	A Linear Method for Deviation Detection in Large Databases.
KDD	Exploiting Background Knowledge in Automated Discovery.	John M. Aronis,Foster J. Provost,Bruce G. Buchanan	1996	Exploiting Background Knowledge in Automated Discovery.
KDD	Using a Hybrid Neural/Expert System for Data Base Mining in Market Survey Data.	Victor Ciesielski,Gregory Palstra	1996	Using a Hybrid Neural/Expert System for Data Base Mining in Market Survey Data.
KDD	Mining Knowledge in Noisy Audio Data.	Andrzej Czyzewski	1996	Mining Knowledge in Noisy Audio Data.
KDD	Sharing Learned Models among Remote Database Partitions by Local Meta-Learning.	Philip K. Chan,Salvatore J. Stolfo	1996	Sharing Learned Models among Remote Database Partitions by Local Meta-Learning.
KDD	Growing Simpler Decision Trees to Facilitate Knowledge Discovery.	Kevin J. Cherkauer,Jude W. Shavlik	1996	Growing Simpler Decision Trees to Facilitate Knowledge Discovery.
KDD	Maintenance of Discovered Knowledge: A Case in Multi-Level Association Rules.	David Wai-Lok Cheung,Vincent T. Y. Ng,Benjamin W. Tam	1996	Maintenance of Discovered Knowledge: A Case in Multi-Level Association Rules.
KDD	Linear-Time Rule Induction.	Pedro Domingos	1996	Linear-Time Rule Induction.
KDD	Efficient Specific-to-General Rule Induction.	Pedro Domingos	1996	Efficient Specific-to-General Rule Induction.
KDD	Local Induction of Decision Trees: Towards Interactive Data Mining.	Truxton Fulton,Simon Kasif,Steven Salzberg,David L. Waltz	1996	Local Induction of Decision Trees: Towards Interactive Data Mining.
KDD	Mining Entity-Identification Rules for Database Integration.	M. Ganesh,Jaideep Srivastava,Travis Richardson	1996	Mining Entity-Identification Rules for Database Integration.
KDD	Planning Tasks for Knowledge Discovery in Databases; Performing Task-Oriented User-Guidance.	Robert Engels	1996	Planning Tasks for Knowledge Discovery in Databases; Performing Task-Oriented User-Guidance.
KDD	A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.	Martin Ester,Hans-Peter Kriegel,Jörg Sander,Xiaowei Xu	1996	A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.
KDD	Data Mining with Sparse and Simplified Interaction Selection.	Gerald Fahner	1996	Data Mining with Sparse and Simplified Interaction Selection.
KDD	Combining Data Mining and Machine Learning for Effective User Profiling.	Tom Fawcett,Foster J. Provost	1996	Combining Data Mining and Machine Learning for Effective User Profiling.
KDD	KDD for Science Data Analysis: Issues and Examples.	Usama M. Fayyad,David Haussler,Paul E. Stolorz	1996	KDD for Science Data Analysis: Issues and Examples.
KDD	Knowledge Discovery and Data Mining: Towards a Unifying Framework.	Usama M. Fayyad,Gregory Piatetsky-Shapiro,Padhraic Smyth	1996	Knowledge Discovery and Data Mining: Towards a Unifying Framework.
KDD	Learning from Biased Data Using Mixture Models.	A. J. Feelders	1996	Learning from Biased Data Using Mixture Models.
KDD	Harnessing the Human in Knowledge Discovery.	Georges G. Grinstein	1996	Harnessing the Human in Knowledge Discovery.
KDD	Data Mining and Tree-Based Optimization.	Robert L. Grossman,Haim Bodek,Dave Northcutt,Vince Poor	1996	Data Mining and Tree-Based Optimization.
KDD	Mining Associations in Text in the Presence of Background Knowledge.	Ronen Feldman,Haym Hirsh	1996	Mining Associations in Text in the Presence of Background Knowledge.
KDD	A Genetic Algorithm-Based Approach to Data Mining.	Ian W. Flockhart,Nicholas J. Radcliffe	1996	A Genetic Algorithm-Based Approach to Data Mining.
KDD	DBMiner: A System for Mining Knowledge in Large Relational Databases.	Jiawei Han,Yongjian Fu,Wei Wang,Jenny Chiang,Wan Gong,Krzysztof Koperski,Deyi Li,Yijun Lu,Amynmohamed Rajan,Nebojsa Stefanovic,Betty Xia,Osmar R. Zaïane	1996	DBMiner: A System for Mining Knowledge in Large Relational Databases.
KDD	Knowledge Discovery in RNA Sequence Families of HIV Using Scalable Computers.	Ivo L. Hofacker,Martijn A. Huynen,Peter F. Stadler,Paul E. Stolorz	1996	Knowledge Discovery in RNA Sequence Families of HIV Using Scalable Computers.
KDD	Inferring Hierarchical Clustering Structures by Deterministic Annealing.	Thomas Hofmann,Joachim M. Buhmann	1996	Inferring Hierarchical Clustering Structures by Deterministic Annealing.
KDD	Discovering Knowledge in Commercial Databases Using Modern Heuristic Techniques.	Beatriz de la Iglesia,Justin C. W. Debuse,Victor J. Rayward-Smith	1996	Discovering Knowledge in Commercial Databases Using Modern Heuristic Techniques.
KDD	DataMine: Application Programming Interface and Query Language for Database Mining.	Tomasz Imielinski,Aashu Virmani,Amin Abdulghani	1996	DataMine: Application Programming Interface and Query Language for Database Mining.
KDD	Discovery of Relevant New Features by Generating Non-Linear Decision Trees.	Andreas Ittner,Michael Schlosser	1996	Discovery of Relevant New Features by Generating Non-Linear Decision Trees.
KDD	Static Versus Dynamic Sampling for Data Mining.	George H. John,Pat Langley	1996	Static Versus Dynamic Sampling for Data Mining.
KDD	Evaluating the Interestingness of Characteristic Rules.	Micheline Kamber,Rajjan Shinghal	1996	Evaluating the Interestingness of Characteristic Rules.
KDD	A Method for Reasoning with Structured and Continuous Attributes in the INLEN-2 Multistrategy Knowledge Discovery System.	Kenneth A. Kaufman,Ryszard S. Michalski	1996	A Method for Reasoning with Structured and Continuous Attributes in the INLEN-2 Multistrategy Knowledge Discovery System.
KDD	Analysing Binary Associations.	Arno J. Knobbe,Pieter W. Adriaans	1996	Analysing Binary Associations.
KDD	Extraction of Spatial Proximity Patterns by Concept Generalization.	Edwin M. Knorr,Raymond T. Ng	1996	Extraction of Spatial Proximity Patterns by Concept Generalization.
KDD	Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid.	Ron Kohavi	1996	Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid.
KDD	Error-Based and Entropy-Based Discretization of Continuous Features.	Ron Kohavi,Mehran Sahami	1996	Error-Based and Entropy-Based Discretization of Continuous Features.
KDD	Predictive Data Mining with Finite Mixtures.	Petri Kontkanen,Petri Myllymäki,Henry Tirri	1996	Predictive Data Mining with Finite Mixtures.
KDD	Efficient Search for Strong Partial Determinations.	Stefan Kramer,Bernhard Pfahringer	1996	Efficient Search for Strong Partial Determinations.
KDD	Self-Organizing Maps of Document Collections: A New Approach to Interactive Exploration.	Krista Lagus,Timo Honkela,Samuel Kaski,Teuvo Kohonen	1996	Self-Organizing Maps of Document Collections: A New Approach to Interactive Exploration.
KDD	Imputation of Missing Data Using Machine Learning Techniques.	Kamakshi Lakshminarayan,Steven A. Harp,Robert P. Goldman,Tariq Samad	1996	Imputation of Missing Data Using Machine Learning Techniques.
KDD	An Empirical Test of the Weighted Effect Approach to Generalized Prediction Using Recursive Neural Nets.	Rense Lange	1996	An Empirical Test of the Weighted Effect Approach to Generalized Prediction Using Recursive Neural Nets.
KDD	Induction of Condensed Determinations.	Pat Langley	1996	Induction of Condensed Determinations.
KDD	An Overview of Issues in Developing Industrial Data Mining and Knowledge Discovery Applications.	Gregory Piatetsky-Shapiro,Ronald J. Brachman,Tom Khabaza,Willi Klösgen,Evangelos Simoudis	1996	An Overview of Issues in Developing Industrial Data Mining and Knowledge Discovery Applications.
KDD	Reverse Engineering Databases for Knowledge Discovery.	Stephen McKearney,Huw Roberts	1996	Reverse Engineering Databases for Knowledge Discovery.
KDD	Discovering Generalized Episodes Using Minimal Occurrences.	Heikki Mannila,Hannu Toivonen	1996	Discovering Generalized Episodes Using Minimal Occurrences.
KDD	Multiple Uses of Frequent Sets and Condensed Representations (Extended Abstract).	Heikki Mannila,Hannu Toivonen	1996	Multiple Uses of Frequent Sets and Condensed Representations (Extended Abstract).
KDD	A Comparison of Approaches for Maximizing Business Payoff of Prediction Models.	Brij M. Masand,Gregory Piatetsky-Shapiro	1996	A Comparison of Approaches for Maximizing Business Payoff of Prediction Models.
KDD	The Field Matching Problem: Algorithms and Applications.	Alvaro E. Monge,Charles Elkan	1996	The Field Matching Problem: Algorithms and Applications.
KDD	Rethinking the Learning of Belief Network Probabilities.	Ron Musick	1996	Rethinking the Learning of Belief Network Probabilities.
KDD	Pattern Discovery in Temporal Databases: A Temporal Logic Approach.	Balaji Padmanabhan,Alexander Tuzhilin	1996	Pattern Discovery in Temporal Databases: A Temporal Logic Approach.
KDD	Parallel Halo Finding in N-Body Cosmology Simulations.	David W. Pfitzner,John K. Salmon	1996	Parallel Halo Finding in N-Body Cosmology Simulations.
KDD	Performing Effective Feature Selection by Investigating the Deep Structure of the Data.	Marco Richeldi,Pier Luca Lanzi	1996	Performing Effective Feature Selection by Investigating the Deep Structure of the Data.
KDD	Data Mining and Model Simplicity: A Case Study in Diagnosis.	Gregory M. Provan,Moninder Singh	1996	Data Mining and Model Simplicity: A Case Study in Diagnosis.
KDD	SE-Trees Outperform Decision Trees in Noisy Domains.	Ron Rymon	1996	SE-Trees Outperform Decision Trees in Noisy Domains.
KDD	Deriving Queries from Results Using Genetic Programming.	Tae-Wan Ryu,Christoph F. Eick	1996	Deriving Queries from Results Using Genetic Programming.
KDD	Learning Limited Dependence Bayesian Classifiers.	Mehran Sahami	1996	Learning Limited Dependence Bayesian Classifiers.
KDD	Discovering Classification Knowledge in Databases Using Rough Sets.	Ning Shan,Wojciech Ziarko,Howard J. Hamilton,Nick Cercone	1996	Discovering Classification Knowledge in Databases Using Rough Sets.
KDD	Scalable Exploratory Data Mining of Distributed Geoscientific Data.	Eddie C. Shek,Richard R. Muntz,Edmond Mesrobian,Kenneth W. Ng	1996	Scalable Exploratory Data Mining of Distributed Geoscientific Data.
KDD	Metapattern Generation for Integrated Data Mining.	Wei-Min Shen,Bing Leng	1996	Metapattern Generation for Integrated Data Mining.
KDD	Clustering Using Monte Carlo Cross-Validation.	Padhraic Smyth	1996	Clustering Using Monte Carlo Cross-Validation.
KDD	Harnessing Graphical Structure in Markov Chain Monte Carlo Learning.	Paul E. Stolorz,Philip C. Chew	1996	Harnessing Graphical Structure in Markov Chain Monte Carlo Learning.
KDD	Quakefinder: A Scalable Data Mining System for Detecting Earthquakes from Space.	Paul E. Stolorz,Christopher Dean	1996	Quakefinder: A Scalable Data Mining System for Detecting Earthquakes from Space.
KDD	Exceptional Knowledge Discovery in Databases Based on Information Theory.	Einoshin Suzuki,Masamichi Shimura	1996	Exceptional Knowledge Discovery in Databases Based on Information Theory.
KDD	Undiscovered Public Knowledge: A Ten-Year Update.	Don R. Swanson,Neil R. Smalheiser	1996	Undiscovered Public Knowledge: A Ten-Year Update.
KDD	Interactive Knowledge Discovery from Marketing Questionnaire Using Simulated Breeding and Inductive Learning Methods.	Takao Terano,Yoko Ishino	1996	Interactive Knowledge Discovery from Marketing Questionnaire Using Simulated Breeding and Inductive Learning Methods.
KDD	Automated Discovery of Medical Expert System Rules from Clinical Databases Based on Rough Sets.	Shusaku Tsumoto,Hiroshi Tanaka	1996	Automated Discovery of Medical Expert System Rules from Clinical Databases Based on Rough Sets.
KDD	Efficient Implementation of Data Cubes Via Materialized Views.	Jeffrey D. Ullman	1996	Efficient Implementation of Data Cubes Via Materialized Views.
KDD	RITIO - Rule Induction Two In One.	David Urpani,Xindong Wu,Jim Sykes	1996	RITIO - Rule Induction Two In One.
KDD	Automated Discovery of Active Motifs in Multiple RNA Secondary Structures.	Jason Tsong-Li Wang,Bruce A. Shapiro,Dennis Shasha,Kaizhong Zhang,Chia-Yo Chang	1996	Automated Discovery of Active Motifs in Multiple RNA Secondary Structures.
KDD	Representing Discovered Patterns Using Attributed Hypergraph.	Yang Wang,Andrew K. C. Wong	1996	Representing Discovered Patterns Using Attributed Hypergraph.
KDD	Detecting Early Indicator Cars in an Automotive Database: A Multi-Strategy Approach.	Rüdiger Wirth,Thomas P. Reinartz	1996	Detecting Early Indicator Cars in an Automotive Database: A Multi-Strategy Approach.
KDD	Extensibility in Data Mining Systems.	Stefan Wrobel,Dietrich Wettschereck,Edgar Sommer,Werner Emde	1996	Extensibility in Data Mining Systems.
KDD	Automated Pattern Mining with a Scale Dimension.	Jan M. Zytkow,Robert Zembowicz	1996	Automated Pattern Mining with a Scale Dimension.
KDD	Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), Portland, Oregon, USA	Evangelos Simoudis,Jiawei Han,Usama M. Fayyad	1996	Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), Portland, Oregon, USA
KDD	Discovery of Actionable Patterns in Databases: The Action Hierarchy Approach.	Gediminas Adomavicius,Alexander Tuzhilin	1997	Discovery of Actionable Patterns in Databases: The Action Hierarchy Approach.
KDD	Brute-Force Mining of High-Confidence Classification Rules.	Roberto J. Bayardo Jr.	1997	Brute-Force Mining of High-Confidence Classification Rules.
KDD	Applying Data Mining and Machine Learning Techniques to Submarine Intelligence Analysis.	Ulla Bergsten,Johan Schubert,Per Svensson	1997	Applying Data Mining and Machine Learning Techniques to Submarine Intelligence Analysis.
KDD	Partial Classification Using Association Rules.	Kamal Ali,Stefanos Manganaris,Ramakrishnan Srikant	1997	Partial Classification Using Association Rules.
KDD	Increasing the Efficiency of Data Mining Algorithms with Breadth-First Marker Propagation.	John M. Aronis,Foster J. Provost	1997	Increasing the Efficiency of Data Mining Algorithms with Breadth-First Marker Propagation.
KDD	Process-Based Database Support for the Early Indicator Method.	Christoph Breitner,Jörg Schlösser,Rüdiger Wirth	1997	Process-Based Database Support for the Early Indicator Method.
KDD	MineSet: An Integrated System for Data Mining.	Clifford Brunk,James Kelly,Ron Kohavi	1997	MineSet: An Integrated System for Data Mining.
KDD	Zeta: A Global Method for Discretization of Continuous Variables.	K. M. Ho,Paul D. Scott	1997	Zeta: A Global Method for Discretization of Continuous Variables.
KDD	Proposal and Empirical Comparison of a Parallelizable Distance-Based Discretization Method.	Jesús Cerquides,Ramon López de Mántaras	1997	Proposal and Empirical Comparison of a Parallelizable Distance-Based Discretization Method.
KDD	Mining Multivariate Time-Series Sensor Data to Discover Behavior Envelopes.	Dennis DeCoste	1997	Mining Multivariate Time-Series Sensor Data to Discover Behavior Envelopes.
KDD	Large Scale Data Mining: Challenges and Responses.	Jaturon Chattratichat,John Darlington,Moustafa Ghanem,Yike Guo,Harald Hüning,Martin Köhler,Janjao Sutiwaraphun,Hing Wing To,Dan Yang	1997	Large Scale Data Mining: Challenges and Responses.
KDD	An Interactive Visualization Environment for Data Exploration.	Mark Derthick,John Kolojejchick,Steven F. Roth	1997	An Interactive Visualization Environment for Data Exploration.
KDD	Why Does Bagging Work? A Bayesian Account and its Implications.	Pedro Domingos	1997	Why Does Bagging Work? A Bayesian Account and its Implications.
KDD	Using Artificial Intelligence Planning to Automate Science Data Analysis for Large Image Databases.	Steve A. Chien,Forest Fisher,Helen Mortensen,Edisanter Lo,Ronald Greeley	1997	Using Artificial Intelligence Planning to Automate Science Data Analysis for Large Image Databases.
KDD	Fast Committee Machines for Regression and Classification.	Harris Drucker	1997	Fast Committee Machines for Regression and Classification.
KDD	Improving Scalability in a Scientific Discovery System by Exploiting Parallelism.	Gehad Galal,Diane J. Cook,Lawrence B. Holder	1997	Improving Scalability in a Scientific Discovery System by Exploiting Parallelism.
KDD	A Guided Tour through the Data Mining Jungle.	Robert Engels,Guido Lindner,Rudi Studer	1997	A Guided Tour through the Data Mining Jungle.
KDD	Density-Connected Sets and their Application for Trend Detection in Spatial Databases.	Martin Ester,Hans-Peter Kriegel,Jörg Sander,Xiaowei Xu	1997	Density-Connected Sets and their Application for Trend Detection in Spatial Databases.
KDD	Maximal Association Rules: A New Tool for Mining for Keyword Co-Occurrences in Document Collections.	Ronen Feldman,Yonatan Aumann,Amihood Amir,Amir Zilberstein,Willi Klösgen	1997	Maximal Association Rules: A New Tool for Mining for Keyword Co-Occurrences in Document Collections.
KDD	Visualization Techniques to Explore Data Mining Results for Document Collections.	Ronen Feldman,Willi Klösgen,Amir Zilberstein	1997	Visualization Techniques to Explore Data Mining Results for Document Collections.
KDD	Deep Knowledge Discovery from Natural Language Texts.	Udo Hahn,Klemens Schnattinger	1997	Deep Knowledge Discovery from Natural Language Texts.
KDD	Integrating and Mining Distributed Customer Databases.	Ira J. Haimowitz,Özden Gür-Ali,Henry Schwarz	1997	Integrating and Mining Distributed Customer Databases.
KDD	GA-Based Rule Enhancement in Concept Learning.	Jukka Hekanaho	1997	"We describe an application of DOGMA, a GA-based theory revision system, to MDL-based rule enhancement in supervised concept learning. The system takes as input classification data and a rule-based classification theory, produced by some rule-based learner, and builds a second, hopefully more accurate, model of the data. Unlike most theory revision systems DOGMA doesn''t revise the initial rules, but builds instead a completely new theory, using stochastic sampling and adaptation of the initial rules. The search for the new model is guided by a MDL-based complexity measure. The proposed methodology offers a partial solution both to the local mimima trap of fast greedy rule-based concept learners, and to the time complexity problem of GA-based concept learners. As an example we show how the system improves rules produced by C4.5Rules."
KDD	Target-Independent Mining for Scientific Data: Capturing Transients and Trends for Phenomena Mining.	Thomas H. Hinke,John A. Rushing,Heggere S. Ranganath,Sara J. Graves	1997	Target-Independent Mining for Scientific Data: Capturing Transients and Trends for Phenomena Mining.
KDD	"From Large to Huge: A Statistician's Reactions to KDD & DM."	Peter J. Huber	1997	"From Large to Huge: A Statistician's Reactions to KDD & DM."
KDD	Adjusting for Multiple Comparisons in Decision Tree Pruning.	David Jensen,Matthew D. Schmill	1997	Adjusting for Multiple Comparisons in Decision Tree Pruning.
KDD	SIPping from the Data Firehose.	George H. John,Brian Lent	1997	SIPping from the Data Firehose.
KDD	Mining Generalized Term Associations: Count Propagation Algorithm.	Jonghyun Kahng,Wen-Hsiang Kevin Liao,Dennis McLeod	1997	Mining Generalized Term Associations: Count Propagation Algorithm.
KDD	Metarule-Guided Mining of Multi-Dimensional Association Rules Using Data Cubes.	Micheline Kamber,Jiawei Han,Jenny Chiang	1997	Metarule-Guided Mining of Multi-Dimensional Association Rules Using Data Cubes.
KDD	Scalable, Distributed Data Mining - An Agent Architecture.	Hillol Kargupta,Ilker Hamzaoglu,Brian Stafford	1997	Scalable, Distributed Data Mining - An Agent Architecture.
KDD	A Probabilistic Approach to Fast Pattern Matching in Time Series Databases.	Eamonn J. Keogh,Padhraic Smyth	1997	A Probabilistic Approach to Fast Pattern Matching in Time Series Databases.
KDD	Clustering Sequences of Complex Objects.	A. Ketterlin	1997	Clustering Sequences of Complex Objects.
KDD	A Unified Notion of Outliers: Properties and Computation.	Edwin M. Knorr,Raymond T. Ng	1997	A Unified Notion of Outliers: Properties and Computation.
KDD	Mining for Causes of Cancer: Machine Learning Experiments at Various Levels of Detail.	Stefan Kramer,Bernhard Pfahringer,Christoph Helma	1997	Mining for Causes of Cancer: Machine Learning Experiments at Various Levels of Detail.
KDD	Discovering Trends in Text Databases.	Brian Lent,Rakesh Agrawal,Ramakrishnan Srikant	1997	Discovering Trends in Text Databases.
KDD	Using General Impressions to Analyze Discovered Classification Rules.	Bing Liu,Wynne Hsu,Shu Chen	1997	Using General Impressions to Analyze Discovered Classification Rules.
KDD	Fast Robust Visual Data Mining.	Ted Mihalisin,John Timlin	1997	Fast Robust Visual Data Mining.
KDD	Development of Multi-Criteria Metrics for Evaluation of Data Mining Algorithms.	Gholamreza Nakhaeizadeh,Alexander Schnabl	1997	Development of Multi-Criteria Metrics for Evaluation of Data Mining Algorithms.
KDD	Beyond Concise and Colorful: Learning Intelligible Rules.	Michael J. Pazzani,Subramani Mani,William Rodman Shankle	1997	Beyond Concise and Colorful: Learning Intelligible Rules.
KDD	Discriminative vs Informative Learning.	Y. Dan Rubinstein,Trevor Hastie	1997	Discriminative vs Informative Learning.
KDD	Analysis and Visualization of Classifier Performance: Comparison under Imprecise Class and Cost Distributions.	Foster J. Provost,Tom Fawcett	1997	Analysis and Visualization of Classifier Performance: Comparison under Imprecise Class and Cost Distributions.
KDD	Scaling Up Inductive Algorithms: An Overview.	Foster J. Provost,Venkateswarlu Kolluri	1997	Scaling Up Inductive Algorithms: An Overview.
KDD	Knowledge Discovery in Integrated Call Centers: A Framework for Effective Customer-Driven Marketing.	Paul Xia	1997	Knowledge Discovery in Integrated Call Centers: A Framework for Effective Customer-Driven Marketing.
KDD	Visualizing Bagged Decision Trees.	J. Sunil Rao,William J. E. Potts	1997	Visualizing Bagged Decision Trees.
KDD	KESO: Minimizing Database Interaction.	Arno Siebes,Martin L. Kersten	1997	KESO: Minimizing Database Interaction.
KDD	Detecting Atmospheric Regimes Using Cross-Validated Clustering.	Padhraic Smyth,Michael Ghil,Kayo Ide,Joseph Roden,Andrew Fraser	1997	Detecting Atmospheric Regimes Using Cross-Validated Clustering.
KDD	Anytime Exploratory Data Analysis for Massive Data Sets.	Padhraic Smyth,David Wolpert	1997	Anytime Exploratory Data Analysis for Massive Data Sets.
KDD	Learning to Extract Text-Based Information from the World Wide Web.	Stephen Soderland	1997	Learning to Extract Text-Based Information from the World Wide Web.
KDD	Mining Association Rules with Item Constraints.	Ramakrishnan Srikant,Quoc Vu,Rakesh Agrawal	1997	Mining Association Rules with Item Constraints.
KDD	JAM: Java Agents for Meta-Learning over Distributed Databases.	Salvatore J. Stolfo,Andreas L. Prodromidis,Shelley Tselepis,Wenke Lee,Dave W. Fan,Philip K. Chan	1997	JAM: Java Agents for Meta-Learning over Distributed Databases.
KDD	Image Feature Reduction through Spoiling: Its Application to Multiple Matched Filters for Focus of Attention.	Timothy M. Stough,Carla E. Brodley	1997	Image Feature Reduction through Spoiling: Its Application to Multiple Matched Filters for Focus of Attention.
KDD	A Visual Interactive Framework for Attribute Discretization.	Ramesh Subramonian,Ramana Venkata,Joyce Chen	1997	A Visual Interactive Framework for Attribute Discretization.
KDD	Autonomous Discovery of Reliable Exception Rules.	Einoshin Suzuki	1997	Autonomous Discovery of Reliable Exception Rules.
KDD	An Efficient Algorithm for the Incremental Updation of Association Rules in Large Databases.	Shiby Thomas,Sreenath Bodagala,Khaled Alsabti,Sanjay Ranka	1997	An Efficient Algorithm for the Incremental Updation of Association Rules in Large Databases.
KDD	Bayesian Inference for Identifying Solar Active Regions.	Michael J. Turmon,Saleem Mukhtar,Judit Pap	1997	Bayesian Inference for Identifying Solar Active Regions.
KDD	Schema Discovery for Semistructured Data.	Ke Wang,Huiqing Liu	1997	Schema Discovery for Semistructured Data.
KDD	Selecting Features by Vertical Compactness of Data.	Ke Wang,Suman Sundaresh	1997	Selecting Features by Vertical Compactness of Data.
KDD	Automated Discovery of Active Motifs in Three Dimensional Molecules.	Xiong Wang,Jason Tsong-Li Wang,Dennis Shasha,Bruce A. Shapiro,Sitaram Dikshitulu,Isidore Rigoutsos,Kaizhong Zhang	1997	Automated Discovery of Active Motifs in Three Dimensional Molecules.
KDD	Fast and Intuitive Clustering of Web Documents.	Oren Zamir,Oren Etzioni,Omid Madani,Richard M. Karp	1997	Fast and Intuitive Clustering of Web Documents.
KDD	KDD Process Planning.	Ning Zhong,Chunnian Liu,Yoshitsugu Kakemoto,Setsuo Ohsuga	1997	KDD Process Planning.
KDD	Optimal Multiple Intervals Discretization of Continuous Attributes for Supervised Learning.	Djamel A. Zighed,Ricco Rakotomalala,Fabien Feschet	1997	Optimal Multiple Intervals Discretization of Continuous Attributes for Supervised Learning.
KDD	A Dataset Decomposition Approach to Data Mining and Machine Discovery.	Blaz Zupan,Marko Bohanec,Ivan Bratko,Bojan Cestnik	1997	A Dataset Decomposition Approach to Data Mining and Machine Discovery.
KDD	Knowledge = Concepts: A Harmful Equation.	Jan M. Zytkow	1997	Knowledge = Concepts: A Harmful Equation.
KDD	Computing Optimized Rectilinear Regions for Association Rules.	Kunikazu Yoda,Takeshi Fukuda,Yasuhiko Morimoto,Shinichi Morishita,Takeshi Tokuyama	1997	Computing Optimized Rectilinear Regions for Association Rules.
KDD	New Algorithms for Fast Discovery of Association Rules.	Mohammed Javeed Zaki,Srinivasan Parthasarathy,Mitsunori Ogihara,Wei Li	1997	Association rule discovery has emerged as an important problem in knowledge discovery and data mining. The association mining task consists of identifying the frequent itemsets, and then forming conditional implication rules among them. In this paper we present efficient algorithms for the discovery of frequent itemsets, which forms the compute intensive phase of the task. The algorithms utilize the structural properties of frequent itemsets to facilitate fast discovery. The related database items are grouped together into clusters representing the potential maximal frequent itemsets in the database. Each cluster induces a sub-lattice of the itemset lattice. Efficient lattice traversal techniques are presented, which quickly identify all the true maximal frequent itemsets, and all their subsets if desired. We also present the effect of using different database layout schemes combined with the proposed clustering and traversal techniques. The proposed algorithms scan a (pre-processed) database only once, addressing the open question in association mining, whether all the rules can be efficiently extracted in a single database pass. We experimentally compare the new algorithms against the previous approaches, obtaining improvements of more than an order of magnitude for our test databases.
KDD	Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (KDD-97), Newport Beach, California, USA, August 14-17, 1997	David Heckerman,Heikki Mannila,Daryl Pregibon	1997	Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (KDD-97), Newport Beach, California, USA, August 14-17, 1997
PKDD	Discovery of Health Risks and Case-Based Forecasting of Epidemics in a Health Surveillance System.	Mathias Bull,Günther Kundt,Lothar Gierl	1997	Discovery of Health Risks and Case-Based Forecasting of Epidemics in a Health Surveillance System.
PKDD	Data Mining in the Telecommunications Industry (Abstract).	Leo Carbonara,Huw Roberts,Blaise Egan	1997	Data Mining in the Telecommunications Industry (Abstract).
PKDD	Mining in the Phrasal Frontier.	Helena Ahonen,Oskari Heinonen,Mika Klemettinen,A. Inkeri Verkamo	1997	Mining in the Phrasal Frontier.
PKDD	Share Based Measures for Itemsets.	Colin L. Carter,Howard J. Hamilton,Nick Cercone	1997	Share Based Measures for Itemsets.
PKDD	A New and Versatile Method for Association Generation.	Amihood Amir,Ronen Feldman,Reuven Kashi	1997	A New and Versatile Method for Association Generation.
PKDD	Using Signature Files for Querying Time-Series Data.	Henrik André-Jönsson,Dushan Z. Badal	1997	Using Signature Files for Querying Time-Series Data.
PKDD	Interactive Interpretation of Hierarchical Clustering.	Eric Boudaillier,Georges Hébrail	1997	Interactive Interpretation of Hierarchical Clustering.
PKDD	Recognizing Reliability of Discovered Knowledge.	Petr Berka	1997	Recognizing Reliability of Discovered Knowledge.
PKDD	Bivariate Decision Trees.	Jan C. Bioch,Onno van der Meer,Rob Potharst	1997	Bivariate Decision Trees.
PKDD	Mining Time Series Using Rough Sets - A Case Study.	Anders Torvill Bjorvand	1997	Mining Time Series Using Rough Sets - A Case Study.
PKDD	Finding Similar Time Series.	Gautam Das,Dimitrios Gunopulos,Heikki Mannila	1997	Finding Similar Time Series.
PKDD	Efficient Multisplitting on Numerical Data.	Tapio Elomaa,Juho Rousu	1997	Efficient Multisplitting on Numerical Data.
PKDD	Pattern Based Browsing in Document Collections.	Ronen Feldman,Willi Klösgen,Yaniv Ben-Yehuda,Gil Kedar,Vladimir Reznikov	1997	Pattern Based Browsing in Document Collections.
PKDD	Knowledge Discovery - A Control Theory Perspective.	Bjarne A. Foss	1997	Knowledge Discovery - A Control Theory Perspective.
PKDD	The Pronciple of Transformation between Efficiency and Effectiveness: Towards a Fair Evaluation of the Cost-Effectiveness of KDD Techniques.	Alex Alves Freitas	1997	The Pronciple of Transformation between Efficiency and Effectiveness: Towards a Fair Evaluation of the Cost-Effectiveness of KDD Techniques.
PKDD	Parallel Knowledge Discovery Using Domain Generalization Graphs.	Robert J. Hilderman,Howard J. Hamilton,Robert J. Kowalchuk,Nick Cercone	1997	Parallel Knowledge Discovery Using Domain Generalization Graphs.
PKDD	Finding Spatial Clusters.	Friedrich Gebhardt	1997	Finding Spatial Clusters.
PKDD	A Tutorial Introduction to High Performance Data Mining (Abstract).	Robert L. Grossman	1997	A Tutorial Introduction to High Performance Data Mining (Abstract).
PKDD	Regression-Based Classification Methods and Their Comparisons with Desision Tree Algorithms.	Mikhail V. Kiselev,Sergei M. Ananyan,Sergei B. Arseniev	1997	Regression-Based Classification Methods and Their Comparisons with Desision Tree Algorithms.
PKDD	techniques and Applications of KDD (Abstract).	Willi Klösgen,Jan M. Zytkow	1997	techniques and Applications of KDD (Abstract).
PKDD	Exploration of Document Collections with Self-Organizing Maps: A Novel Approach to Similarity Representation.	Dieter Merkl	1997	Exploration of Document Collections with Self-Organizing Maps: A Novel Approach to Similarity Representation.
PKDD	Rough Sets for Data Mining and Knowledge Discovery (Abstract).	Henryk Jan Komorowski,Lech Polkowski,Andrzej Skowron	1997	Rough Sets for Data Mining and Knowledge Discovery (Abstract).
PKDD	Modelling Customer Retention with Rough Data Models.	Wojciech Kowalczyk,Frank Slisser	1997	Modelling Customer Retention with Rough Data Models.
PKDD	Algorithms for Constructing of Decision Trees.	Mikhail Moshkov	1997	Algorithms for Constructing of Decision Trees.
PKDD	Clustering Techniques in Biological Sequence Analysis.	Anna M. Manning,Andy Brass,Carole A. Goble,John A. Keane	1997	Clustering Techniques in Biological Sequence Analysis.
PKDD	Generation of Rules from Incomplete Information Systems.	Marzena Kryszkiewicz	1997	Generation of Rules from Incomplete Information Systems.
PKDD	Searching for Relational Patterns in Data.	Sinh Hoa Nguyen,Andrzej Skowron	1997	Searching for Relational Patterns in Data.
PKDD	Rough Set Theory and Rule Induction Techniques for Discovery of Attribute Dependencies in Medical Information Systems.	Jerzy Stefanowski,Krzysztof Slowinski	1997	Rough Set Theory and Rule Induction Techniques for Discovery of Attribute Dependencies in Medical Information Systems.
PKDD	Induction of Strong Feature Subsets.	Mohamed Quafafou,Moussa Boussouf	1997	Induction of Strong Feature Subsets.
PKDD	Induction of Fuzzy Characteristic Rules.	Dan Rasmussen,Ronald R. Yager	1997	Induction of Fuzzy Characteristic Rules.
PKDD	Logical Calculi for Knowledge Discovery in Databases.	Jan Rauch	1997	Logical Calculi for Knowledge Discovery in Databases.
PKDD	Knowledge Discovery from Software Engineering Data: Rough Set Analysis and Its Interaction with Goal-Oriented Measurement.	Günther Ruhe	1997	Knowledge Discovery from Software Engineering Data: Rough Set Analysis and Its Interaction with Goal-Oriented Measurement.
PKDD	"Extraction of Experts' Decision Process from Clinical Databases Using Rough Set Model."	Shusaku Tsumoto	1997	"Extraction of Experts' Decision Process from Clinical Databases Using Rough Set Model."
PKDD	A Connectionist Approach to Structural Simiarity Determination as a Basis of Clustering, Classification and Feature Detection.	Kristina Schädler,Fritz Wysotzki	1997	A Connectionist Approach to Structural Simiarity Determination as a Basis of Clustering, Classification and Feature Detection.
PKDD	Neural Networks Design: Rough Set Approach to Continuous Data.	Hung Son Nguyen,Marcin S. Szczuka,Dominik Slezak	1997	Neural Networks Design: Rough Set Approach to Continuous Data.
PKDD	SNOUT: An Intelligent Assistant for Exploratory Data Anaylsis.	Paul D. Scott,A. P. M. Coxon,M. H. Hobbs,R. J. Williams	1997	SNOUT: An Intelligent Assistant for Exploratory Data Anaylsis.
PKDD	Exploratory Analysis of Biochemical Processes Using Hybrid Modeling Methods.	Rimvydas Simutis	1997	Exploratory Analysis of Biochemical Processes Using Hybrid Modeling Methods.
PKDD	TOAS Intelligence Mining; Analysis of Natural Language Processing and Computational Linguistics.	Robert J. Watts,Alan L. Porter,Scott Cunningham,Donghua Zhu	1997	TOAS Intelligence Mining; Analysis of Natural Language Processing and Computational Linguistics.
PKDD	Attribute Discovery and Rough Sets.	Jaroslaw Stepaniuk	1997	Attribute Discovery and Rough Sets.
PKDD	Towards Process-Oriented Tool Support for Knowledge Discovery in Databases.	Rüdiger Wirth,Colin Shearer,Udo Grimmer,Thomas P. Reinartz,Jörg Schlösser,Christoph Breitner,Robert Engels,Guido Lindner	1997	Towards Process-Oriented Tool Support for Knowledge Discovery in Databases.
PKDD	An Algorithm for Multi-relational Discovery of Subgroups.	Stefan Wrobel	1997	An Algorithm for Multi-relational Discovery of Subgroups.
PKDD	On Meta Levels of an Organized Society of KDD Agents.	Ning Zhong,Setsuo Ohsuga,Chunnian Liu,Yoshitsugu Kakemoto,Xiaosong Zhang	1997	On Meta Levels of an Organized Society of KDD Agents.
PKDD	Using Neural Network to Extract Knowledge from Database.	Yuanhui Zhou,Yuchang Lu,Chunyi Shi	1997	Using Neural Network to Extract Knowledge from Database.
PKDD	"Principles of Data Mining and Knowledge Discovery, First European Symposium, PKDD '97, Trondheim, Norway, June 24-27, 1997, Proceedings"	Henryk Jan Komorowski,Jan M. Zytkow	1997	"Principles of Data Mining and Knowledge Discovery, First European Symposium, PKDD '97, Trondheim, Norway, June 24-27, 1997, Proceedings"
KDD	ADtrees for Fast Counting and for Fast Learning of Association Rules.	Brigham S. Anderson,Andrew W. Moore	1998	ADtrees for Fast Counting and for Fast Learning of Association Rules.
KDD	Mining Association Rules in Hypertext Databases.	José Borges,Mark Levene	1998	Mining Association Rules in Hypertext Databases.
KDD	Online Generation of Profile Association Rules.	Charu C. Aggarwal,Zheng Sun,Philip S. Yu	1998	Online Generation of Profile Association Rules.
KDD	Independence Diagrams: A Technique for Visual Data Mining.	Stefan Berchtold,H. V. Jagadish,Kenneth A. Ross	1998	Independence Diagrams: A Technique for Visual Data Mining.
KDD	CLOUDS: A Decision Tree Classifier for Large Datasets.	Khaled Alsabti,Sanjay Ranka,Vineet Singh	1998	CLOUDS: A Decision Tree Classifier for Large Datasets.
KDD	Direct Marketing Response Models Using Genetic Algorithms.	Siddhartha Bhattacharyya	1998	Direct Marketing Response Models Using Genetic Algorithms.
KDD	Scaling Clustering Algorithms to Large Databases.	Paul S. Bradley,Usama M. Fayyad,Cory Reina	1998	Scaling Clustering Algorithms to Large Databases.
KDD	Joins that Generalize: Text Classification Using WHIRL.	William W. Cohen,Haym Hirsh	1998	Joins that Generalize: Text Classification Using WHIRL.
KDD	Giga-Mining.	Corinna Cortes,Daryl Pregibon	1998	Giga-Mining.
KDD	Rule Discovery from Time Series.	Gautam Das,King-Ip Lin,Heikki Mannila,Gopal Renganathan,Padhraic Smyth	1998	Rule Discovery from Time Series.
KDD	Similarity of Attributes by External Probes.	Gautam Das,Heikki Mannila,Pirjo Ronkainen	1998	Similarity of Attributes by External Probes.
KDD	Blurring the Distinction between Command and Data in Scientific KDD.	John V. Carlis,Elizabeth Shoop,Scott Krieger	1998	Blurring the Distinction between Command and Data in Scientific KDD.
KDD	Probabilistic Modeling for Information Retrieval with Unsupervised Training Data.	Ernest P. Chan,Santiago Garcia,Salim Roukos	1998	Probabilistic Modeling for Information Retrieval with Unsupervised Training Data.
KDD	Interactive Interpretation of Kohonen Maps Applied to Curves.	Anne Debregeas,Georges Hébrail	1998	Interactive Interpretation of Kohonen Maps Applied to Curves.
KDD	Finding Frequent Substructures in Chemical Compounds.	Luc Dehaspe,Hannu Toivonen,Ross D. King	1998	Finding Frequent Substructures in Chemical Compounds.
KDD	Toward Scalable Learning with Non-Uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection.	Philip K. Chan,Salvatore J. Stolfo	1998	Toward Scalable Learning with Non-Uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection.
KDD	"Occam's Two Razors: The Sharp and the Blunt."	Pedro Domingos	1998	"Occam's Two Razors: The Sharp and the Blunt."
KDD	FlexiMine - A Flexible Platform for KDD Research and Application Construction.	Carmel Domshlak,D. Gershkovich,Ehud Gudes,N. Liusternik,Amnon Meisels,Tzachi Rosen,Solomon Eyal Shimony	1998	FlexiMine - A Flexible Platform for KDD Research and Application Construction.
KDD	A Fast Computer Intrusion Detection Algorithm Based on Hypothesis Testing of Command Transition Probabilities.	William DuMouchel,Matthias Schonlau	1998	A Fast Computer Intrusion Detection Algorithm Based on Hypothesis Testing of Command Transition Probabilities.
KDD	Algorithms for Characterization and Trend Detection in Spatial Databases.	Martin Ester,Alexander Frommelt,Hans-Peter Kriegel,Jörg Sander	1998	Algorithms for Characterization and Trend Detection in Spatial Databases.
KDD	Knowledge Discovery and the Interface of Computing and Statistics.	Arnold Goodman,John F. Elder IV	1998	Knowledge Discovery and the Interface of Computing and Statistics.
KDD	On the Efficient Gathering of Sufficient Statistics for Classification from Large SQL Databases.	Goetz Graefe,Usama M. Fayyad,Surajit Chaudhuri	1998	On the Efficient Gathering of Sufficient Statistics for Classification from Large SQL Databases.
KDD	Initialization of Iterative Refinement Clustering Algorithms.	Usama M. Fayyad,Cory Reina,Paul S. Bradley	1998	Initialization of Iterative Refinement Clustering Algorithms.
KDD	Coactive Learning for Distributed Data Mining.	Dan L. Grecu,Lee A. Becker	1998	Coactive Learning for Distributed Data Mining.
KDD	Mining in the Presence of Selectivity Bias and its Application to Reject Inference.	A. J. Feelders,Soong Chang,Geoffrey J. McLachlan	1998	Mining in the Presence of Selectivity Bias and its Application to Reject Inference.
KDD	Pattern Directed Mining of Sequence Data.	Valery Guralnik,Duminda Wijesekera,Jaideep Srivastava	1998	Pattern Directed Mining of Sequence Data.
KDD	Mining Segment-Wise Periodic Patterns in Time-Related Databases.	Jiawei Han,Wan Gong,Yiwen Yin	1998	Mining Segment-Wise Periodic Patterns in Time-Related Databases.
KDD	Learning to Predict the Duration of an Automobile Trip.	Simon Handley,Pat Langley,Folke A. Rauscher	1998	Learning to Predict the Duration of an Automobile Trip.
KDD	An Efficient Approach to Clustering in Large Multimedia Databases with Noise.	Alexander Hinneburg,Daniel A. Keim	1998	An Efficient Approach to Clustering in Large Multimedia Databases with Noise.
KDD	Comparing Massive High-Dimensional Data Sets.	Theodore Johnson,Tamraparni Dasu	1998	Comparing Massive High-Dimensional Data Sets.
KDD	Fast Computation of 2-Dimensional Depth Contours.	Theodore Johnson,Ivy Kwok,Raymond T. Ng	1998	Fast Computation of 2-Dimensional Depth Contours.
KDD	Defining the Goals to Optimise Data Mining Performance.	Mark G. Kelly,David J. Hand,Niall M. Adams	1998	Defining the Goals to Optimise Data Mining Performance.
KDD	An Enhanced Representation of Time Series Which Allows Fast and Accurate Classification, Clustering and Relevance Feedback.	Eamonn J. Keogh,Michael J. Pazzani	1998	An Enhanced Representation of Time Series Which Allows Fast and Accurate Classification, Clustering and Relevance Feedback.
KDD	Active Templates: Comprehensive Support for the Knowledge Discovery Process.	Randy Kerber,Hal Beck,Tej Anand,Bill Smart	1998	Active Templates: Comprehensive Support for the Knowledge Discovery Process.
KDD	Targeting Business Users with Decision Table Classifiers.	Ron Kohavi,Dan Sommerfield	1998	Targeting Business Users with Decision Table Classifiers.
KDD	BAYDA: Software for Bayesian Classification and Feature Selection.	Petri Kontkanen,Petri Myllymäki,Tomi Silander,Henry Tirri	1998	BAYDA: Software for Bayesian Classification and Feature Selection.
KDD	Approaches to Online Learning and Concept Drift for User Identification in Computer Security.	Terran Lane,Carla E. Brodley	1998	Approaches to Online Learning and Concept Drift for User Identification in Computer Security.
KDD	Mining Audit Data to Build Intrusion Detection Models.	Wenke Lee,Salvatore J. Stolfo,Kui W. Mok	1998	Mining Audit Data to Build Intrusion Detection Models.
KDD	Data Mining for Direct Marketing: Problems and Solutions.	Charles X. Ling,Chenghui Li	1998	Data Mining for Direct Marketing: Problems and Solutions.
KDD	Integrating Classification and Association Rule Mining.	Bing Liu,Wynne Hsu,Yiming Ma	1998	Integrating Classification and Association Rule Mining.
KDD	Human Performance on Clustering Web Pages: A Preliminary Study.	Sofus A. Macskassy,Arunava Banerjee,Brian D. Davison,Haym Hirsh	1998	Human Performance on Clustering Web Pages: A Preliminary Study.
KDD	Aggregation of Imprecise and Uncertain Information for Knowledge Discovery in Databases.	Sally I. McClean,Bryan W. Scotney,Mary Shapcott	1998	Aggregation of Imprecise and Uncertain Information for Knowledge Discovery in Databases.
KDD	Discovering Predictive Association Rules.	Nimrod Megiddo,Ramakrishnan Srikant	1998	Discovering Predictive Association Rules.
KDD	Reinforcement Learning for Trading Systems and Portfolios.	John E. Moody,Matthew Saffell	1998	Reinforcement Learning for Trading Systems and Portfolios.
KDD	Group Bitmap Index: A Structure for Association Rules Retrieval.	Tadeusz Morzy,Maciej Zakrzewicz	1998	Group Bitmap Index: A Structure for Association Rules Retrieval.
KDD	Towards the Personalization of Algorithms Evaluation in Data Mining.	Gholamreza Nakhaeizadeh,Alexander Schnabl	1998	Towards the Personalization of Algorithms Evaluation in Data Mining.
KDD	Evaluating Usefulness for Dynamic Classification.	Gholamreza Nakhaeizadeh,Charles Taylor,Carsten Lanquillon	1998	Evaluating Usefulness for Dynamic Classification.
KDD	Large Datasets Lead to Overly Complex Models: An Explanation and a Solution.	Tim Oates,David Jensen	1998	Large Datasets Lead to Overly Complex Models: An Explanation and a Solution.
KDD	Analysing Rock Samples for the Mars Lander.	Jonathan J. Oliver,Ted Roush,Paul Gazis,Wray L. Buntine,Rohan A. Baxter,Steven R. Waterhouse	1998	Analysing Rock Samples for the Mars Lander.
KDD	A Belief-Driven Method for Discovering Unexpected Patterns.	Balaji Padmanabhan,Alexander Tuzhilin	1998	A Belief-Driven Method for Discovering Unexpected Patterns.
KDD	Memory Placement Techniques for Parallel Association Mining.	Srinivasan Parthasarathy,Mohammed Javeed Zaki,Wei Li	1998	Memory Placement Techniques for Parallel Association Mining.
KDD	Interpretable Boosted Naïve Bayes Classification.	"Greg Ridgeway,David Madigan,Thomas Richardson,John O'Kane"	1998	Interpretable Boosted Naïve Bayes Classification.
KDD	Methods for Linking and Mining Massive Heterogeneous Databases.	José C. Pinheiro,Don X. Sun	1998	Methods for Linking and Mining Massive Heterogeneous Databases.
KDD	Ranking - Methods for Flexible Evaluation and Efficient Comparison of Classification Performance.	Saharon Rosset	1998	Ranking - Methods for Flexible Evaluation and Efficient Comparison of Classification Performance.
KDD	Time Series Forecasting from High-Dimensional Data with Multiple Adaptive Layers.	R. Bharat Rao,Scott Rickard,Frans Coetzee	1998	Time Series Forecasting from High-Dimensional Data with Multiple Adaptive Layers.
KDD	A Robust System Architecture for Mining Semi-Structured Data.	Lisa Singh,Bin Chen,Rebecca Haight,Peter Scheuermann,Kiyoko Aoki	1998	A Robust System Architecture for Mining Semi-Structured Data.
KDD	A Data Mining Support Environment and its Application on Insurance Data.	Martin Staudt,Jörg-Uwe Kietz,Ulrich Reimer	1998	A Data Mining Support Environment and its Application on Insurance Data.
KDD	Coincidence Detection: A Fast Method for Discovering Higher-Order Correlations in Multidimensional Data.	Evan W. Steeg,Derek A. Robinson,Ed Willis	1998	Coincidence Detection: A Fast Method for Discovering Higher-Order Correlations in Multidimensional Data.
KDD	Mining Databases with Different Schemas: Integrating Incompatible Classifiers.	Andreas L. Prodromidis,Salvatore J. Stolfo	1998	Mining Databases with Different Schemas: Integrating Incompatible Classifiers.
KDD	Defining as a Data Mining Primitive.	Ramesh Subramonian	1998	Defining as a Data Mining Primitive.
KDD	Simultaneous Reliability Evaluation of Generality and Accuracy for Rule Discovery in Databases.	Einoshin Suzuki	1998	Simultaneous Reliability Evaluation of Generality and Accuracy for Rule Discovery in Databases.
KDD	Mining Generalized Association Rules and Sequential Patterns Using SQL Queries.	Shiby Thomas,Sunita Sarawagi	1998	Mining Generalized Association Rules and Sequential Patterns Using SQL Queries.
KDD	Data Reduction Based on Hyper Relations.	Hui Wang,Ivo Düntsch,David A. Bell	1998	Data Reduction Based on Hyper Relations.
KDD	Interestingness-Based Interval Merger for Numeric Association Rules.	Ke Wang,Soon Hock William Tay,Bing Liu	1998	Interestingness-Based Interval Merger for Numeric Association Rules.
KDD	Discovering Technical Traders in the T-bond Futures Market.	Andreas S. Weigend,Fei Chen,Stephen Figlewski,Steven R. Waterhouse	1998	Discovering Technical Traders in the T-bond Futures Market.
KDD	Learning to Predict Rare Events in Event Sequences.	Gary M. Weiss,Haym Hirsh	1998	Learning to Predict Rare Events in Event Sequences.
KDD	Daily Prediction of Major Stock Indices from Textual WWW Data.	Beat Wüthrich,D. Permunetilleke,S. Leung,Vincent Cho,Jian Zhang,W. Lam	1998	Daily Prediction of Major Stock Indices from Textual WWW Data.
KDD	PlanMine: Sequence Mining for Plan Failures.	Mohammed Javeed Zaki,Neal Lesh,Mitsunori Ogihara	1998	PlanMine: Sequence Mining for Plan Failures.
KDD	Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (KDD-98), New York City, New York, USA, August 27-31, 1998	Rakesh Agrawal,Paul E. Stolorz,Gregory Piatetsky-Shapiro	1998	Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (KDD-98), New York City, New York, USA, August 27-31, 1998
PKDD	A Comparison of Batch and Incremental Supervised Learning Algorithms.	Leonardo Carbonara,Alastair Borrowman	1998	A Comparison of Batch and Incremental Supervised Learning Algorithms.
PKDD	Interactive Visualization for Predictive Modelling with Decision Tree Induction.	Tu Bao Ho,Trong Dung Nguyen	1998	Interactive Visualization for Predictive Modelling with Decision Tree Induction.
PKDD	Extended Functional Dependencies as a Basis for Linguistic Summaries.	Patrick Bosc,Ludovic Lietard,Olivier Pivert	1998	Extended Functional Dependencies as a Basis for Linguistic Summaries.
PKDD	Overcoming Fragmentation in Decision Trees Through Attribute Value Grouping.	K. M. Ho,Paul D. Scott	1998	Overcoming Fragmentation in Decision Trees Through Attribute Value Grouping.
PKDD	Knowledge Discovery with Qualitative Influences and Synergies.	Jesús Cerquides,Ramon López de Mántaras	1998	Knowledge Discovery with Qualitative Influences and Synergies.
PKDD	Discretization and Grouping: Preprocessing Steps for Data Mining.	Petr Berka,Ivan Bruha	1998	Discretization and Grouping: Preprocessing Steps for Data Mining.
PKDD	Language Support for Temporal Data Mining.	Xiaodong Chen,Ilias Petrounias	1998	Language Support for Temporal Data Mining.
PKDD	Fuzzy Spatial OQL for Fuzzy Knowledge Discovery in Databases.	Nara Martini Bigolin,Christophe Marsala	1998	Fuzzy Spatial OQL for Fuzzy Knowledge Discovery in Databases.
PKDD	Object Mining: A Practical Application of Data Mining for the Construction and Maintenance of Software Components.	Anders Torvill Bjorvand	1998	Object Mining: A Practical Application of Data Mining for the Construction and Maintenance of Software Components.
PKDD	Ranked Rules and Data Visualization.	Leon Bobrowski,Tomasz Sowinski	1998	Ranked Rules and Data Visualization.
PKDD	Resampling in an Indefinite Database to Approximate Functional Dependencies.	Ethan Collopy,Mark Levene	1998	Resampling in an Indefinite Database to Approximate Functional Dependencies.
PKDD	Querying Inductive Databases: A Case Study on the MINE RULE Operator.	Jean-François Boulicaut,Mika Klemettinen,Heikki Mannila	1998	Querying Inductive Databases: A Case Study on the MINE RULE Operator.
PKDD	A Hybrid Approach to Feature Selection.	Moussa Boussouf	1998	A Hybrid Approach to Feature Selection.
PKDD	Knowledge Discovery from Client-Server Databases.	Neil Dewhurst,Simon H. Lavington	1998	Knowledge Discovery from Client-Server Databases.
PKDD	Cost Sensitive Discretization of Numeric Attributes.	Tom Brijs,Koen Vanhoof	1998	Cost Sensitive Discretization of Numeric Attributes.
PKDD	Postponing the Evaluation of Attributes with a High Number of Boundary Points.	Tapio Elomaa,Juho Rousu	1998	Postponing the Evaluation of Attributes with a High Number of Boundary Points.
PKDD	Practical Text Mining.	Ronen Feldman	1998	Practical Text Mining.
PKDD	Trend Graphs: Visualizing the Evolution of Concept Relationships in Large Document Collections.	Ronen Feldman,Yonatan Aumann,Amir Zilberstein,Yaron Ben-Yehuda	1998	Trend Graphs: Visualizing the Evolution of Concept Relationships in Large Document Collections.
PKDD	Text Mining at the Term Level.	Ronen Feldman,Moshe Fresko,Yakkov Kinar,Yehuda Lindell,Orly Liphstat,Martin Rajman,Yonatan Schler,Oren Zamir	1998	Text Mining at the Term Level.
PKDD	A New Algorithm for Faster Mining of Generalized Association Rules.	Jochen Hipp,Andreas Myka,Rüdiger Wirth,Ulrich Güntzer	1998	A New Algorithm for Faster Mining of Generalized Association Rules.
PKDD	Discovery of Common Subsequences in Cognitive Evoked Potentials.	Arthur Flexer,Herbert Bauer	1998	Discovery of Common Subsequences in Cognitive Evoked Potentials.
PKDD	On Objective Measures of Rule Surprisingness.	Alex Alves Freitas	1998	On Objective Measures of Rule Surprisingness.
PKDD	Scalable, High-Performance Data Mining with Parallel Processing.	Alex Alves Freitas	1998	Scalable, High-Performance Data Mining with Parallel Processing.
PKDD	A Metric for Selection of the Most Promising Rules.	Pedro Gago,Carlos Bento	1998	A Metric for Selection of the Most Promising Rules.
PKDD	Data Mining at a Major Bank: Lessons from a Large Marketing Application.	Petra Hunziker,Andreas Maier,Alex Nippe,Markus Tresch,Douglas Weers,Peter Zemp	1998	Data Mining at a Major Bank: Lessons from a Large Marketing Application.
PKDD	Knowledge Discovery with Clustering Based on Rules. Interpreting Results.	Karina Gibert,Tomàs Aluja-Banet,Ulises Cortés	1998	Knowledge Discovery with Clustering Based on Rules. Interpreting Results.
PKDD	Improving the Discovery of Association Rules with Intensity of Implication.	Sylvie Guillaume,Fabrice Guillet,Jacques Philippe	1998	Improving the Discovery of Association Rules with Intensity of Implication.
PKDD	Generalization Lattices.	Howard J. Hamilton,Robert J. Hilderman,Liangchun Li,Dee Jay Randall	1998	Generalization Lattices.
PKDD	Model Switching for Bayesian Classification Trees with Soft Splits.	Jörg Kindermann,Gerhard Paass	1998	Model Switching for Bayesian Classification Trees with Soft Splits.
PKDD	The PSP Approach for Mining Sequential Patterns.	Florent Masseglia,Fabienne Cathala,Pascal Poncelet	1998	The PSP Approach for Mining Sequential Patterns.
PKDD	PolyAnalyst Data Analysis Technique and Its Specialization for Processing Data Organized as a Set of Attribute Values.	Mikhail V. Kiselev,Sergei M. Ananyan,Sergei B. Arseniev	1998	PolyAnalyst Data Analysis Technique and Its Specialization for Processing Data Organized as a Set of Attribute Values.
PKDD	Exploratory Attributes Search for Time-Series Data: An Experimental System for Agricultural Application.	Kazunori Matsumoto	1998	Exploratory Attributes Search for Time-Series Data: An Experimental System for Agricultural Application.
PKDD	A Procedure to Compute Prototypes for Data Mining in Non-structured Domains.	Juan Méndez,Mario Hernández,Javier Lorenzo	1998	A Procedure to Compute Prototypes for Data Mining in Non-structured Domains.
PKDD	From the Data Mine to the Knowledge Mill: Applying the Principles of Lexical Analysis to the Data Mining and Knowledge Discovery Process.	Jean Moscarola,Richard Bolden	1998	From the Data Mine to the Knowledge Mill: Applying the Principles of Lexical Analysis to the Data Mining and Knowledge Discovery Process.
PKDD	Using Loglinear Clustering for Subcategorization Identification.	Nuno Miguel Marques,José Gabriel Pereira Lopes,Carlos Agra Coelho	1998	Using Loglinear Clustering for Subcategorization Identification.
PKDD	Representative Association Rules and Minimum Condition Maximum Consequence Association Rules.	Marzena Kryszkiewicz	1998	Representative Association Rules and Minimum Condition Maximum Consequence Association Rules.
PKDD	Discovery of Decision Rules from Databases: An Evolutionary Approach.	Wojciech Kwedlo,Marek Kretowski	1998	Discovery of Decision Rules from Databases: An Evolutionary Approach.
PKDD	A Relational Data Mining Tool Based On Genetic Programming.	Lionel Martin,Frédéric Moal,Christel Vrain	1998	A Relational Data Mining Tool Based On Genetic Programming.
PKDD	TextVis: An Integrated Visual Environment for Text Mining.	David Landau,Ronen Feldman,Yonatan Aumann,Moshe Fresko,Yehuda Lindell,Orly Liphstat,Oren Zamir	1998	TextVis: An Integrated Visual Environment for Text Mining.
PKDD	Industrial Applications of Data Mining.	Gholamreza Nakhaeizadeh	1998	Industrial Applications of Data Mining.
PKDD	Discovery of Diagnostic Patterns from Protein Sequence Databases.	Björn Olsson,Kim Laurio	1998	Discovery of Diagnostic Patterns from Protein Sequence Databases.
PKDD	Detection of Interdependences in Attribute Selection.	Javier Lorenzo,Mario Hernández,Juan Méndez	1998	Detection of Interdependences in Attribute Selection.
PKDD	For Visualization-Based Analysis Tools in Knowledge Discovery Process: A Multilayer Perceptron versus Principal Components Analysis: A Comparative Study.	Xavier Polanco,Claire François,Mohamed Aly Ould Louly	1998	For Visualization-Based Analysis Tools in Knowledge Discovery Process: A Multilayer Perceptron versus Principal Components Analysis: A Comparative Study.
PKDD	Data Transformation and Rough Sets.	Jaroslaw Stepaniuk,Marcin Maj	1998	Data Transformation and Rough Sets.
PKDD	Knowledge Discovery in Spatial Data by Means of ILP.	Lubos Popelínsky	1998	Knowledge Discovery in Spatial Data by Means of ILP.
PKDD	Conceptual Knowledge Discovery in Databases Using Formal Concept Analysis Methods.	Gerd Stumme,Rudolf Wille,Uta Wille	1998	Conceptual Knowledge Discovery in Databases Using Formal Concept Analysis Methods.
PKDD	Preprocessing of Missing Values Using Robust Association Rules.	Arnaud Ragel	1998	Preprocessing of Missing Values Using Robust Association Rules.
PKDD	Discovery of Surprising Exception Rules Based on Intensity of Implication.	Einoshin Suzuki,Yves Kodratoff	1998	Discovery of Surprising Exception Rules Based on Intensity of Implication.
PKDD	Efficient Construction of Comprehensible Hierarchical Clusterings.	Luis Talavera,Javier Béjar	1998	Efficient Construction of Comprehensible Hierarchical Clusterings.
PKDD	Classes of Four-Fold Table Quantifiers.	Jan Rauch	1998	Classes of Four-Fold Table Quantifiers.
PKDD	Similarity-Driven Sampling for Data Mining.	Thomas P. Reinartz	1998	Similarity-Driven Sampling for Data Mining.
PKDD	Inducing Cost-Sensitive Trees via Instance Weighting.	Kai Ming Ting	1998	Inducing Cost-Sensitive Trees via Instance Weighting.
PKDD	CLASITEX: A Tool for Knowledge Discovery from Texts.	José Francisco Martínez Trinidad,Beatriz Beltrán Martínez,Adolfo Guzmán-Arenas,José Ruiz-Shulcloper	1998	CLASITEX: A Tool for Knowledge Discovery from Texts.
PKDD	Modeling the Business Process by Mining Multiple Databases.	Arun P. Sanjeev,Jan M. Zytkow	1998	Modeling the Business Process by Mining Multiple Databases.
PKDD	Discovery of Approximate Medical Knowledge Based on Rough Set Model.	Shusaku Tsumoto	1998	Discovery of Approximate Medical Knowledge Based on Rough Set Model.
PKDD	Handling KDD Process Changes by Incremental Replanning.	Ning Zhong,Chunnian Liu,Yoshitsugu Kakemoto,Setsuo Ohsuga	1998	Handling KDD Process Changes by Incremental Replanning.
PKDD	"Principles of Data Mining and Knowledge Discovery, Second European Symposium, PKDD '98, Nantes, France, September 23-26, 1998, Proceedings"	Jan M. Zytkow,Mohamed Quafafou	1998	"Principles of Data Mining and Knowledge Discovery, Second European Symposium, PKDD '98, Nantes, France, September 23-26, 1998, Proceedings"
KDD	Visual Classification: An Interactive Approach to Decision Tree Construction.	Mihael Ankerst,Christian Elsen,Martin Ester,Hans-Peter Kriegel	1999	Visual Classification: An Interactive Approach to Decision Tree Construction.
KDD	A Statistical Theory for Quantitative Association Rules.	Yonatan Aumann,Yehuda Lindell	1999	Association rules are a key data-mining tool and as such have been well researched. So far, this research has focused predominantly on databases containing categorical data only. However, many real-world databases contain quantitative attributes and current solutions for this case are so far inadequate. In this paper we introduce a new definition of quantitative association rules based on statistical inference theory. Our definition reflects the intuition that the goal of association rules is to find extraordinary and therefore interesting phenomena in databases. We also introduce the concept of sub-rules which can be applied to any type of association rule. Rigorous experimental evaluation on real-world datasets is presented, demonstrating the usefulness and characteristics of rules mined according to our definition.
KDD	An Efficient Algorithm to Update Large Itemsets with Early Pruning.	Necip Fazil Ayan,Abdullah Uz Tansel,M. Erol Arkun	1999	An Efficient Algorithm to Update Large Itemsets with Early Pruning.
KDD	A Classification-Based Methodology for Planning Audit Strategies in Fraud Detection.	Francesco Bonchi,Fosca Giannotti,Gianni Mainetto,Dino Pedreschi	1999	A Classification-Based Methodology for Planning Audit Strategies in Fraud Detection.
KDD	User Profiling in Personalization Applications Through Rule Discovery and Validation.	Gediminas Adomavicius,Alexander Tuzhilin	1999	User Profiling in Personalization Applications Through Rule Discovery and Validation.
KDD	Using Approximations to Scale Exploratory Data Analysis in Datacubes.	Daniel Barbará,Xintao Wu	1999	Using Approximations to Scale Exploratory Data Analysis in Datacubes.
KDD	Detecting Change in Categorical Data: Mining Contrast Sets.	Stephen D. Bay,Michael J. Pazzani	1999	Detecting Change in Categorical Data: Mining Contrast Sets.
KDD	Mining the Most Interesting Rules.	Roberto J. Bayardo Jr.,Rakesh Agrawal	1999	Mining the Most Interesting Rules.
KDD	On the Merits of Building Categorization Systems by Supervised Clustering.	Charu C. Aggarwal,Stephen C. Gates,Philip S. Yu	1999	On the Merits of Building Categorization Systems by Supervised Clustering.
KDD	Horting Hatches an Egg: A New Graph-Theoretic Approach to Collaborative Filtering.	Charu C. Aggarwal,Joel L. Wolf,Kun-Lung Wu,Philip S. Yu	1999	Horting Hatches an Egg: A New Graph-Theoretic Approach to Collaborative Filtering.
KDD	Density-Based Indexing for Approximate Nearest-Neighbor Queries.	Kristin P. Bennett,Usama M. Fayyad,Dan Geiger	1999	Density-Based Indexing for Approximate Nearest-Neighbor Queries.
KDD	Data Mining: Crossing the Chasm (Invited talk, Abstract only).	Rakesh Agrawal	1999	Data Mining: Crossing the Chasm (Invited talk, Abstract only).
KDD	NonStop SQL/MX Primitives for Knowledge Discovery.	John Clear,Debbie Dunn,Brad Harvey,Michael L. Heytens,Peter Lohman,Abhay Mehta,Mark Melton,Lars Rohrberg,Ashok Savasere,Robert M. Wehrmeister,Melody Xu	1999	NonStop SQL/MX Primitives for Knowledge Discovery.
KDD	Using Association Rules for Product Assortment Decisions: A Case Study.	Tom Brijs,Gilbert Swinnen,Koen Vanhoof,Geert Wets	1999	Using Association Rules for Product Assortment Decisions: A Case Study.
KDD	Mining Optimized Gain Rules for Numeric Attributes.	Sergey Brin,Rajeev Rastogi,Kyuseok Shim	1999	"Association rules are useful for determining correlations between attributes of a relation and have applications in the marketing, financial, and retail sectors. Furthermore, optimized association rules are an effective way to focus on the most interesting characteristics involving certain attributes. Optimized association rules are permitted to contain uninstantiated attributes and the problem is to determine instantiations such that either the support, confidence, or gain of the rule is maximized. In this paper, we generalize the optimized gain association rule problem by permitting rules to contain disjunctions over uninstantiated numeric attributes. Our generalized association rules enable us to extract more useful information about seasonal and local patterns involving the uninstantiated attribute. For rules containing a single numeric attribute, we present an algorithm with linear complexity for computing optimized gain rules. Furthermore, we propose a bucketing technique that can result in a significant reduction in input size by coalescing contiguous values without sacrificing optimality. We also present an approximation algorithm based on dynamic programming for two numeric attributes. Using recent results on binary space partitioning trees, we show that the approximations are within a constant factor of the optimal optimized gain rules. Our experimental results with synthetic data sets for a single numeric attribute demonstrate that our algorithm scales up linearly with the attribute's domain size as well as the number of disjunctions. In addition, we show that applying our optimized rule framework to a population survey real-life data set enables us to discover interesting underlying correlations among the attributes."
KDD	Information Mining Platforms: An Infrastructure for KDD Rapid Deployment.	Corinna Cortes,Daryl Pregibon	1999	Information Mining Platforms: An Infrastructure for KDD Rapid Deployment.
KDD	Towards Automated Synthesis of Data Mining Programs.	Wray L. Buntine,Bernd Fischer,Thomas Pressburger	1999	Towards Automated Synthesis of Data Mining Programs.
KDD	Applying General Bayesian Techniques to Improve TAN Induction.	Jesús Cerquides	1999	Applying General Bayesian Techniques to Improve TAN Induction.
KDD	Bayesian Networks for Lossless Dataset Compression.	Scott Davies,Andrew W. Moore	1999	Bayesian Networks for Lossless Dataset Compression.
KDD	The PanQ Tool and EMF SQL for Complex Data Management.	Damianos Chatziantoniou	1999	The PanQ Tool and EMF SQL for Complex Data Management.
KDD	Entropy-based Subspace Clustering for Mining Numerical Data.	Chun Hung Cheng,Ada Wai-Chee Fu,Yi Zhang	1999	Entropy-based Subspace Clustering for Mining Numerical Data.
KDD	MetaCost: A General Method for Making Classifiers Cost-Sensitive.	Pedro Domingos	1999	MetaCost: A General Method for Making Classifiers Cost-Sensitive.
KDD	Efficient Mining of Emerging Patterns: Discovering Trends and Differences.	Guozhu Dong,Jinyan Li	1999	Efficient Mining of Emerging Patterns: Discovering Trends and Differences.
KDD	Text Mining: Finding Nuggets in Mountains of Textual Data.	Jochen Dörre,Peter Gerstl,Roland Seiffert	1999	Text Mining: Finding Nuggets in Mountains of Textual Data.
KDD	Squashing Flat Files Flatter.	William DuMouchel,Chris Volinsky,Theodore Johnson,Corinna Cortes,Daryl Pregibon	1999	Squashing Flat Files Flatter.
KDD	Trajectory Clustering with Mixtures of Regression Models.	Scott Gaffney,Padhraic Smyth	1999	Trajectory Clustering with Mixtures of Regression Models.
KDD	CACTUS - Clustering Categorical Data Using Summaries.	Venkatesh Ganti,Johannes Gehrke,Raghu Ramakrishnan	1999	CACTUS - Clustering Categorical Data Using Summaries.
KDD	The Application of AdaBoost for Distributed, Scalable and On-Line Learning.	Wei Fan,Salvatore J. Stolfo,Junxin Zhang	1999	The Application of AdaBoost for Distributed, Scalable and On-Line Learning.
KDD	Activity Monitoring: Noticing Interesting Changes in Behavior.	Tom Fawcett,Foster J. Provost	1999	Activity Monitoring: Noticing Interesting Changes in Behavior.
KDD	Event Detection from Time Series Data.	Valery Guralnik,Jaideep Srivastava	1999	Event Detection from Time Series Data.
KDD	Farming the Web for Systematic Business Intelligence (Invited talk, Abstract only).	Richard D. Hackathorn	1999	The technologies of data warehousing, data mining, hypertext analysis, information visualization, and web information resources are rapidly converging. The challenge is to architect these technologies into a system for systematic business intelligence for a corporation. We need to move from an information refining process that is often haphazard and narrow to one that is reliable and continuous. Web farming is a new area that suggests a methodology and architecture for accomplishing this.
KDD	WAPS, a Data Mining Support Environment for the Planning of Warranty and Goodwill Costs in the Automobile Industry.	Edgar Hotz,Gholamreza Nakhaeizadeh,B. Petzsche,H. Spiegelberger	1999	WAPS, a Data Mining Support Environment for the Planning of Warranty and Goodwill Costs in the Automobile Industry.
KDD	Adaptive Query Processing for Time-Series Data.	Yun-Wu Huang,Philip S. Yu	1999	Adaptive Query Processing for Time-Series Data.
KDD	Optimization of Collection Efforts in Automobile Financing - a KDD Supported Environment.	H. Kauderer,Gholamreza Nakhaeizadeh,F. Artiles,H. Jeromin	1999	Optimization of Collection Efforts in Automobile Financing - a KDD Supported Environment.
KDD	The Impact of Changing Populations on Classifier Performance.	Mark G. Kelly,David J. Hand,Niall M. Adams	1999	The Impact of Changing Populations on Classifier Performance.
KDD	Fast and Effective Text Mining Using Linear-Time Document Clustering.	Bjornar Larsen,Chinatsu Aone	1999	Fast and Effective Text Mining Using Linear-Time Document Clustering.
KDD	Mining in a Data-Flow Environment: Experience in Network Intrusion Detection.	Wenke Lee,Salvatore J. Stolfo,Kui W. Mok	1999	Mining in a Data-Flow Environment: Experience in Network Intrusion Detection.
KDD	Mining Features for Sequence Classification.	Neal Lesh,Mohammed Javeed Zaki,Mitsunori Ogihara	1999	Mining Features for Sequence Classification.
KDD	Pruning and Summarizing the Discovered Associations.	Bing Liu,Wynne Hsu,Yiming Ma	1999	Pruning and Summarizing the Discovered Associations.
KDD	Mining Association Rules with Multiple Minimum Supports.	Bing Liu,Wynne Hsu,Yiming Ma	1999	Mining Association Rules with Multiple Minimum Supports.
KDD	Mining Interesting Knowledge Using DM-II.	Bing Liu,Wynne Hsu,Yiming Ma,Shu Chen	1999	Mining Interesting Knowledge Using DM-II.
KDD	Origami: A New Data Visualization Tool.	Jen Que Louie,Tom Kraay	1999	Origami: A New Data Visualization Tool.
KDD	Mining Lesion-Deficit Associations in a Brain Image Database.	Vasileios Megalooikonomou,Christos Davatzikos,Edward Herskovits	1999	Mining Lesion-Deficit Associations in a Brain Image Database.
KDD	Statistics and Data Mining Techniques for Lifetime Value Modeling.	D. R. Mani,James Drew,Andrew Betz,Piew Datta	1999	Statistics and Data Mining Techniques for Lifetime Value Modeling.
KDD	Prediction with Local Patterns using Cross-Entropy.	Heikki Mannila,Dmitry Pavlov,Padhraic Smyth	1999	Prediction with Local Patterns using Cross-Entropy.
KDD	Extending Naïve Bayes Classifiers Using Long Itemsets.	Dimitris Meretakis,Beat Wüthrich	1999	Extending Naïve Bayes Classifiers Using Long Itemsets.
KDD	Using a Knowledge Cache for Interactive Discovery of Association Rules.	Biswadeep Nag,Prasad Deshpande,David J. DeWitt	1999	Using a Knowledge Cache for Interactive Discovery of Association Rules.
KDD	Identifying Distinctive Subsequences in Multivariate Time Series by Clustering.	Tim Oates	1999	Identifying Distinctive Subsequences in Multivariate Time Series by Clustering.
KDD	Accelerating Exact -means Algorithms with Geometric Reasoning.	Dan Pelleg,Andrew W. Moore	1999	Accelerating Exact -means Algorithms with Geometric Reasoning.
KDD	Estimating Campaign Benefits and Modeling Lift.	Gregory Piatetsky-Shapiro,Brij M. Masand	1999	Estimating Campaign Benefits and Modeling Lift.
KDD	Discovery of Fraud Rules for Telecommunications - Challenges and Solutions.	Saharon Rosset,Uzi Murad,Einat Neumann,Yizhak Idan,Gadi Pinkas	1999	Discovery of Fraud Rules for Telecommunications - Challenges and Solutions.
KDD	Generalized Additive Neural Networks.	William J. E. Potts	1999	Generalized Additive Neural Networks.
KDD	2001: A Statistical Odyssey (Invited talk, Abstract only).	Daryl Pregibon	1999	2001: A Statistical Odyssey (Invited talk, Abstract only).
KDD	Efficient Progressive Sampling.	Foster J. Provost,David Jensen,Tim Oates	1999	Efficient Progressive Sampling.
KDD	Interestingness via What is Not Interesting.	Sigal Sahar	1999	Interestingness via What is Not Interesting.
KDD	Mining GPS Data to Augment Road Models.	Seth Rogers,Pat Langley,Christopher Wilson	1999	Mining GPS Data to Augment Road Models.
KDD	Compressed Data Cubes for OLAP Aggregate Query Approximation on Continuous Dimensions.	Jayavel Shanmugasundaram,Usama M. Fayyad,Paul S. Bradley	1999	Compressed Data Cubes for OLAP Aggregate Query Approximation on Continuous Dimensions.
KDD	Monitoring a Newsfeed for Hot Topics.	Mark Shewhart,Mark Wasson	1999	Monitoring a Newsfeed for Hot Topics.
KDD	A Study of Support Vectors on Model Independent Example Selection.	Nadeem Ahmed Syed,Huan Liu,Kah Kay Sung	1999	A Study of Support Vectors on Model Independent Example Selection.
KDD	Handling Concept Drifts in Incremental Learning with Support Vector Machines.	Nadeem Ahmed Syed,Huan Liu,Kah Kay Sung	1999	Handling Concept Drifts in Incremental Learning with Support Vector Machines.
KDD	Breaking the Barrier of Transactions: Mining Inter-Transaction Association Rules.	Anthony K. H. Tung,Hongjun Lu,Jiawei Han,Ling Feng	1999	Breaking the Barrier of Transactions: Mining Inter-Transaction Association Rules.
KDD	Evaluating a Class of Distance-Mapping Algorithms for Data Mining and Clustering.	Jason Tsong-Li Wang,Xiong Wang,King-Ip Lin,Dennis Shasha,Bruce A. Shapiro,Kaizhong Zhang	1999	Evaluating a Class of Distance-Mapping Algorithms for Data Mining and Clustering.
KDD	Discovering Roll-Up Dependencies.	Jef Wijsen,Raymond T. Ng,Toon Calders	1999	Discovering Roll-Up Dependencies.
KDD	Fast Density Estimation Using CF-Kernel for Very Large Databases.	Tian Zhang,Raghu Ramakrishnan,Miron Livny	1999	Fast Density Estimation Using CF-Kernel for Very Large Databases.
KDD	Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Diego, CA, USA, August 15-18, 1999	Usama M. Fayyad,Surajit Chaudhuri,David Madigan	1999	Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Diego, CA, USA, August 15-18, 1999
PKDD	An Innovative GA-Based Decision Tree Classifier in Large Scale Data Mining.	Zhiwei Fu	1999	An Innovative GA-Based Decision Tree Classifier in Large Scale Data Mining.
PKDD	Taming Large Rule Models in Rough Set Approaches.	Thomas Ågotnes,Henryk Jan Komorowski,Terje Løken	1999	Taming Large Rule Models in Rough Set Approaches.
PKDD	Selective Propositionalization for Relational Learning.	Érick Alphonse,Céline Rouveirol	1999	Selective Propositionalization for Relational Learning.
PKDD	Using Genetic Algorithms to Evolve a Rule Hierarchy.	Robert Cattral,Franz Oppacher,Dwight Deugo	1999	Using Genetic Algorithms to Evolve a Rule Hierarchy.
PKDD	Circle Graphs: New Visualization Tools for Text-Mining.	Yonatan Aumann,Ronen Feldman,Yaron Ben-Yehuda,David Landau,Orly Liphstat,Yonatan Schler	1999	Circle Graphs: New Visualization Tools for Text-Mining.
PKDD	Rough Dependencies as a Particular Case of Correlation: Application to the Calculation of Approximative Reducts.	María C. Fernández-Baizán,Ernestina Menasalvas Ruiz,José M. Peña Sánchez,Socorro Millán,Eloina Mesa	1999	Rough Dependencies as a Particular Case of Correlation: Application to the Calculation of Approximative Reducts.
PKDD	Mining Temporal Features in Association Rules.	Xiaodong Chen,Ilias Petrounias	1999	Mining Temporal Features in Association Rules.
PKDD	Applying Data Mining Techniques to Wafer Manufacturing.	Elisa Bertino,Barbara Catania,Eleonora Caglio	1999	Applying Data Mining Techniques to Wafer Manufacturing.
PKDD	TopCat: Data Mining for Topic Identification in a Text Corpus.	Chris Clifton,Robert Cooley	1999	TopCat: Data Mining for Topic Identification in a Text Corpus.
PKDD	The Improvement of Response Modeling: Combining Rule-Induction and Case-Based Reasoning.	Filip Coenen,Gilbert Swinnen,Koen Vanhoof,Geert Wets	1999	The Improvement of Response Modeling: Combining Rule-Induction and Case-Based Reasoning.
PKDD	Simultaneous Prediction of Mulriple Chemical Parameters of River Water Quality with TILDE.	Hendrik Blockeel,Saso Dzeroski,Jasna Grbovic	1999	Simultaneous Prediction of Mulriple Chemical Parameters of River Water Quality with TILDE.
PKDD	On the Consistency of Information Filters for Lazy Learning Algorithms.	Henry Brighton,Chris Mellish	1999	On the Consistency of Information Filters for Lazy Learning Algorithms.
PKDD	Analyzing an Email Collection Using Formal Concept Analysis.	Richard J. Cole II,Peter W. Eklund	1999	Analyzing an Email Collection Using Formal Concept Analysis.
PKDD	Query Languages for Knowledge Discovery in Databases.	Jean-François Boulicaut	1999	Query Languages for Knowledge Discovery in Databases.
PKDD	Bisiness Focused Evaluation Methods: A Case Study.	Piew Datta	1999	Bisiness Focused Evaluation Methods: A Case Study.
PKDD	OPTICS-OF: Identifying Local Outliers.	Markus M. Breunig,Hans-Peter Kriegel,Raymond T. Ng,Jörg Sander	1999	OPTICS-OF: Identifying Local Outliers.
PKDD	Taxonomy Formation by Approximate Equivalence Relations, Revisited.	F. A. El-Mouadib,Jacek Koronacki,Jan M. Zytkow	1999	Taxonomy Formation by Approximate Equivalence Relations, Revisited.
PKDD	Speeding Up the Search for Optimal Partitions.	Tapio Elomaa,Juho Rousu	1999	Speeding Up the Search for Optimal Partitions.
PKDD	Combining Data and Knowledge by MaxEnt-Optimization of Probability Distributions.	Wolfgang Ertel,Manfred Schramm	1999	Combining Data and Knowledge by MaxEnt-Optimization of Probability Distributions.
PKDD	Handling Missing Data in Trees: Surrogate Splits or Statistical Imputation.	A. J. Feelders	1999	Handling Missing Data in Trees: Surrogate Splits or Statistical Imputation.
PKDD	Text Mining via Information Extraction.	Ronen Feldman,Yonatan Aumann,Moshe Fresko,Orly Liphstat,Binyamin Rosenfeld,Yonatan Schler	1999	Text Mining via Information Extraction.
PKDD	A Fuzzy Beam-Search Rule Induction Algorithm.	Christina S. Fertig,Alex Alves Freitas,Lucia V. R. Arruda,Celso A. A. Kaestner	1999	A Fuzzy Beam-Search Rule Induction Algorithm.
PKDD	On the Use of Self-Organizing Maps for Clustering and Visualization.	Arthur Flexer	1999	We show that the number of output units used in a self-organizing map (SOM) influences its applicability for either clustering or visualization. By reviewing the appropriate literature and theory and own empirical results, we demonstrate that SOMs can be used for clustering or visualization separately, for simultaneous clustering and visualization, and even for clustering via visualization. For all these different kinds of application, SOM is compared to other statistical approaches. This will show SOM to be a flexible tool which can be used for various forms of explorative data analysis but it will also be made obvious that this flexibility comes with a price in terms of impaired performance.
PKDD	Extension to C-means Algorithm for the Use of Similarity Functions.	Javier Raymundo García-Serrano,José Francisco Martínez Trinidad	1999	Extension to C-means Algorithm for the Use of Similarity Functions.
PKDD	On the Correspondence between Classes of Implicational and Equivalence Quantifiers.	Jirí Ivánek	1999	On the Correspondence between Classes of Implicational and Equivalence Quantifiers.
PKDD	Querying Inductive Databases via Logic-Based User-Defined Aggregates.	Fosca Giannotti,Giuseppe Manco	1999	Querying Inductive Databases via Logic-Based User-Defined Aggregates.
PKDD	Logics and Statistics for Association Rules and Beyond Abstract of Tutorial.	Petr Hájek,Jan Rauch	1999	Logics and Statistics for Association Rules and Beyond Abstract of Tutorial.
PKDD	Predicting Chemical Carcinogenesis Using Structural Information Only.	Claire J. Kennedy,Christophe G. Giraud-Carrier,Douglas W. Bristol	1999	This paper reports on the application of the Strongly Typed Evolutionary Programming System (STEPS) to the PTE2 challenge, which consists of predicting the carcinogenic activity of chemical compounds from their molecular structure and the outcomes of a number of laboratory analyses. Most contestants so far have relied heavily on results of short term toxicity (STT) assays. Using both types of information made available, most models incorporate attributes that make them strongly dependent on STT results. Although such models may prove to be accurate and informative, the use of toxicological information requires time cost and in some cases substantial utilisation of laboratory animals. If toxicological information only makes explicit, properties implicit in the molecular structure of chemicals, then provided a sufficiently expressive representation language, accurate solutions may be obtained from the structural information only. Such solutions may offer more tangible insight into the mechanistic paths and features that govern chemical toxicity as well as prediction based on virtual chemistry for the universe of compounds.
PKDD	Scaling up Dynamic Time Warping to Massive Dataset.	Eamonn J. Keogh,Michael J. Pazzani	1999	Scaling up Dynamic Time Warping to Massive Dataset.
PKDD	Heuristic Measures of Interestingness.	Robert J. Hilderman,Howard J. Hamilton	1999	Heuristic Measures of Interestingness.
PKDD	Discovering Rules in Information Trees.	Zbigniew W. Ras	1999	Discovering Rules in Information Trees.
PKDD	Learning from Highly Structured Data by Decomposition.	René MacKinney-Romero,Christophe G. Giraud-Carrier	1999	This paper addresses the problem of learning from highly structured data. Specifically, it describes a procedure, called decomposition, that allows a learner to access automatically the subparts of examples represented as closed terms in a higher-order language. This procedure maintains a clear distinction between the structure of an individual and its properties. A learning system based on decomposition is also presented and several examples of its use are described.
PKDD	Discovering and Visualizing Attribute Associations Using Bayesian Networks and Their Use in KDD.	Gou Masuda,Rei Yano,Norihiro Sakamoto,Kazuo Ushijima	1999	Discovering and Visualizing Attribute Associations Using Bayesian Networks and Their Use in KDD.
PKDD	LA - A Clustering Algorithm with an Automated Selection of Attributes, wich is Invariant to Functional Transformations of Coordinates.	Mikhail V. Kiselev,Sergei M. Ananyan,Sergei B. Arseniev	1999	LA - A Clustering Algorithm with an Automated Selection of Attributes, wich is Invariant to Functional Transformations of Coordinates.
PKDD	Association Rule Selection in a Data Mining Environment.	Mika Klemettinen,Heikki Mannila,A. Inkeri Verkamo	1999	Association Rule Selection in a Data Mining Environment.
PKDD	Combinatorial Approach for Data Binarization.	Eddy Mayoraz,Miguel Moreira	1999	Combinatorial Approach for Data Binarization.
PKDD	Multi-relational Decision Tree Induction.	Arno J. Knobbe,Arno Siebes,Danïel van der Wallen	1999	Multi-relational Decision Tree Induction.
PKDD	The ESPRIT Project CreditMine and Its Relevance for the Internet Market.	Susanne Köhler,Michael Krieger	1999	The ESPRIT Project CreditMine and Its Relevance for the Internet Market.
PKDD	Data Mining for Robust Business Intelligence Solutions.	Jan Mrázek	1999	Data Mining for Robust Business Intelligence Solutions.
PKDD	Learning of Simple Conceptual Graphs from Positive and Negative Examples.	Sergei O. Kuznetsov	1999	Learning of Simple Conceptual Graphs from Positive and Negative Examples.
PKDD	Unsupervised Profiling for Identifying Superimposed Fraud.	Uzi Murad,Gadi Pinkas	1999	Unsupervised Profiling for Identifying Superimposed Fraud.
PKDD	An Evolutionary Algorithm Using Multivariate Discretization for Decision Rule Induction.	Wojciech Kwedlo,Marek Kretowski	1999	An Evolutionary Algorithm Using Multivariate Discretization for Decision Rule Induction.
PKDD	Extending Attribute-Oriented Induction as a Key-Preserving Data Mining Method.	Maybin K. Muyeba,John A. Keane	1999	Extending Attribute-Oriented Induction as a Key-Preserving Data Mining Method.
PKDD	ZigZag, a New Clustering Algorithm to Analyze Categorical Variable Cross-Classification Tables.	Stéphane Lallich	1999	ZigZag, a New Clustering Algorithm to Analyze Categorical Variable Cross-Classification Tables.
PKDD	"An Application of Data Mining to the Problem of the University Students' Dropout Using Markov Chains."	Silvia Massa,Pier Paolo Puliafito	1999	"An Application of Data Mining to the Problem of the University Students' Dropout Using Markov Chains."
PKDD	Automated Discovery of Polynomials by Inductive Genetic Programming.	Nikolay I. Nikolaev,Hitoshi Iba	1999	Automated Discovery of Polynomials by Inductive Genetic Programming.
PKDD	"Experiments on a Representation-Independent ""Top-Down and Prune"" Induction Scheme."	Richard Nock,Marc Sebban,Pascal Jappy	1999	"Experiments on a Representation-Independent ""Top-Down and Prune"" Induction Scheme."
PKDD	Diagnosing Acute Appendicitis with Very Simple Classification Rules.	Aleksander Øhrn,Henryk Jan Komorowski	1999	Diagnosing Acute Appendicitis with Very Simple Classification Rules.
PKDD	Rule Induction in Cascade Model Based on Sum of Squares Decomposition.	Takashi Okada	1999	Rule Induction in Cascade Model Based on Sum of Squares Decomposition.
PKDD	Efficient Mining of High Confidience Association Rules without Support Thresholds.	Jinyan Li,Xiuzhen Zhang,Guozhu Dong,Kotagiri Ramamohanarao,Qun Sun	1999	Efficient Mining of High Confidience Association Rules without Support Thresholds.
PKDD	A Logical Approach to Fuzzy Data Analysis.	Churn-Jung Liau,Duen-Ren Liu	1999	A Logical Approach to Fuzzy Data Analysis.
PKDD	AST: Support for Algorithm Selection with a CBR Approach.	Guido Lindner,Rudi Studer	1999	AST: Support for Algorithm Selection with a CBR Approach.
PKDD	Automated Discovery of Rules and Exeptions from Distributed Databases Using Aggregates.	Rónán Páircéir,Sally I. McClean,Bryan W. Scotney	1999	Automated Discovery of Rules and Exeptions from Distributed Databases Using Aggregates.
PKDD	Efficient Shared Near Neighbours Clustering of Large Metric Data Sets.	Stefano Lodi,Luisella Reami,Claudio Sartori	1999	Efficient Shared Near Neighbours Clustering of Large Metric Data Sets.
PKDD	"Discovery of ""Interesting"" Data Dependencies from a Workload of SQL Statements."	Stéphane Lopes,Jean-Marc Petit,Farouk Toumani	1999	"Discovery of ""Interesting"" Data Dependencies from a Workload of SQL Statements."
PKDD	Maintenance of Discovered Knowledge.	Michal Pechoucek,Olga Stepánková,Petr Miksovský	1999	Maintenance of Discovered Knowledge.
PKDD	A Divise Initialisation Method for Clustering Algorithms.	Clara Pizzuti,Domenico Talia,Giorgio Vonella	1999	A Divise Initialisation Method for Clustering Algorithms.
PKDD	Data Mining for the Web.	Myra Spiliopoulou	1999	Data Mining for the Web.
PKDD	A Comparison of Model Selection Procedures for Predicting Turning Points in Financial Time Series.	Thorsten Poddig,Claus Huber	1999	A Comparison of Model Selection Procedures for Predicting Turning Points in Financial Time Series.
PKDD	The Haar Wavelet Transform in the Time Series Similarity Paradigm.	Zbigniew R. Struzik,Arno Siebes	1999	The Haar Wavelet Transform in the Time Series Similarity Paradigm.
PKDD	Mining Lemma Disambiguation Rules from Czech Corpora.	Lubos Popelínsky,Tomás Pavelek	1999	Mining Lemma Disambiguation Rules from Czech Corpora.
PKDD	Support Vector Machines for Knowledge Discovery.	Shinsuke Sugaya,Einoshin Suzuki,Shusaku Tsumoto	1999	Support Vector Machines for Knowledge Discovery.
PKDD	Relational Learning and Inductive Logic Programming Made Easy Abstract of Tutorial.	Luc De Raedt,Hendrik Blockeel	1999	Relational Learning and Inductive Logic Programming Made Easy Abstract of Tutorial.
PKDD	Adding Temporal Semantics to Association Rules.	Chris P. Rainsford,John F. Roddick	1999	Adding Temporal Semantics to Association Rules.
PKDD	Studying the Behavior of Generalized Entropy in Induction Trees Using a M-of-N Concept.	Ricco Rakotomalala,Stéphane Lallich,S. Di Palma	1999	Studying the Behavior of Generalized Entropy in Induction Trees Using a M-of-N Concept.
PKDD	Mining Text Archives: Creating Readable Maps to Structure and Describe Document Collections.	Andreas Rauber,Dieter Merkl	1999	Mining Text Archives: Creating Readable Maps to Structure and Describe Document Collections.
PKDD	Experiments in Meta-level Learning with ILP.	Ljupco Todorovski,Saso Dzeroski	1999	Experiments in Meta-level Learning with ILP.
PKDD	Neuro-fuzzy Data Mining for Target Group Selection in Retail Banking.	Johannes Ruhland,Thomas Wittmann	1999	Neuro-fuzzy Data Mining for Target Group Selection in Retail Banking.
PKDD	Rule Discovery in Large Time-Series Medical Databases.	Shusaku Tsumoto	1999	Rule Discovery in Large Time-Series Medical Databases.
PKDD	Knowledge Discovery in Medical Multi-databases: A Rough Set Approach.	Shusaku Tsumoto	1999	Knowledge Discovery in Medical Multi-databases: A Rough Set Approach.
PKDD	Mining Possibilistic Set-Valued Rules by Generating Prime Disjunctions.	Alexandr A. Savinov	1999	Mining Possibilistic Set-Valued Rules by Generating Prime Disjunctions.
PKDD	Regression by Feature Projections.	Ilhan Uysal,H. Altay Güvenir	1999	Regression by Feature Projections.
PKDD	Contribution of Boosting in Wrapper Models.	Marc Sebban,Richard Nock	1999	Contribution of Boosting in Wrapper Models.
PKDD	Selection and Statistical Validation of Features and Prototypes.	Marc Sebban,Djamel A. Zighed,S. Di Palma	1999	Selection and Statistical Validation of Features and Prototypes.
PKDD	Managing Interesting Rules in Sequence Mining.	Myra Spiliopoulou	1999	Managing Interesting Rules in Sequence Mining.
PKDD	Boolean Reasoning Scheme with Some Applications in Data Mining.	Andrzej Skowron,Hung Son Nguyen	1999	Boolean Reasoning Scheme with Some Applications in Data Mining.
PKDD	Towards Discovery of Information Granules.	Andrzej Skowron,Jaroslaw Stepaniuk	1999	Towards Discovery of Information Granules.
PKDD	Classification Algorithms Based on Linear Combinations of Features.	Dominik Slezak,Jakub Wroblewski	1999	Classification Algorithms Based on Linear Combinations of Features.
PKDD	Enhancing Rule Interestingness for Neuro-fuzzy Systems.	Thomas Wittmann,Johannes Ruhland,Matthias Eichholz	1999	Enhancing Rule Interestingness for Neuro-fuzzy Systems.
PKDD	Generating Linguistic Fuzzy Rules for Pattern Classification with Genetic Algorithms.	N. Xiong,Lothar Litz	1999	Generating Linguistic Fuzzy Rules for Pattern Classification with Genetic Algorithms.
PKDD	Optimizing Disjunctive Association Rules.	Dmitry Zelenko	1999	Optimizing Disjunctive Association Rules.
PKDD	Peculiarity Oriented Multi-database Mining.	Ning Zhong,Yiyu Yao,Setsuo Ohsuga	1999	Peculiarity Oriented Multi-database Mining.
PKDD	"Principles of Data Mining and Knowledge Discovery, Third European Conference, PKDD '99, Prague, Czech Republic, September 15-18, 1999, Proceedings"	Jan M. Zytkow,Jan Rauch	1999	"Principles of Data Mining and Knowledge Discovery, Third European Conference, PKDD '99, Prague, Czech Republic, September 15-18, 1999, Proceedings"
KDD	Using the fractal dimension to cluster datasets.	Daniel Barbará,Ping Chen	2000	Using the fractal dimension to cluster datasets.
KDD	Depth first generation of long patterns.	Ramesh C. Agarwal,Charu C. Aggarwal,V. V. V. Prasad	2000	Depth first generation of long patterns.
KDD	Multivariate discretization of continuous variables for set mining.	Stephen D. Bay	2000	Multivariate discretization of continuous variables for set mining.
KDD	Automating exploratory data analysis for efficient data mining.	Jonathan D. Becher,Pavel Berkhin,Edmund Freeman	2000	Automating exploratory data analysis for efficient data mining.
KDD	Agglomerative clustering of a search engine query log.	Doug Beeferman,Adam L. Berger	2000	Agglomerative clustering of a search engine query log.
KDD	The IGrid index: reversing the dimensionality curse for similarity indexing in high dimensional space.	Charu C. Aggarwal,Philip S. Yu	2000	The IGrid index: reversing the dimensionality curse for similarity indexing in high dimensional space.
KDD	Visualization and interactive feature selection for unsupervised data.	Jennifer G. Dy,Carla E. Brodley	2000	Visualization and interactive feature selection for unsupervised data.
KDD	Towards an effective cooperation of the user and the computer for classification.	Mihael Ankerst,Martin Ester,Hans-Peter Kriegel	2000	Towards an effective cooperation of the user and the computer for classification.
KDD	Evolutionary algorithms in data mining: multi-objective performance modeling for direct marketing.	Siddhartha Bhattacharyya	2000	Evolutionary algorithms in data mining: multi-objective performance modeling for direct marketing.
KDD	Hardening soft information sources.	William W. Cohen,Henry A. Kautz,David A. McAllester	2000	Hardening soft information sources.
KDD	A data mining framework for optimal product selection in retail supermarket data: the generalized PROFSET model.	Tom Brijs,Bart Goethals,Gilbert Swinnen,Koen Vanhoof,Geert Wets	2000	A data mining framework for optimal product selection in retail supermarket data: the generalized PROFSET model.
KDD	Deformable Markov model templates for time-series pattern matching.	Xianping Ge,Padhraic Smyth	2000	Deformable Markov model templates for time-series pattern matching.
KDD	Informed knowledge discovery: using prior knowledge in discovery programs (invited talk, abstract only).	Bruce G. Buchanan	2000	Informed knowledge discovery: using prior knowledge in discovery programs (invited talk, abstract only).
KDD	Hancock: a language for extracting signatures from data streams.	Corinna Cortes,Kathleen Fisher,Daryl Pregibon,Anne Rogers	2000	Hancock: a language for extracting signatures from data streams.
KDD	Visualization and the process of modeling: a cognitive-theoretic view.	Andrew W. Crapo,Laurie B. Waisel,William A. Wallace,Thomas R. Willemain	2000	Visualization and the process of modeling: a cognitive-theoretic view.
KDD	A general probabilistic framework for clustering individuals and objects.	Igor V. Cadez,Scott Gaffney,Padhraic Smyth	2000	A general probabilistic framework for clustering individuals and objects.
KDD	"E-metrics: tomorrow's business metrics today (invited talk, abstract only)."	Matt Cutler	2000	"E-metrics: tomorrow's business metrics today (invited talk, abstract only)."
KDD	Visualization of navigation patterns on a Web site using model-based clustering.	Igor V. Cadez,David Heckerman,Christopher Meek,Padhraic Smyth,Steven White	2000	Visualization of navigation patterns on a Web site using model-based clustering.
KDD	Discovering similar patterns in time series.	Juan Pedro Caraça-Valente,Ignacio Lopez-Chavarrias	2000	Discovering similar patterns in time series.
KDD	Identifying prospective customers.	Paul B. Chou,Edna Grossman,Dimitrios Gunopulos,Pasumarti Kamesam	2000	Identifying prospective customers.
KDD	Among those dark electronic mills: privacy and data mining (invited talk, abstract only).	Jason Catlett	2000	Among those dark electronic mills: privacy and data mining (invited talk, abstract only).
KDD	Alpha seeding for support vector machines.	Dennis DeCoste,Kiri Wagstaff	2000	Alpha seeding for support vector machines.
KDD	Data mining techniques for optimizing inventories for electronic commerce.	Anjali Dhond,Amar Gupta,Sanjeev Vadhavkar	2000	Data mining techniques for optimizing inventories for electronic commerce.
KDD	Incremental quantile estimation for massive tracking.	Fei Chen,Diane Lambert,José C. Pinheiro	2000	Incremental quantile estimation for massive tracking.
KDD	Mining high-speed data streams.	Pedro Domingos,Geoff Hulten	2000	Mining high-speed data streams.
KDD	Data selection for support vector machine classifiers.	Glenn Fung,Olvi L. Mangasarian	2000	Data selection for support vector machine classifiers.
KDD	Explicitly representing expected cost: an alternative to ROC representation.	Chris Drummond,Robert C. Holte	2000	Explicitly representing expected cost: an alternative to ROC representation.
KDD	Data mining solves tough semiconductor manufacturing problems.	Mike Gardner,Jack Bieker	2000	Data mining solves tough semiconductor manufacturing problems.
KDD	Efficient algorithms for constructing decision trees with constraints.	Minos N. Garofalakis,Dongjoon Hyun,Rajeev Rastogi,Kyuseok Shim	2000	Efficient algorithms for constructing decision trees with constraints.
KDD	Mining the stock market (extended abstract): which measure is best?	Martin Gavrilov,Dragomir Anguelov,Piotr Indyk,Rajeev Motwani	2000	Mining the stock market (extended abstract): which measure is best?
KDD	Predictive modeling in automotive direct marketing: tools, experiences and open issues.	Wendy Gersten,Rüdiger Wirth,Dirk Arndt	2000	Predictive modeling in automotive direct marketing: tools, experiences and open issues.
KDD	Hybrid Poisson process.	Ayman Farahat	2000	Hybrid Poisson process.
KDD	Decision support in the booming e-world (invited talk, abstract only).	James Goodnight	2000	Decision support in the booming e-world (invited talk, abstract only).
KDD	Text mining (workshop session - title only).	Marko Grobelnik,Dunja Mladenic,Natasa Milic-Frayling	2000	Text mining (workshop session - title only).
KDD	A framework for specifying explicit bias for revision of approximate information extraction rules.	Ronen Feldman,Yair Liberzon,Binyamin Rosenfeld,Yonatan Schler,Jonathan Stoppi	2000	A framework for specifying explicit bias for revision of approximate information extraction rules.
KDD	Efficient identification of Web communities.	Gary William Flake,Steve Lawrence,C. Lee Giles	2000	Efficient identification of Web communities.
KDD	Mining IC test data to optimize VLSI testing.	Tony Fountain,Thomas G. Dietterich,Bill Sudyka	2000	Mining IC test data to optimize VLSI testing.
KDD	: a model for visualizing knowledge discovery process.	Jianchao Han,Nick Cercone	2000	: a model for visualizing knowledge discovery process.
KDD	FreeSpan: frequent pattern-projected sequential pattern mining.	Jiawei Han,Jian Pei,Behzad Mortazavi-Asl,Qiming Chen,Umeshwar Dayal,Meichun Hsu	2000	FreeSpan: frequent pattern-projected sequential pattern mining.
KDD	Visualizing association rules with interactive mosaic plots.	Heike Hofmann,Arno Siebes,Adalbert F. X. Wilhelm	2000	Visualizing association rules with interactive mosaic plots.
KDD	Exploration mining in diabetic patients databases: findings and conclusions.	Wynne Hsu,Mong-Li Lee,Bing Liu,Tok Wang Ling	2000	Exploration mining in diabetic patients databases: findings and conclusions.
KDD	Classification and visualization for high-dimensional data.	Alfred Inselberg,Tova Avidan	2000	Classification and visualization for high-dimensional data.
KDD	Active learning using adaptive resampling.	Vijay S. Iyengar,Chidanand Apté,Tong Zhang	2000	Active learning using adaptive resampling.
KDD	Distributed and parallel knowledge discovery (workshop session - title only).	Hillol Kargupta,Philip Chan,Vipin Kumar,Zoran Obradovic	2000	Distributed and parallel knowledge discovery (workshop session - title only).
KDD	Scaling up dynamic time warping for datamining applications.	Eamonn J. Keogh,Michael J. Pazzani	2000	Scaling up dynamic time warping for datamining applications.
KDD	Feature selection in unsupervised learning via evolutionary search.	YongSeog Kim,W. Nick Street,Filippo Menczer	2000	Feature selection in unsupervised learning via evolutionary search.
KDD	Genome scale prediction of protein functional class from sequence using data mining.	Ross D. King,Andreas Karwath,Amanda Clare,Luc Dehaspe	2000	Genome scale prediction of protein functional class from sequence using data mining.
KDD	Cross-sell: a fast promotion-tunable customer-item recommendation method based on conditionally independent probabilities.	Brendan Kitts,David Freed,Martin Vrieze	2000	Cross-sell: a fast promotion-tunable customer-item recommendation method based on conditionally independent probabilities.
KDD	Web mining for e-commerce (workshop session - title only).	Ron Kohavi,Myra Spiliopoulou,Jaideep Srivastava	2000	Web mining for e-commerce (workshop session - title only).
KDD	Unsupervised Bayesian visualization of high-dimensional data.	Petri Kontkanen,Jussi Lahtinen,Petri Myllymäki,Henry Tirri	2000	Unsupervised Bayesian visualization of high-dimensional data.
KDD	IntelliClean: a knowledge-based intelligent data cleaner.	Mong-Li Lee,Tok Wang Ling,Wai Lup Low	2000	IntelliClean: a knowledge-based intelligent data cleaner.
KDD	Multi-level organization and summarization of the discovered rules.	Bing Liu,Minqing Hu,Wynne Hsu	2000	Multi-level organization and summarization of the discovered rules.
KDD	Targeting the right students using data mining.	Yiming Ma,Bing Liu,Ching Kian Wong,Philip S. Yu,Shuik Ming Lee	2000	Targeting the right students using data mining.
KDD	Global partial orders from sequential data.	Heikki Mannila,Christopher Meek	2000	Global partial orders from sequential data.
KDD	Efficient clustering of high-dimensional data sets with application to reference matching.	Andrew McCallum,Kamal Nigam,Lyle H. Ungar	2000	Efficient clustering of high-dimensional data sets with application to reference matching.
KDD	Small is beautiful: discovering the minimal set of unexpected patterns.	Balaji Padmanabhan,Alexander Tuzhilin	2000	Small is beautiful: discovering the minimal set of unexpected patterns.
KDD	Discovery of multi-level rules and exceptions from a distributed database.	Rónán Páircéir,Sally I. McClean,Bryan W. Scotney	2000	Discovery of multi-level rules and exceptions from a distributed database.
KDD	On certain rigorous approaches to data mining (invited talk, abstract only).	Christos H. Papadimitriou	2000	On certain rigorous approaches to data mining (invited talk, abstract only).
KDD	Towards scalable support vector machines using squashing.	Dmitry Pavlov,Darya Chudova,Padhraic Smyth	2000	Towards scalable support vector machines using squashing.
KDD	Can we push more constraints into frequent pattern mining?	Jian Pei,Jiawei Han	2000	Can we push more constraints into frequent pattern mining?
KDD	Data mining to detect abnormal behavior in aerospace data.	José Manuel Peña,Fazel Famili,Sylvain Létourneau	2000	Data mining to detect abnormal behavior in aerospace data.
KDD	Defection detection: using activity profiles to predict ISP customer vulnerability.	Nandini Raghavan,Robert M. Bell,Matthias Schonlau	2000	Defection detection: using activity profiles to predict ISP customer vulnerability.
KDD	A sequential sampling algorithm for a general class of utility criteria.	Tobias Scheffer,Stefan Wrobel	2000	A sequential sampling algorithm for a general class of utility criteria.
KDD	"Ongoing management and application of discovered knowledge in a large regulatory organization: a case study of the use and impact of NASD Regulation's Advanced Detection System (RADS)."	Ted E. Senator	2000	"Ongoing management and application of discovered knowledge in a large regulatory organization: a case study of the use and impact of NASD Regulation's Advanced Detection System (RADS)."
KDD	Multimedia data mining (workshop session - title only).	Simeon J. Simoff,Osmar R. Zaïane	2000	Multimedia data mining (workshop session - title only).
KDD	After the gold rush (invited talk, abstract only): data mining in the new economy (invited talk, abstract only).	David Stodder	2000	After the gold rush (invited talk, abstract only): data mining in the new economy (invited talk, abstract only).
KDD	An empirical analysis of techniques for constructing and searching k-dimensional trees.	Douglas A. Talbert,Douglas H. Fisher	2000	An empirical analysis of techniques for constructing and searching k-dimensional trees.
KDD	Textual data mining of service center call records.	Pang-Ning Tan,Hannah Blau,Steven A. Harp,Robert P. Goldman	2000	Textual data mining of service center call records.
KDD	The generalized Bayesian committee machine.	Volker Tresp	2000	The generalized Bayesian committee machine.
KDD	Application of neural networks to biological data mining: a case study in protein sequence classification.	Jason Tsong-Li Wang,Qicheng Ma,Dennis Shasha,Cathy H. Wu	2000	Application of neural networks to biological data mining: a case study in protein sequence classification.
KDD	Efficient mining of weighted association rules (WAR).	Wei Wang,Jiong Yang,Philip S. Yu	2000	Efficient mining of weighted association rules (WAR).
KDD	Growing decision trees on support-less association rules.	Ke Wang,Senqiang Zhou,Yu He	2000	Growing decision trees on support-less association rules.
KDD	Efficient search for association rules.	Geoffrey I. Webb	2000	Efficient search for association rules.
KDD	Exploring constraints to efficiently mine emerging patterns from large high-dimensional datasets.	Xiuzhen Zhang,Guozhu Dong,Kotagiri Ramamohanarao	2000	Exploring constraints to efficiently mine emerging patterns from large high-dimensional datasets.
KDD	On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms.	Kenji Yamanishi,Jun-ichi Takeuchi,Graham J. Williams,Peter Milne	2000	"Outlier detection is a fundamental issue in data mining, specifically in fraud detection, network intrusion detection, network monitoring, etc. SmartSifter is an outlier detection engine addressing this problem from the viewpoint of statistical learning theory. This paper provides a theoretical basis for SmartSifter and empirically demonstrates its effectiveness. SmartSifter detects outliers in an on-line process through the on-line unsupervised learning of a probabilistic model (using a finite mixture model) of the information source. Each time a datum is input SmartSifter employs an on-line discounting learning algorithm to learn the probabilistic model. A score is given to the datum based on the learned model with a high score indicating a high possibility of being a statistical outlier. The novel features of SmartSifter are: (1) it is adaptive to non-stationary sources of data&semi; (2) a score has a clear statistical/information-theoretic meaning&semi; (3) it is computationally inexpensive&semi; and (4) it can handle both categorical and continuous variables. An experimental application to network intrusion detection shows that SmartSifter was able to identify data with high scores that corresponded to attacks, with low computational costs. Further experimental application has identified a number of meaningful rare cases in actual health insurance pathology data from Australia's Health Insurance Commission."
KDD	Interactive exploration of very large relational datasets through 3D dynamic projections.	Li Yang	2000	Interactive exploration of very large relational datasets through 3D dynamic projections.
KDD	Mining asynchronous periodic patterns in time series data.	Jiong Yang,Wei Wang,Philip S. Yu	2000	Periodicy detection in time series data is a challenging problem of great importance in many applications. Most previous work focused on mining synchronous periodic patterns and did not recognize the misaligned presence of a pattern due to the intervention of random noise. In this paper, we propose a more flexible model of asynchronous periodic pattern that may be present only within a subsequence and whose occurrences may be shifted due to disturbance. Two parameters min_rep and max_dis are employed to specify the minimum number of repetitions that is required within each segment of nondisrupted pattern occurrences and the maximum allowed disturbance between any two successive valid segments. Upon satisfying these two requirements, the longest valid subsequence of a pattern is returned. A two-phase algorithm is devised to first generate potential periods by distance-based pruning followed by an iterative procedure to derive and validate candidate patterns and locate the longest valid subsequence. We also show that this algorithm cannot only provide linear time complexity with respect to the length of the sequence but also achieve space efficiency.
KDD	A classifier for semi-structured documents.	Jeonghee Yi,Neel Sundaresan	2000	A classifier for semi-structured documents.
KDD	Generating non-redundant association rules.	Mohammed Javeed Zaki	2000	Generating non-redundant association rules.
KDD	Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, Boston, MA, USA, August 20-23, 2000	Raghu Ramakrishnan,Salvatore J. Stolfo,Roberto J. Bayardo,Ismail Parsa	2000	Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, Boston, MA, USA, August 20-23, 2000
PKDD	Image Access and Data Mining: An Approach.	Chabane Djeraba	2000	Image Access and Data Mining: An Approach.
PKDD	Decision Tree Toolkit: A Component-Based Library of Decision Tree Algorithms.	Nikos Drossos,Athanassios Papagelis,Dimitrios Kalles	2000	Decision Tree Toolkit: A Component-Based Library of Decision Tree Algorithms.
PKDD	Learning Right Sized Belief Networks by Means of a Hybrid Methodology.	Silvia Acid,Luis M. de Campos	2000	Learning Right Sized Belief Networks by Means of a Hybrid Methodology.
PKDD	A Genetic Algorithm-Based Solution for the Problem of Small Disjuncts.	Deborah R. Carvalho,Alex Alves Freitas	2000	A Genetic Algorithm-Based Solution for the Problem of Small Disjuncts.
PKDD	Algorithms for Mining Share Frequent Itemsets Containing Infrequent Subsets.	Brock Barber,Howard J. Hamilton	2000	Algorithms for Mining Share Frequent Itemsets Containing Infrequent Subsets.
PKDD	Discovering Task Neighbourhoods Through Landmark Learning Performances.	Hilan Bensusan,Christophe G. Giraud-Carrier	2000	Discovering Task Neighbourhoods Through Landmark Learning Performances.
PKDD	Inductive Logic Programming in Clementine.	Sam Brewer,Tom Khabaza	2000	Inductive Logic Programming in Clementine.
PKDD	Sampling Strategies for Targeting Rare Groups from a Bank Customer Database.	Jean-Hugues Chauchat,Ricco Rakotomalala,Didier Robert	2000	Sampling Strategies for Targeting Rare Groups from a Bank Customer Database.
PKDD	Basis of Fuzzy Knowledge Discovery System.	Maurice Bernadet	2000	Basis of Fuzzy Knowledge Discovery System.
PKDD	Clustering Large, Multi-level Data Sets: An Apporach Based on Kohonen Self Organizing Maps.	Antonio Ciampi,Yves Lechevallier	2000	Clustering Large, Multi-level Data Sets: An Apporach Based on Kohonen Self Organizing Maps.
PKDD	Trees and Induction Graphs for Multivariate Response.	Antonio Ciampi,Djamel A. Zighed,Jérémy Clech	2000	Trees and Induction Graphs for Multivariate Response.
PKDD	Induction of Multivariate Decision Trees by Using Dipolar Criteria.	Leon Bobrowski,Marek Kretowski	2000	Induction of Multivariate Decision Trees by Using Dipolar Criteria.
PKDD	CEM-Visualisation and Discovery in Email.	Richard J. Cole II,Peter W. Eklund,Gerd Stumme	2000	CEM-Visualisation and Discovery in Email.
PKDD	Approximation of Frequency Queris by Means of Free-Sets.	Jean-François Boulicaut,Artur Bykowski,Christophe Rigotti	2000	Approximation of Frequency Queris by Means of Free-Sets.
PKDD	Context-Based Similarity Measures for Categorical Databases.	Gautam Das,Heikki Mannila	2000	Context-Based Similarity Measures for Categorical Databases.
PKDD	Fast Hierarchical Clustering Based on Compressed Data and OPTICS.	Markus M. Breunig,Hans-Peter Kriegel,Jörg Sander	2000	Fast Hierarchical Clustering Based on Compressed Data and OPTICS.
PKDD	Application of Reinforcement Learning to Electrical Power System Closed-Loop Emergency Control.	Christophe Druet,Damien Ernst,Louis Wehenkel	2000	Application of Reinforcement Learning to Electrical Power System Closed-Loop Emergency Control.
PKDD	Determination of Screening Descriptors for Chemical Reaction Databases.	Laurent Dury,Laurence Leherte,Daniel P. Vercauteren	2000	Determination of Screening Descriptors for Chemical Reaction Databases.
PKDD	Prior Knowledge in Economic Applications of Data Mining.	A. J. Feelders	2000	Prior Knowledge in Economic Applications of Data Mining.
PKDD	Observational Logic Integrates Data Mining Based on Statistics and Neural Networks.	Martin Holena	2000	Observational Logic Integrates Data Mining Based on Statistics and Neural Networks.
PKDD	Supporting Discovery in Medicine by Association Rule Mining of Bibliographic Databases.	Dimitar Hristovski,Saso Dzeroski,Borut Peterlin,Anamarija Rozic-Hristovski	2000	Supporting Discovery in Medicine by Association Rule Mining of Bibliographic Databases.
PKDD	Confirmation Rule Sets.	Dragan Gamberger,Nada Lavrac	2000	Confirmation Rule Sets.
PKDD	Some Enhencements of Decision Tree Bagging.	Pierre Geurts	2000	Some Enhencements of Decision Tree Bagging.
PKDD	An Apriori-Based Algorithm for Mining Frequent Substructures from Graph Data.	Akihiro Inokuchi,Takashi Washio,Hiroshi Motoda	2000	An Apriori-Based Algorithm for Mining Frequent Substructures from Graph Data.
PKDD	Temporal Machine Learning for Switching Control.	Pierre Geurts,Louis Wehenkel	2000	Temporal Machine Learning for Switching Control.
PKDD	Improving Dissimilarity Functions with Domain Knowledge, Applications with IKBS System.	David Grosser,Jean Diatta,Noël Conruyt	2000	Improving Dissimilarity Functions with Domain Knowledge, Applications with IKBS System.
PKDD	Mining Weighted Association Rules for Fuzzy Quantitative Items.	Attila Gyenesei	2000	During the last ten years, data mining, also known as knowledge discovery in databases, has established its position as a prominent and important research area. Mining association rules is one of the important research problems in data mining. Many algorithms have been proposed to find association rules in large databases containing both categorical and quantitative attributes. We generalize this to the case where part of attributes are given weights to reflect their importance to the user. In this paper, we introduce the problem of mining weighted quantitative association rules based on fuzzy approach. Using the fuzzy set concept, the discovered rules are more understandable to a human. We propose two different definitions of weighted support: with and with- out normalization. In the normalized case, a subset of a frequent itemset may not be frequent, and we cannot generate candidate k-itemsets simply from the frequent (k-1)-itemsets. We tackle this problem by using the concept of z-potential frequent subset for each candidate itemset. We give an algorithm for mining such quantitative association rules. Fi- nally, we describe the results of using this approach on a real-life dataset.
PKDD	Collective Principal Component Analysis from Distributed, Heterogeneous Data.	Hillol Kargupta,Weiyun Huang,Krishnamoorthy Sivakumar,Byung-Hoon Park,Shuren Wang	2000	Collective Principal Component Analysis from Distributed, Heterogeneous Data.
PKDD	Hierarchical Document Clustering Based on Tolerance Rough Set Model.	Saori Kawasaki,Ngoc Binh Nguyen,Tu Bao Ho	2000	Hierarchical Document Clustering Based on Tolerance Rough Set Model.
PKDD	Application of Data-Mining and Knowledge Discovery in Automotive Data Engineering.	Jörg Keller,Valerij Bauer,Wojciech Kwedlo	2000	Application of Data-Mining and Knowledge Discovery in Automotive Data Engineering.
PKDD	Quality Scheme Assessment in the Clustering Process.	Maria Halkidi,Michalis Vazirgiannis,Yannis Batistakis	2000	Quality Scheme Assessment in the Clustering Process.
PKDD	Centroid-Based Document Classification: Analysis and Experimental Results.	Eui-Hong Han,George Karypis	2000	Centroid-Based Document Classification: Analysis and Experimental Results.
PKDD	"Mining Association Rules: Deriving a Superior Algorithm by Analyzing Today's Approaches."	Jochen Hipp,Ulrich Güntzer,Gholamreza Nakhaeizadeh	2000	"Mining Association Rules: Deriving a Superior Algorithm by Analyzing Today's Approaches."
PKDD	Quantifying the Resilience of Inductive Classification Algorithms.	Melanie Hilario,Alexandros Kalousis	2000	Quantifying the Resilience of Inductive Classification Algorithms.
PKDD	Applying Objective Interestingness Measures in Data Mining Systems.	Robert J. Hilderman,Howard J. Hamilton	2000	Applying Objective Interestingness Measures in Data Mining Systems.
PKDD	Web Usage Mining: How to Efficiently Manage New Transactions and New Clients.	Florent Masseglia,Pascal Poncelet,Maguelonne Teisseire	2000	Web Usage Mining: How to Efficiently Manage New Transactions and New Clients.
PKDD	Multi-Relational Data Mining, Using UML for ILP.	Arno J. Knobbe,Arno Siebes,Hendrik Blockeel,Danïel van der Wallen	2000	Multi-Relational Data Mining, Using UML for ILP.
PKDD	Mining Relational Databases.	Frédéric Moal,Teddy Turmeaux,Christel Vrain	2000	Mining Relational Databases.
PKDD	Mining Generalized Multiple-Level Association Rules.	Show-Jane Yen	2000	Mining Generalized Multiple-Level Association Rules.
PKDD	Towards Knowledge Discovery from cDNA Microarray Gene Expression Data.	Henryk Jan Komorowski,Torgeir R. Hvidsten,Tor-Kristian Jenssen,Dyre Tjeldvoll,Eivind Hovig,Arne K. Sandvik,Astrid Lægreid	2000	Towards Knowledge Discovery from cDNA Microarray Gene Expression Data.
PKDD	Materialized Data Mining Views.	Tadeusz Morzy,Marek Wojciechowski,Maciej Zakrzewicz	2000	Materialized Data Mining Views.
PKDD	Mining with Cover and Extension Operators.	Marzena Kryszkiewicz	2000	Mining with Cover and Extension Operators.
PKDD	Learning Dynamic Bayesian Belief Networks Using Conditional Phase-Type Distributions.	Adele H. Marshall,Sally I. McClean,Mary Shapcott,Peter H. Millard	2000	Learning Dynamic Bayesian Belief Networks Using Conditional Phase-Type Distributions.
PKDD	A User-Driven Process for Mining Association Rules.	Pascale Kuntz,Fabrice Guillet,Rémi Lehn,Henri Briand	2000	A User-Driven Process for Mining Association Rules.
PKDD	Efficient Score-Based Learning of Equivalence Classes of Bayesian Networks.	Paul Munteanu,Denis Cau	2000	Efficient Score-Based Learning of Equivalence Classes of Bayesian Networks.
PKDD	Interestingness in Attribute-Oriented Induction (AOI): Multiple-Level Rule Generation.	Maybin K. Muyeba,John A. Keane	2000	Interestingness in Attribute-Oriented Induction (AOI): Multiple-Level Rule Generation.
PKDD	Fast Feature Selection Using Partial Correlation for Multi-vaslued Attributes.	Stéphane Lallich,Ricco Rakotomalala	2000	Fast Feature Selection Using Partial Correlation for Multi-vaslued Attributes.
PKDD	Learning from Labeled and Unlabeled Documents: A Comparative Study on Semi-Supervised Text Classification.	Carsten Lanquillon	2000	Learning from Labeled and Unlabeled Documents: A Comparative Study on Semi-Supervised Text Classification.
PKDD	Schema Mining: Finding Structural Regularity among Semistructured Data.	Pierre-Alain Laur,Florent Masseglia,Pascal Poncelet	2000	Schema Mining: Finding Structural Regularity among Semistructured Data.
PKDD	MSTS: A System for Mining Sets of Time Series.	Georg Lausen,Iztok Savnik,Aldar Dougarjapov	2000	MSTS: A System for Mining Sets of Time Series.
PKDD	A Mixed Similarity Measure in Near-Linear Computational Complexity for Distance-Based Methods.	Ngoc Binh Nguyen,Tu Bao Ho	2000	A Mixed Similarity Measure in Near-Linear Computational Complexity for Distance-Based Methods.
PKDD	Instance-Based Classification by Emerging Patterns.	Jinyan Li,Guozhu Dong,Kotagiri Ramamohanarao	2000	Instance-Based Classification by Emerging Patterns.
PKDD	Discovery of Characteristic subgraph Patterns Using Relative Indexing and the Cascade Model.	Takashi Okada,Mayumi Oyama	2000	Discovery of Characteristic subgraph Patterns Using Relative Indexing and the Cascade Model.
PKDD	Transparency and Predicive Power: Explaining Complex Classification Models.	Gerhard Paass,Jörg Kindermann	2000	Transparency and Predicive Power: Explaining Complex Classification Models.
PKDD	Improving an Association Rule Based Classifier.	Bing Liu,Yiming Ma,Ching Kian Wong	2000	Improving an Association Rule Based Classifier.
PKDD	Relative Unsupervised Discretization for Association Rule Mining.	Marcus-Christopher Ludl,Gerhard Widmer	2000	Relative Unsupervised Discretization for Association Rule Mining.
PKDD	Clustering Distributed Homogeneous Datasets.	Srinivasan Parthasarathy,Mitsunori Ogihara	2000	Clustering Distributed Homogeneous Datasets.
PKDD	Discovery of Generalized Association Rules with Multiple Minimum Supports.	Chung-Leung Lui,Korris Fu-Lai Chung	2000	Discovery of Generalized Association Rules with Multiple Minimum Supports.
PKDD	Empirical Evaluation of Feature Subset Selection Based on a Real-World Data Set.	Petra Perner,Chidanand Apté	2000	Empirical Evaluation of Feature Subset Selection Based on a Real-World Data Set.
PKDD	Zoomed Ranking: Selection of Classification Algorithms Based on Relevant Performance Information.	Carlos Soares,Pavel Brazdil	2000	Zoomed Ranking: Selection of Classification Algorithms Based on Relevant Performance Information.
PKDD	Unified Algorithm for Undirected Discovery of Execption Rules.	Einoshin Suzuki,Jan M. Zytkow	2000	Unified Algorithm for Undirected Discovery of Execption Rules.
PKDD	Discovery of Ambiguous Patterns in Sequences: Application to Bioinformatics.	Gérard Ramstein,Pascal Bunelle,Yannick Jacques	2000	Discovery of Ambiguous Patterns in Sequences: Application to Bioinformatics.
PKDD	Supporting Case Acquisition and Labelling in the Cotext of Web Mining.	Vojtech Svátek,Martin Kavalec	2000	Supporting Case Acquisition and Labelling in the Cotext of Web Mining.
PKDD	Action-Rules: How to Increase Profit of a Company.	Zbigniew W. Ras,Alicja Wieczorkowska	2000	Action-Rules: How to Increase Profit of a Company.
PKDD	Indirect Association: Mining Higher Order Dependencies in Data.	Pang-Ning Tan,Vipin Kumar,Jaideep Srivastava	2000	Indirect Association: Mining Higher Order Dependencies in Data.
PKDD	Discovering Association Rules in Large, Dense Databases.	Tudor Teusan,Gilles Nachouki,Henri Briand,Jacques Philippe	2000	Discovering Association Rules in Large, Dense Databases.
PKDD	Aggregation and Association in Cross Tables.	Gilbert Ritschard,Nicolas Nicoloyannis	2000	Aggregation and Association in Cross Tables.
PKDD	Combining Multiple Models with Meta Decision Trees.	Ljupco Todorovski,Saso Dzeroski	2000	Combining Multiple Models with Meta Decision Trees.
PKDD	An Experimental Study of Partition Quality Indices in Clustering.	Céline Robardet,Fabien Feschet,Nicolas Nicoloyannis	2000	An Experimental Study of Partition Quality Indices in Clustering.
PKDD	Learning First Order Logic Time Series Classifiers: Rules and Boosting.	Juan J. Rodríguez Diez,Carlos Alonso González,Henrik Boström	2000	Learning First Order Logic Time Series Classifiers: Rules and Boosting.
PKDD	Predictive Performance of Weghted Relative Accuracy.	Ljupco Todorovski,Peter A. Flach,Nada Lavrac	2000	Predictive Performance of Weghted Relative Accuracy.
PKDD	Expert Constrained Clustering: A Symbolic Approach.	Fabrice Rossi,Frédérick Vautrain	2000	Expert Constrained Clustering: A Symbolic Approach.
PKDD	Discovering Differences in Patients with Uveitis Through Typical Testors by Class.	José Francisco Martínez Trinidad,Miriam Velasco-Sánchez,Edgar E. Contreras-Aravelo	2000	Discovering Differences in Patients with Uveitis Through Typical Testors by Class.
PKDD	Providing Advice to Website Designers Towards Effective Websites Re-Organization.	Peter Tselios,Agapios Platis,George A. Vouros	2000	Providing Advice to Website Designers Towards Effective Websites Re-Organization.
PKDD	Clinical Knowledge Discovery in Hospital Information Systems: Two Case Studies.	Shusaku Tsumoto	2000	Clinical Knowledge Discovery in Hospital Information Systems: Two Case Studies.
PKDD	An Application of Association Rules Discovery to Geographic Information Systems.	Ansaf Salleb,Christel Vrain	2000	An Application of Association Rules Discovery to Geographic Information Systems.
PKDD	Bagging and Boosting with Dynamic Integration of Classifiers.	Alexey Tsymbal,Seppo Puuronen	2000	Bagging and Boosting with Dynamic Integration of Classifiers.
PKDD	Algorithm for Matching Sets of Time Series.	Iztok Savnik,Georg Lausen,Hans-Peter Kahle,Heinrich Spiecker,Sebastian Hein	2000	Algorithm for Matching Sets of Time Series.
PKDD	Contribution of Dataset Reduction Techniques to Tree-Simplification and Knowledge Discovery.	Marc Sebban,Richard Nock	2000	Contribution of Dataset Reduction Techniques to Tree-Simplification and Knowledge Discovery.
PKDD	Knowledge Discovery Using Least Squares Support Vector Machine Classifiers: A Direct Marketing Case.	Stijn Viaene,Bart Baesens,Tony Van Gestel,Johan A. K. Suykens,Dirk Van den Poel,Jan Vanthienen,Bart De Moor,Guido Dedene	2000	Knowledge Discovery Using Least Squares Support Vector Machine Classifiers: A Direct Marketing Case.
PKDD	Generalized Entropy and Projection Clustering of Categorical Data.	Dan A. Simovici,Dana Cristofor,Laurentiu Cristofor	2000	Generalized Entropy and Projection Clustering of Categorical Data.
PKDD	Leightweight Document Clustering.	Sholom M. Weiss,Brian F. White,Chidanand Apté	2000	Leightweight Document Clustering.
PKDD	Automatic Category Structure Generation and Categorization of Chinese Text Documents.	Hsin-Chang Yang,Chung-Hong Lee	2000	Automatic Category Structure Generation and Categorization of Chinese Text Documents.
PKDD	An Efficient Approach to Discovering Sequential Patterns in Large Databases.	Show-Jane Yen,Chung-Wen Cho	2000	An Efficient Approach to Discovering Sequential Patterns in Large Databases.
PKDD	Accurate Recasting of Parameter Estimation Algorithms Using Sufficient Statistics for Efficient Parallel Speed-Up: Demonstrated for Center-Based Data Clustering Algorithms.	Bin Zhang,Meichun Hsu,George Forman	2000	Accurate Recasting of Parameter Estimation Algorithms Using Sufficient Statistics for Efficient Parallel Speed-Up: Demonstrated for Center-Based Data Clustering Algorithms.
PKDD	Using Background Knowledge as a Bias to Control the Rule Discovery Process.	Ning Zhong,Juzhen Dong,Setsuo Ohsuga	2000	Using Background Knowledge as a Bias to Control the Rule Discovery Process.
PKDD	Principles of Data Mining and Knowledge Discovery, 4th European Conference, PKDD 2000, Lyon, France, September 13-16, 2000, Proceedings	Djamel A. Zighed,Henryk Jan Komorowski,Jan M. Zytkow	2000	Principles of Data Mining and Knowledge Discovery, 4th European Conference, PKDD 2000, Lyon, France, September 13-16, 2000, Proceedings
ICDM	Distributed Web Mining Using Bayesian Networks from Multiple Data Streams.	R. Chen,Krishnamoorthy Sivakumar,Hillol Kargupta	2001	We present a collective approach to mine Bayesian net-works from distributed heterogenous web-log data streams. In this approach we first learn a local Bayesian network at each site using the local data. Then each site identifies the observations that are most likely to be evidence of coupling between local and non-local variables and transmits asub-set of these observations to a central site. Another Bayesian network is learnt at the central site using the data transmittedfrom the local site. The local and central Bayesian networks are combined to obtain a collective Bayesian net-work, that models the entire data. We applied this techniqueto mine multiple data streams where data centralization is difficult because of large response time and scalability issues.Experimental results and theoretical justification that demonstrate the feasibility of our approach are presented.
ICDM	A Hypergraph Based Clustering Algorithm for Spatial Data Sets.	Jong-Sheng Cherng,Mei-Jung Lo	2001	Clustering is a discovery process in data mining an can be used to group together the objects of a database into meaningful subclasses which serve as the foundation for other data analysis techniques.In this paper, we focus on dealing with a set of spatial data. For the spatial data, the clustering problem becomes that of finding the densely populate regions of the space and thus grouping these regions into clusters such that the intracluster similarity is maximized and theintercluster similarity is minimized. We develop a novel hierarchical clustering algorithm that uses a hypergraph to represent a set of spatial data. This hypergraph is initially constructed from the Delaunay triangulation graph of the data set and can correctly capture the relationships among sets of data points. Two phases are developed for the proposed clustering algorithm to find the clusters in the data set.We evaluate our hierarchical clustering algorithm with some spatial data sets in which contain clusters of different sizes, shapes, densities, and noise. Experimental results on these data sets are very encouraging.
ICDM	On Effective Conceptual Indexing and Similarity Search in Text Data.	Charu C. Aggarwal,Philip S. Yu	2001	Similarity search in text has proven to be an interesting problem from the qualitative perspective because of inherent redundancies and ambiguities in textual descriptions. The methods used in search engines in order to retrieve documents most similar to user-defined sets of keywords are not applicable to targets which are medium to large size documents, because of even greater noise effects stemming from the presence of a large number of words unrelated to the overall topic in the document. The inverted representation is the dominant method for indexing text, but it is not as suitable for document-to-document similarity search, as for short user-queries. One way of improving the quality of similarity search is Latent Semantic Indexing (LSI), which maps the documents from the original set of words to a concept space. U fortunately, LSI maps the data into a domain in which it is not possible to provide effectiveindexing techniques. In this paper, we investigate new ways of providing conceptual search among documents bycreating a representation in terms of conceptual word-chains. This technique also allows effective indexing techniques so that similarity queries ca be performed on large collectionsof documents by accessing a small amount of data. We demonstrate that our scheme outperforms standard textual similarity search o the inverted representation both in terms of quality a d search efficiency.
ICDM	Efficient Determination of Dynamic Split Points in a Decision Tree.	David Maxwell Chickering,Christopher Meek,Robert Rounthwaite	2001	We consider the problem of choosing split points forcontinuous predictor variables in a decision tree. Previousapproaches to this problem typically either (1) discretize the continuous predictor values prior to learning or (2) apply a dynamic method that considers all possible split points for each potential split. In this paper, we describe anumber of alternative approaches that generate a smallnumber of candidate split points dynamically with littleoverhead. We argue that these approaches are preferable to pre-discretization, and provide experimental evidence that they yield probabilistic decision trees with the same prediction accuracy as the traditional dynamic approach.Furthermore, because the time to grow a decision tree isproportional to the number of split points evaluated, our approach is significantly faster than the traditional dynamic approach.
ICDM	Efficient Splitting Rules Based on the Probabilities of Pre-assigned Intervals.	June-Suh Cho,Nabil R. Adam	2001	This paper describes new methods for classification in orderto find an optimal tree. Unlike the current splitting rules that areprovided by searching all threshold values, this paper proposes thesplitting rules that are based on the probabilities of pre-assignedintervals.
ICDM	Discovering Similar Patterns for Characterising Time Series in a Medical Domain.	Fernando Alonso,Juan Pedro Caraça-Valente,Loïc Martínez,César Montes	2001	In this article, we describe the process of discovering similar patterns in time series and creating reference models for population groups in a medical domain, and particularly in the field of physiotherapy, using data mining techniques on a set of isokinetic data. The discovered knowledge was evaluated against the expertise of a physician specialized in isokinetic techniques, and applied in the I4 (Intelligent Interpretation of Isokinetic Information) project developed in conjunction with the Spanish National Center for Sports Research and Sciences for muscular diagnosis and rehabilitation, injury prevention, training evaluation and planning, etc., of elite athletes and ordinary people.
ICDM	Comparisons of Classification Methods for Screening Potential Compounds.	Aijun An,Yuanyuan Wang	2001	Comparisons of Classification Methods for Screening Potential Compounds.
ICDM	Knowledge Discovery from Diagrammatically Represented Data.	Michael Anderson	2001	Knowledge discover from diagrammatic data can be facilitated by a language that permits queries on such data.Such a language (diagrammatic SQL) is being developed to expedite the development of an autonomous artificially intelligent agent with a capacity to deal with diagrammatic information.This language is described and examples of how it can be used to facilitatediagrammatic data mining are detaled
ICDM	Integrating E-Commerce and Data Mining: Architecture and Challenges.	Suhail Ansari,Ron Kohavi,Llew Mason,Zijian Zheng	2001	We show that the e-commerce domain can provide all the right ingredients for successful data mining. We describe an integrate architecture for supporting this integration. Thearchitecture can dramatically reduce the pre-processing, cleaning, and data understanding effort often documented to take 80%of the time in knowledge discovery projects. We emphasize the need for data collection at the application server layer (not the web server)in order to support logging of data and metadata that is essential to the discovery process. We describe the datatransformation bridges require from the transaction processing systems an customer event streams (e.g.,clickstreams) to the data warehouse. We detail the mining workbench, which needs to provide multiple views of the data through reporting, data mining algorithms, visualization, and OLAP. We conclude with a set of challenges.
ICDM	Classification with Degree of Membership: A Fuzzy Approach.	Wai-Ho Au,Keith C. C. Chan	2001	algorithms adopt either a decision tree based approach or an approach that requires users to provide some user-specifiedthresholds to guide the search for interesting rules. In this paper, we propose a new approach based on the use of an objective interestingness measure todistinguish interesting rules from uninteresting ones. Using linguistic terms to represent the revealed regularities and exceptions, this approach s especially useful when the discovered rules are presented to human experts for examination because of the affinity with thehuman knowledge representation. The use of fuzzy technique allows the predict on of attribute values to be associated with degree of membership. Our approach s, therefore, able to deal with the cases that an object can belong to more than one class. For example, a person can suffer from cold and fever to certain extent at the same time. Furthermore, our approach is more resilient to noise and missing data values because of the use of fuzzy technique. To evaluate the performance of our approach, we tested it using several real-life databases. The experimental results show that it can be very effective at data mining tasks. In fact, when compared to popular data mining algorithms, our approach can be better ableto uncover useful rules hidden in databases.
ICDM	Heuristic Optimization for Decentralized Frequent Itemset Counting.	Viviane Crestana-Jensen,Nandit Soparkar	2001	The choices for mining of decentralized data are numerous, and we have developed techniques to enumerate andoptimize decentralized frequent itemset counting. In thispaper, we introduce our heuristic approach to improve theperformance of such techniques developed in ways similarto query processing in database systems. We also describeempirical results that validate our heuristic techniques.
ICDM	Provably Fast Training Algorithms for Support Vector Machines.	José L. Balcázar,Yang Dai,Osamu Watanabe	2001	Support Vector Machines are a family of data analysis algorithms, based on convex Quadratic Programming. We focus on their use for classification that case the SVM algorithms work by maximizing the margin of a classifying hyperplane in a feature space. The feature space is handled by means of kernels f the problems are formulated in dual form. Random Sampling techniques successfully used for similar problems are studied here. The main contribute onis a random zed algorithm for training SVMs for which we can formally prove an upper bound on the expected running time that is quasilinear on the number of data points. To ourknowledge, this is the first algorithm for training SVMs in dual formulation and with kernels for which such a quasi-linear time bound has been formally proved.
ICDM	Inexact Field Learning: An Approach to Induce High Quality Rules from Low Quality Data.	Honghua Dai,Xiaoshu Hang,Gang Li	2001	To avoid low quality problem caused by low quality data, this paper introduces an inexactfield learning approach which derives rules by working on the fields of attributes with respect to classes, rather than on individual point values of attributes. The experimental results show that field learning achieved a higher prediction accuracy rate on new unseen test cases which is particularly true when the learning is performed on large low qualitydata.
ICDM	Efficient Yet Accurate Clustering.	Manoranjan Dash,Kian-Lee Tan,Huan Liu	2001	In this paper we show that most hierarchical agglomerativeclustering (HAC)algorithms follow a 90-10 rule where roughly 90%iterations from the beginning merge cluster pairs with dissimilarity less than 10%of the maximumdissimilarity. We propose two algorithms - 2-phase andnested - based on partially overlapping partitioning (POP).To handle high-dimensional data efficiently, we propose a tree structure particularly suitable for POP. Extensive experimentsshow that the proposed algorithms reduce the time andmemory requirement of existing HAC algorithms significantly without compromising in accuracy.
ICDM	Using Rough Sets Theory and Database Operations to Construct a Good Ensemble of Classifiers for Data Mining Applications.	Xiaohua Hu	2001	In this paper we present a new approach to construct a good ensemble of classifiers using rough sets theory and database operations. Ensembles of classifiers is formulated precisely within the framework of rough sets theory and constructed very efficiently by using set-oriented database operations. Our method first computes a set of reductswhich include all the indispensable attributes required for the decision categories. For each reduct, a reduct table is generated by removing those attributes which are not in the reduct. Next, a novel rule induction algorithm is used to compute the maximal generalized rules for each reducttable and a set of reduct classifiers is formed based on thecorresponding reducts. The distinctive features of our method as compared to other methods of constructing ensembles of classifiers are:(1) present a theoretical model to explain the mechanism of constructing ensemble of classifiers, (2) each reduct is a minimum subset of attributes, has the same classification ability as the entire attributes,(3)ea h reduct classifier constructed from the corresponding reduct has a minimal set of classification rules, and is as accurate andcomplete as possible and at the same time as diverse as possible from the other classifiers, (4)the test indicates that the number of classifiers used to improve the accuracy is muchless than other methods
ICDM	Who Links to Whom: Mining Linkage between Web Sites.	Krishna Bharat,Bay-Wei Chang,Monika Rauch Henzinger,Matthias Ruhl	2001	"Previous studies of the web graph structure have focused on the graph structure at the level of individual pages. In actuality the web is a hierarchically nested graph, with domains, hosts and web sites introducing intermediate levels of affiliation and administrativecontrol. To better understand the growth of the web we need to understand its macro-structure, in terms of the linkage between web sites. In this paper e approximate this by studying the graph of the linkage between hosts on the web. This as done based on snapshots of the web taken by Google in Oct 1999,Aug 2000 and Jun 2001.The connectivity between hosts is represented by a directed graph, with hosts as nodes and weighted edges representingthe count of hyperlinks between pages on the corresponding hosts. We demonstrate how such a ""hostgraph"" an be used to study connectivity properties of hosts and domains over time, anddiscuss a modified ""copy model"" too explain observed link eight distributions as a function of subgraph size. We discuss changes in the web over time in the size and connectivity of web sites and country domains. We also describe a data mining application of the hostgraph: a related host finding algorithm which achieves a precision of 0.65 at rank 3."
ICDM	Better Rules, Few Features: A Semantic Approach to Selecting Features from Text.	Catherine Blake,Wanda Pratt	2001	The choice of features used to represent a domain has a profound effect on the quality of the model produced; yet, few researchers have investigated the relationship between the features used to represent text and the quality of the final model. We explored this relationship formedical texts by comparing association rules based on features with three different semantic levels: (1) words (2) manually assigned keywords and (3) automatically selected medical concepts. Our preliminary findings indicate that bi-directional association rules based onconcepts or keywords are more plausible and more useful than those based on word features. The concept and keyword representations also required 90% fewer features than the word representation. This drastic dimensionality reduction suggests that this approach is well suited to large textual corpus of medical text, such as parts of the Web.
ICDM	A Min-max Cut Algorithm for Graph Partitioning and Data Clustering.	Chris H. Q. Ding,Xiaofeng He,Hongyuan Zha,Ming Gu,Horst D. Simon	2001	An important application of graph partitioning is data clustering using a graph model - the pairwise similarities between all data objects form a weighted graph adjacency matrix that contains all necessary information for clustering. Here we propose a new algorithm for graph partition with an objective function that follows the min-max clustering principle. The relaxed version of the optimization of the min-max cut objective function leads to the Fiedler vector in spectral graph partition. Theoretical analyses of min-max cut indicate that it leads to balanced partitions, and lower bonds are derived. The min-max cut algorithm is tested on news-group datasets and is found to outperform other current popular partitioning/clustering methods. The linkage-based refinements in the algorithm further improve the quality of clustering substantially. We also demonstrate that the linearized search order based on linkage differential is better than that based on the Fiedler vector, providing another effectivepartition method.
ICDM	Incremental Support Vector Machine Construction.	Carlotta Domeniconi,Dimitrios Gunopulos	2001	SVMs suffer from the problem of large memory requirement and CPU time when trained in batch mode on large data sets. We overcome these limitations, and at the same time make SVMs suitable for learning with data streams, by constructing incremental learning algorithms.We first introduce and compare different incremental learning techniques, and show that they are capable of producing performance results similar to the batch algorithm, and in some cases superior condensation properties. We then consider the problem of training SVMs using stream data. Our objective is to maintain an updated representation of recent batches of data. We apply incremental schemes to the problem and show that their accuracy is comparable to the batch algorithm.
ICDM	Significance Tests for Patterns in Continuous Data.	Richard J. Bolton,David J. Hand	2001	In this paper we consider the question of uncertainty of detected patterns in data mining. In particular, we develop statistical tests for patterns found in continuous data, indicating the significance of these patterns in terms of the probability that they have occurred by chance. We examine the performance of these tests on patterns detected in several large data sets, including a data set describing the locations of earthquakes in California and another describing flow cytometry measurements on phytoplankton.
ICDM	Preprocessing Opportunities in Optimal Numerical Range Partitioning.	Tapio Elomaa,Juho Rousu	2001	We show that only the segment borders have to be taken into account as cut point candidates in searching for theoptimal multisplit of a numerical value range with respect to convex attribute evaluation functions. Segment borders can be found efficiently in a linear-time preprocessing step. With Training Set Error, which is not strictly convex, the data can be preprocessed into an even smaller number of cut point candidates, called alternations, when striving for the optimal partition. We show that no segment borders(resp. alternations) can be overlooked with strictly convex functions (resp. Training Set Error) without risking to lose optimality. Our experiments show that while in real-world domainssignificant reduction in the number of cut point candidates can be obtained for Training Set Error, the number of segment borders is usually not much lower than that of boundary points.
ICDM	Creating Ensembles of Classifiers.	Nitesh V. Chawla,Steven Eschrich,Lawrence O. Hall	2001	Ensembles of classifiers offer promise in increasing overall classification accuracy. The availability of extremely large datasets has opened avenues for application of distributed and/or parallel learning to efficiently learn models of them. In this paper, distributed learningis done by training classifiers on disjoint subsets of the data. We examine a random partitioning method to create disjoint subsets and propose a more intelligent way of partitioning into disjointsubsets using clustering. It was observed that the intelligent method of partitioning generally performs better than random partitioning for our datasets. In both methods a significant gain in accuracy may be obtained by applying bagging to each of the disjoint subsets, creating multiple diverse classifiers. The significance of our finding is that a partition strategy for even small/moderate sized datasets when combined with bagging can yield better performancethan applying a single learner using the entire dataset.
ICDM	Using Artificial Anomalies to Detect Unknown and Known Network Intrusions.	Wei Fan,Matthew Miller,Salvatore J. Stolfo,Wenke Lee,Philip K. Chan	2001	Intrusion detection systems (IDSs) must be capable of detecting new and unknown attacks, or anomalies. We study the problem of building detection models for both pure anomaly detection and combined misuse and anomaly detection (i.e., detection of both known and unknown intrusions). We show the necessity of artificial anomalies by discussing the failure to use conventional inductive learning methods to detect anomalies. We propose an algorithm to generate artificial anomalies to coerce the inductive learner into discovering an accurate boundary between known classes (normal connections and known intrusions) and anomalies. Empirical studies show that our pure anomaly-detection model trained using normal and artificial anomalies is capable of detecting more than 77% of all unknown intrusion classes with more than 50% accuracy per intrusion class. The combined misuse and anomaly-detection models are as accurate as a pure misuse detection model in detecting known intrusions and are capable of detecting at least 50% of unknown intrusion classes with accuracy measurements between 75 and 100% per class.
ICDM	Using Rule Sets to Maximize ROC Performance.	Tom Fawcett	2001	Rules are commonly use for classification because they are modular, intelligible and easy to learn. Existing work in classification rule learning assumes the goal is to produce categorical classifications to maximize classification accuracy. Recent work in machine learning has pointed out the limitations of classification accuracy: when class distributions are skewed, or error costs are unequal, an accuracy maximizing rule set can perform poorly. Amore flexible use of a rule set is to produce instance scores indicating the likelihood that an instance belongs to a given class. With such an ability, we can apply rulesets effectively whendistributions are skewed or error costs are unequal. This paper empirically investigates different strategies for evaluating rule sets when the goal is to maximize the scoring (ROC)performance.
ICDM	Association Rules Enhanced Classification of Underwater Acoustic Signal.	Jie Chen,Haiying Li,Shiwei Tang	2001	Classification of underwater acoustic signal is one ofthe important fields of pattern recognition. Inspired bythe experience of training man experts in sonar, wepropose a two-phase training algorithm to exploit theassociation rules to reveal the understandable intrinsicrules contributing to correct classification in the knownmisclassification datasets in this paper. Preliminaryexperimental results demonstrate the potential ofclassification association rules to enhance the accuracyof classification of underwater acoustic signals.
ICDM	A Synchronization Based Algorithm for Discovering Ellipsoidal Clusters in Large Datasets.	Hichem Frigui,Mohamed Ben Hadj Rhouma	2001	This paper introduces a new scalable approach to clusteringbased on synchronization of pulse-coupled oscillators. Eachdata point is represented by an integrate-and-fire oscillator, and the interaction between oscillators is defined according to the relative similarity between the points. The set of oscillators will self-organize into stable phase-locked subgroups. Our approach proceeds by loading only a subset of the data and allowing it to self-organize. Groups ofsynchronized oscillators are then summarized and purged from memory. We show that our method is robust, scales linearly, and can determine the number of clusters. The proposedapproach is empirically evaluated with several synthetic data sets and is used to segment large color images.
ICDM	Functional Trees for Classification.	Joao Gama	2001	The design of algorithms that explore multiple representation languages and explore different search space has an intuitive appeal.In this context of classification problems, algorithmsthat generate multivariate trees are able to explore multiplerepresentation languages by using decision test based on acombination of attributes.The same applies to models threesalgorithms, in regression domains, but using linear models atleaf nodes.In this paper we study where to use combinations of attributes in decision tree learning.We present an algorithm for multivariate tree learning that combines a univariate decision tree with a discriminant function by means of constructiveinduction.This algorithm is able to use decision nodes with multivariate tests, and leaf nodes that predict a class using adiscrimnant. Multivariate decision nodes are built when growing the tree, while functional leaves are built when pruning the tree.Functional trees can be seen as a generalization of multivariate trees.Our algorithm was compared against to its components and two simplified versions using 30 benchmark datasets. The experimental evaluation shows that our algorithm has clear Advantages with respect to the generalization ability and model sizes at statistically significant.
ICDM	Mining Generalized Association Rules for Sequential and Path Data.	Wolfgang Gaul,Lars Schmidt-Thieme	2001	While association rules for set data se and describe relations between parts of set valued objects completely, association rules for sequential data are restricted by specific interpretations of the subsequence relation: contiguous subsequences describe localfeatures of a sequence valued object, noncontiguous subsequences its global features. We model both types of features with generalized subsequences that describe local deviations by wildcards, and present a new algorithm of Apriori type for mining all generalized subsequences with prescribed minim m support from a given database of sequences. Furthermore we show that the givenalgorithm automatically takes into account an eventually underlying graph structure, i.e., is applicable to path data also.
ICDM	A Tight Upper Bound on the Number of Candidate Patterns.	Floris Geerts,Bart Goethals,Jan Van den Bussche	2001	In the context of mining for frequent patterns using the standard level wise algorithm, the following question arises: given the current level and the current set of frequentpatterns, what is the maximal number of candidate patterns that can be generated on the next level? We answer this question by providing a tight upper bound, derived from a combinatorial result from the sixties by Kruskal andKatona. Our result is useful to educe the number of databasescans.
ICDM	Combining Labeled and Unlabeled Data for Text Classification with a Large Number of Categories.	Rayid Ghani	2001	"We develop a framework to incorporate unlabeled data in the Error-Correcting Output Coding (ECOC)setup by decomposing multiclass problems into multiple binary problems and then use Co-Training to learn the individual binary classification problems. We show that our method isespecially useful for classification tasks involving a large number of categories where Co-training doesn't perform very well by itself and when combined with ECOC, outperforms several other algorithms that combine labeled and unlabeled data for text classification in terms of accuracy, precision-recall tradeoff, and efficiency."
ICDM	Dependency Derivation in Industrial Process Data.	Daniel Gillblad,Anders Holst	2001	In many industrial processes, finding dependencies and the creation of dependency graphs can increase the understanding of the system significantly. This knowledge can then be used for further optimization and variable selection. Most of the measured attributes in these cases come in the form of time series. There are several ways of determining correlation between series, most of them suffering from specific problems when applied to real-world data. Here, awell performing measure based on the mutual information rate is derived and discussed with results from both synthetic and real data.
ICDM	Evaluating Boosting Algorithms to Classify Rare Classes: Comparison and Improvements.	Mahesh V. Joshi,Vipin Kumar,Ramesh C. Agarwal	2001	Classification of rare vents has many important data mining applications. Boosting is a promising meta-techniquethat improves the classification performance of any weak classifier. So far, no systematic study has been conducted to evaluate how boosting performs for the task of mining rare classes. In this paper, we evaluate three existing categories of boosting algorithms from the single viewpoint of how they update the example weights in eachiteration, and discuss their possible effect on recall andprecision of the rare class. We propose enhanced algorithms in two of the categories, and justify their choice of weightupdating parameters theoretically. Using some specially designed synthetic datasets, we compare the capability of all the algorithms from the rare class perspective. Theresults support our qualitative analysis, and also indicate that our enhancements bring an extra capability for achieving better balance between recall and precision in mining rareclasses.
ICDM	An Agglomerative Hierarchical Clustering Using Partial Maximum Array and Incremental Similarity Computation Method.	Sung Young Jung,Taek-Soo Kim	2001	As the tractable amount of data is growing in computer science area, fast clustering algorithm is being required because traditional clustering algorithms are not so feasible for very large and high dimensional data. Many studies have been reported for clustering of large database, but most of them circumvent this problem by using the approximation method to result in thedeterioration of accuracy. In this paper, we propose a new clustering algorithm by means of partial maximum array, which can realize the agglomerative hierarchical clustering with the same accuracy to the brute-force algorithm and has O(N 2 ) time complexity. And we alsopresent the incremental method of similarity computation which substitutes the scalar calculation for the time-consuming calculation of vector similarity. The experimental results show that clustering becomes significantly fast for large and high dimensional data.
ICDM	Efficiently Mining Maximal Frequent Itemsets.	Karam Gouda,Mohammed Javeed Zaki	2001	We present GenMax, a backtrack search based algorithm for mining maximal frequent itemsets. GenMax uses a number of optimizations to prune the search space.It usesa novel technique called progressive focusing to perform maximality checking, and diffset propagation to perform fast frequency computation. Systematic experimental comparison with previous work indicates that different methods have varying strengths and weaknesses based on dataset characteristics. We found GenMax to be a highly efficient method to mine the exact set of maximal patterns.
ICDM	Distance Measures for Effective Clustering of ARIMA Time-Series.	Konstantinos Kalpakis,Dhiral Gada,Vasundhara Puttagunta	2001	Many environmental and socioeconomic time-series data can be adequately modeled using Auto-RegressiveIntegrated Moving Average (ARIMA) models. We call such Time-series ARIMA time-series. We consider the problem of clustering ARIMA time-series. We propose the use of the Linear Predictive Coding (LPC) cepstrum of time-series for clustering ARIMA time-series, by using the Euclideandistance between the LPC cepstra of two time-series as their dissimilarity measure. We demonstrate that LPC cepstral coefficients have the desire features for accurate clustering and efficient indexing of ARIMA time-series. For example, few LPC cepstral coefficients are sufficient in order todiscriminate between time-series that are modeled by different ARIMA models. In fact this approach requires fewer coefficients than traditional approaches, such as DFT and DWT. The proposed distance measure can be use for measuring the similarity between different ARIMA models as well.We cluster ARIMA time-series using the Partition Around Medoids method with various similarity measures. We present experimental results demonstrating that using the proposed measure we achieve significantly betterclusterings of ARIMA time-series data as compared to clusterings obtained by using other traditional similaritymeasures, such as DFT, DWT, PCA, etc. Experiments wereperformed both on simulated as well as real data.
ICDM	The DIAsDEM Framework for Converting Domain-Specific Texts into XML Documents with Data Mining Techniques.	Henner Graubitz,Myra Spiliopoulou,Karsten Winkler	2001	Modern organizations are accumulating huge volumesof textual documents. To turn archives into valuable know-ledge sources, textual content must become explicit andqueryable. Semantic tagging with markup languages suchas XML satisfies both requirements. We thus introduce theDIAsDEM* framework for extra ting semantics from structural text units (e.g., sentences), assigning XML tags to them and deriving a flat XML DTD for the archive. DIAsDEM focuses on archives characterized by a peculiar terminologyand by an implicit structure such as court filings and company reports. In the knowledge discovery phase, text units are iteratively clustered by similarity of their content. Eachiteration outputs clusters satisfying a set of quality criteria.Text units contained in these clusters are tagged with semi-automatically determined luster labels and XML tags respectively. Additionally, extracted named entities (e.g.,per-sons) serve as attributes of XML tags. We apply the frame-work in a case study on the German Commercial Register.
ICDM	A Scalable Algorithm for Clustering Sequential Data.	Valerie Guralnik,George Karypis	2001	In recent years, we have seen an enormous growth in the amount of available commercial and scientific data. Data from domains such as protein sequences, retail transactions, intrusion detection, and web-logs have an inherent sequential nature. Clustering of such data sets is usefulfor various purposes. For example, clustering of sequences from commercialdata sets may help marketer identify different customer groups based upon their purchasing patterns. Grouping protein sequences that share similar structure helps in identifying sequences with similar functionality. Over the years, many methods have been developed for clustering objects according to their similarity. However these methods tend to have a computational complexity that is at least quadratic on the number of sequences. In this paperwe present an entirely different approach to sequence clustering that does not require an all-against-all analysis and uses a nearlinear complexity K-means based clustering algorithm. Our experiments using data sets derived from sequences of purchasing transactions and protein sequences show that this approach is scalable and leads to reasonably good clusters.
ICDM	Mining Decision Trees from Data Streams in a Mobile Environment.	Hillol Kargupta,Byung-Hoon Park	2001	This paper presents a novel Fourier analysis-based technique toaggregate, communicate, and visualize decision trees in a mobile environment. Fourier representation of a decision tree has several useful properties that are particularly useful for mining continuous data streams from small mobile computing devices. This paper presents algorithms to compute the Fourier spectrum of a decision tree and the vice versa. It offers a framework to aggregate decision trees in their Fourier representations. It a so describes atouch-pad/ticker-based approach to visualize decision trees using their Fourier spectrum and an implementation for PDAs..
ICDM	Clustering Validity Assessment: Finding the Optimal Partitioning of a Data Set.	Maria Halkidi,Michalis Vazirgiannis	2001	"Clustering s a mostly unsupervised procedure and the majority of the clustering algorithms depend on certain assumptions in order to define the subgroups present in a data set. As a consequence, in most applications the resulting clustering scheme requires some sort ofevaluation as regards its validity. In this paper we present a clustering validity procedure,which evaluates the results of clustering algorithms on data sets. We define a validity index, S_Dbw, based on well-defined clustering criteria enabling the selection of the optimal input parameters' values for a clustering algorithm that result in the best partitioning of a data set.We evaluate the reliability of our index both theoretically and experimentally, considering three representative clustering algorithms ran on synthetic and real data sets. Also, we carried out an evaluation study to compare S_Dbw performance with other known validity indices.Our approach performed favorably in all cases, even in those that other indices failed to indicate the correct partitions in a data set."
ICDM	An Online Algorithm for Segmenting Time Series.	Eamonn J. Keogh,Selina Chu,David Hart,Michael J. Pazzani	2001	In recent years, there has been an explosion of interest in mining time series databases. As with most computer science problems, representation of the data is the key to efficient and effective solutions. One of the most commonly used representations is piecewise linear approximation. This representation has been used by various researchers to support clustering, classification, indexing and association rule mining of t me series data. A variety of algorithms have been proposed to obtain this representation, with several algorithms having been independently rediscovered several times. In this paper, we undertake the first extensive review and empirical comparison of all proposed techniques. We show that allthese algorithms have fatal flaws from a data mining perspective. We introduce a novel algorithm that we empirically show to be super or to all others n the literature.
ICDM	Discovering Representative Episodal Association Rules from Event Sequences Using Frequent Closed Episode Sets and Event Constraints.	Sherri K. Harms,Jitender S. Deogun,Jamil Saquer,Tsegaye Tadesse	2001	Discovering association rules from time-series data is an important data mining problem. The number of potential rules grows quickly as the number of items in the antecedent grows. It is therefore difficult for an expert to analyze the rules and identify the useful. An approach for generating representative association rules for transactions that uses only a subset of the set of frequent itemsets called frequent closed itemsets was presented in [6 ]. We employ formalconcept analysis to develop the notion of frequent closed episodes. The concept of representative association rules is formalized in the context of event sequences. Applying constraints to target highly significant rules further reduces the number of rules. Our approach results in a significant reduction of the number of rules generated, while maintaining the minimum set of relevant association rules and retaining the ability to generate the entire set of association rules with respect to the given constraints. We show how our method can be used to discover associations in a drought risk management decision support system and use multiple climatology datasets related to automated weather stations1
ICDM	Automatic Topic Identification Using Webpage Clustering.	Xiaofeng He,Chris H. Q. Ding,Hongyuan Zha,Horst D. Simon	2001	Grouping webpage into distinct topics is one way to organize the large amount of retrieved information on the web. In this paper, we report that based on similaritymetric which incorporates textual information, hyperlinkstructure and co-citation relations, an unsupervised clustering method can automatically and effectively identify relevant topics, a shown in experiments on several retrieved sets of webpages. The clustering method is a state-of-art spectral graph partitioning method based on normalized cutcriterion first developed for image segmentation.
ICDM	AINE: An Immunological Approach to Data Mining.	Thomas Knight,Jon Timmis	2001	An investigation has been undertaken to repeat previous work on an artificial immune system for data analysis called AINE (Artificial Immune Network).The previous work was limited to testing the algorithm on relatively small data sets. The aim of this investigation is two fold,firstly to corroborate the results presented in previous work and secondly, to test the algorithm on a larger and more complex data set. A new re-implementation of AINE is then described and differences in behaviour are identified and explained. It is argued that the behaviourseen in the new implementation is more accurate than that seen in previous work and an in-depth analysis of the algorithm structure is undertaken in order to confirm theseobservations. The algorithm is also tested on new data and the results of this are presented. Comparisons are draw with other similar techniques for data mining and it is argued that AINE is an effective data-mining algorithm.
ICDM	Time Series Segmentation for Context Recognition in Mobile Devices.	Johan Himberg,Kalle Korpiaho,Heikki Mannila,Johanna Tikanmäki,Hannu Toivonen	2001	"Recognizing the context of se is important in making mobile devices as simple to use as possible. Finding out what the user's situation is can help the device andunderlying service in providing an adaptive and personalized user interface. The device can infer parts of the context of the user from sensor data: the mobile device can includesensors for acceleration, noise level, luminosity, humidity, etc. In this paper we consider context recognition by unsupervisedsegmentation of time series produced by sensors.Dynamic programming can be used to find segments that minimize the intra-segment variances. While this method produces optimal solutions, it is too slow for long sequencesof data. We present and analyze randomized variations of the algorithm. One of them, Global Iterative Replacement or GIR, gives approximately optimal results in a fraction of the time required by dynamic programming. Wedemonstrate the se of time series segmentation in contextrecognition for mobile phone applications."
ICDM	Indiscernibility Degree of Objects for Evaluating Simplicity of Knowledge in the Clustering Procedure.	Shoji Hirano,Shusaku Tsumoto	2001	This paper presents a new, rough sets-based clusteringmethod that enables evaluation of simplicity of classification knowledge during the clustering procedure. The method iteratively refines equivalence relations so that they become more simple set of relations that give adequately coarse classification to the objects. At each step ofiteration, importance of the equivalence relation is evaluated on the basis of the newly introduced measure, indiscernibility degree. An indiscernibility degree is defined as a ratio of equivalence relations that classify the two objects into the same equivalence class. If an equivalence relation hasability to discern the two objects that have high indiscernibility degree, it is considered to perform too fine classification and then modified to regard them as indiscernible objects. The refinement is repeated decreasing the threshold level ofindiscernibility degree, and finally simple clusters can beobtained. Experimental results on the artificial data showed that iterative refinement of equivalence relation lead tosuccessful generation of coarse clusters that can be representedby simple knowledge.
ICDM	Mining Coverage-Based Fuzzy Rules by Evolutional Computation.	Tzung-Pei Hong,Yeong-Chyi Lee	2001	In this paper, we propose a novel mining approach based on the genetic process and an evaluation mechanism to automatically construct an effective fuzzy rule base. The proposed approach consists of three phases: fuzzy-rule generating, fuzzy-rule encoding and fuzzy-ruleevolution. In the fuzzy-rule generating phase, a number of fuzzy rules are randomly generated. In the fuzzy-rule encoding phase, all the rules generated are translated into fixed-length bit strings to form an initial population. In the fuzzy-rule evolution phase, genetic operations andcredit assignment are applied at the rule level. The proposed mining approach chooses good individuals in the population for mating, gradually creating better offspring fuzzy rules. A concise and compact fuzzy rule base is thus constructed effectively without human expertintervention.
ICDM	Text Clustering Based on Good Aggregations.	Andreas Hotho,Alexander Maedche,Steffen Staab	2001	Text Clustering Based on Good Aggregations.
ICDM	Concise Representation of Frequent Patterns Based on Disjunction-Free Generators.	Marzena Kryszkiewicz	2001	Many data mining problems require the discover of frequent patterns in order to be solved.Frequent Itemsets are useful in the discover of association rules, episode rules, sequential patterns and clusters. The number of frequent itemsets is usually huge. Therefore, it is important to work out concise representations of frequent itemsets. In the paper, we describe three basic loassless representations of frequent patters in a uniform wayand offer a new lossless representation of frequent patterns based on disjunction-free generators. The new representation is more concisethan two of the basic representations and more efficiently computablethan the third representation. We propose an algorithm for the determining the new representation.
ICDM	Mining the Smallest Association Rule Set for Predictions.	Jiuyong Li,Hong Shen,Rodney W. Topor	2001	Mining transaction databases for association rules usually generates a large number of rules, most of which are unnecessary when used for subsequent prediction. In this paper we define a rule set for a given transaction database that is much smaller than the association rule set but makes the same predictions as the association rule set by the confidence priority. We call this subset the informative rule set. The informative rule set is not constrained to particular target items; and it is smaller than the non-redundant association rule set. We present an algorithm to directly generate the informative rule set, i.e., without generating all frequentitemsets first, and that accesses the database less often than other unconstrained direct methods. We show experimentally that the informative rule set is much smaller than boththe association rule set and the non-redundant association rule set, and that it can be generated more efficiently.
ICDM	Frequent Subgraph Discovery.	Michihiro Kuramochi,George Karypis	2001	As data mining techniques are being increasingly applied to non-traditional domains, existing approaches for finding frequent itemsets cannot be used as they cannot model the requirement of these domains. An alternate way of modeling the objects in these data sets is to use graphs. Within that model, the problem of finding frequent patterns becomes that of discovering subgraphs that occur frequently over the entire set of graphs. In this paper we present a computationally efficient algorithm for finding all frequent subgraphs in large graph databases. We evaluated the performance of the algorithm by experiments with synthetic datasets as well as a chemical compound dataset. The empirical results show that our algorithm scales linearly with the number of input transactions and it is able to discover frequent subgraphs from a set of graph transactions reasonably fast, even though we have to deal with computationally hard problems such as canonical labeling of graphs and subgraph isomorphism which are not necessary for traditional frequent itemset discovery.
ICDM	Statistical Considerations in Learning from Data.	Henry E. Kyburg Jr.	2001	In this paper we focus on statistics. Classical statistics and Bayesian statistics are both employed in data mining. Both have advantages but both also have severe limitations in this context. We point out some of these limitations as well as s me of the advantages. The fact that we may need to take account of evidence both internal and external to the data set presents a difficulty for classical statistics. The need to incorporate an objective measure of reliability creates a difficulty for Bayesian statistics.We outline an approach to uncertainty that promises to capture the best of both worlds by incorporating both background knowledge and objectivity.
ICDM	Subject Classification in the Oxford English Dictionary.	Zarrin Langari,Frank Wm. Tompa	2001	The oxford English Dictionary is a valuable source of lexical information and a rich testing ground for mining highly structured text.Each entry is organized into a hierarchy of senses, which include definitions, labels and cited quotations.Subject labels distinguish the subject classification of a sense, for example they signal how a word may be used in Anthropology, Music or Computing.Unfortunately subject labeling in the dictionary is incomplete. To overcome thisincompleteness, we attempt to classify the senses (i.e., definitions) in the dictionary by their subjects, using thecitations as an information guide.We report on four different approaches: K Nearest Neighbors, a standard classification technique; Term Weighting, an information retrieval method dealing with text; Naïve Bayes, a probabilistic method; and Expectation Maximization, An iterative probabilistic method.Experimental performance of these Methods is compared based on standard classification metrics.
ICDM	An Efficient Fuzzy C-Means Clustering Algorithm.	Ming-Chuan Hung,Don-Lin Yang	2001	The Fuzzy C-Means (FCM) algorithm is commonly used for clustering.The performance of the FCM algorithm depends on the selection of the initial cluster center and/or the initial membership value.If a good initial cluster center that is close to the actual final clustercenter can be found, the FCM algorithm will converge very quickly and the processing time can be drastically.In this paper, we propose a novel algorithm for efficient clustering.This algorithm is a modified FCM called the psFCM algorithm, which significantly reduces the computation timerequired to partition a dataset into desired cluster.We find the actual cluster center by using a simplified set of the original complete dataset.It refines the initial value of the FCM algorithm to speed up the convergence time.Our Experiments show that the proposed psFCM algorithm isAlgorithm.We also demonstrate that the quality of the Proposed psFCM algorithm is the same as the FCM algorithm.
ICDM	Fuzzy Data Mining: Effect of Fuzzy Discretization.	Hisao Ishibuchi,Takashi Yamamoto,Tomoharu Nakashima	2001	When we generate association rules, continuous attributes have to be discretized into intervals while our knowledge representation is not always based on such discretiztion.Forexample, we usually use some linguistic terms (e.g., young, middle age, and old) for dividing our ages into somefuzzy categories.In this paper, we describe the extraction of linguistic association rules and examine the performanceof extracted rules.First we modify the definitions of the two basic measures (i.e., confidence and support) ofassociation rules for extracting linguistic association rules. The main difference between standard and linguistics association rules is the discretiztion of continuous attributes. We divide the domain interval of each attribute into some Fuzzy discretiztion with standard on-fuzzy discretiztion Through computer simulations on a pattern classificationproblem with many continuous attributes.The classification performance of extracted rules on unseen test patterns is examined under various conditions.Simulation results show that linguistic association rules with rule weights have highgeneralization ability even when the domain of each continuous attribute is homogeneously partitioned.
ICDM	Ad Hoc Association Rule Mining as SQL3 Queries.	Hasan M. Jamil	2001	Although there have been several encouraging attempts at developing methods for data mining using SQL, simplicity and efficiency still remain significant impediments for furtherdevelopment. In this paper, we propose a significantly new approach and show that any object relational database can be mined for association rules without any restructuring orpreprocessing using only basic SQL3 constructs and functions, and hence no additional machineries are necessary. In particular, we show that the cost of computing associationrules for a given database does not depend on support and confidence thresholds. More precisely, the set of large items can be computed using one simple join query and anaggregation once the set of all possible meets (least fix point) of item set patterns in the input table is known. The principal focus of this paper is to demonstrate that several SQL3expressions exists for the mining of association rules.
ICDM	On Mining General Temporal Association Rules in a Publication Database.	Chang-Hung Lee,Cheng-Ru Lin,Ming-Syan Chen	2001	In this paper, we explore a new problem of mining general temporal association rules in publication databases. In essence, a publication database is a set of transactions where each transaction T is a set of items of which each item contains an individual exhibition period. The current model of association rule mining is not able to handle the publication database due to the following fundamental problems, i.e., (1) lack of consideration of the exhibition period of each individual item; (2) lack of an equitable support counting basis for each item. To remedy this, we propose an innovative algorithm Progressive-Partition-Miner (abbreviatedly as PPM) to discover general temporal association rules in a publication database. The basic idea ofPPM is to first partition the publication database in light of exhibition periods of items and then progressively accumulate the occurrence count of each candidate 2-itemset based on the intrinsic partitioning characteristics. Algorithm PPM is also designed to employ a filtering threshold in each partition to early prune out those cumulatively infrequent 2-itemsets. Explicitly, the execution time of PPM is, in orders of magnitude, smaller than those required by the schemes which are directly extended from existing methods.
ICDM	Preparations for Semantics-Based XML Mining.	Jung-Won Lee,Kiho Lee,Won Kim	2001	XML allows users to define elements using arbitrary words and organize them in a nested structure. These features of XML offer both challenges and opportunities in information retrieval, document management, and data mining. In this paper,we propose a new methodology for preparing XML documents for quantitative determination of similarity between XML documents by taking account of XML semantics (i.e.,meanings of the elements andnested structures of XML documents).Accurate quantitative determination of similarity between XML documents provides an important basis for a variety of applications of XML document mining and processing. Experiments with XML documents show that ourmethodology provides a 50-100%improvement in determining similarity, over the traditional vector-space model that considers only term-frequency and 100% accuracy in identifying the category of each document from an on-line bookstore.
ICDM	The Computational Complexity of High-Dimensional Correlation Search.	Chris Jermaine	2001	There is a growing awareness that the popular support metric (often used to guide search in market-basket analysis) is not appropriate for use in every association mining application. Support measures only the frequency of co-occurrence of a set of events when determining which pat-terns to report back to the user. It incorporates no rigorous statistical notion of surprise or interest, and many of the patterns deemed interesting by the support metric are uninteresting to the user.However, a positive aspect of support is that search using support is very efficient. The question we address in this paper is: can we retain this efficiency if we move beyond support, and to other, more rigorous metrics? We consider the computational implications of incorporating simple expectation into the data mining task. It turns out that many variations on the problem which incorporate more rigorous tests of dependence (or independence) result in NP-hard problem definitions.
ICDM	CMAR: Accurate and Efficient Classification Based on Multiple Class-Association Rules.	Wenmin Li,Jiawei Han,Jian Pei	2001	Previous studies propose that associative classification has high classification accuracy and strong flexibility at handling unstructured data. However, it still suffers from the huge set of mined rules and sometimes biased classification or overfitting since the classificationis based on only single high-confidence rule. In this study, we propose new associative classification method, CMAR, i.e., Classification based on Multiple Association Rules. The method extends an efficient frequent pattern mining method, FP-growth ,constructs classdistribution-associated FP-tree, and mines large database efficiently. Moreover, it applies CR-tree structure to store and retrieve mined association rulesefficiently, and prunes rules effectively based on confidence, correlation and database coverage. The classification is performed based on weighted X2 analysis using multiple strong association rules. Our extensive experiments on 26 databases from UCI machine learning database repository show that CMAR is consistent, highly effective at classificationof various kinds of databases and has better average classificationaccuracy in comparison with CBA and C4.5.Moreover,our performancestudy shows that the method is highly efficient and scalable in comparison with other reported associative classification methods.
ICDM	Mining Image Features for Efficient Query Processing.	Beitao Li,Wei-Cheng Lai,Edward Y. Chang,Kwang-Ting Cheng	2001	The number of feature required to depict an image can be very large. Using all features simultaneously to measure image similarity and to learn image query-concepts can suffer from the problem of dimensionality curse ,which degrades both search accuracy and search peed. Regarding search accuracy, the presence of irrelevant features with respect to a query can contaminate similarity measurement, and hence decrease both the recall and precision of thatquery. To remedy this problem, we present a mining method that learns online user query concept and identities important features quickly. Regarding search speed, the presence of a large number of feature can low down query-concept learning and indexing performance. We propose a divide-and-conquer method that divides the concept-learning task into G subtasks to achieve speedup. We notice that a task must be divided carefully, or search accuracy maysuffer. We thus propose a genetic-based mining algorithm to discover good feature groupings. Through analysis and mining result, we observe that organizing image features in a multi-resolution manner, and minimizing intra-group feature correlation, can peed up query-concept learning substantially while maintaining high search accuracy.
ICDM	Evolutionary Structure Learning Algorithm for Bayesian Network and Penalized Mutual Information Metric.	Gang Li,Fu Tong,Honghua Dai	2001	This paper formulates the problem of learning Bayesian network structures from data as determining the structure that best approximates the probability distribution indicated by the data. A new metric, Penalized Mutual Information metric, is proposed, and a evolutionary algorithm is designed to search for the best structure among alternatives. The experimental results show that this approach is reliable and promising.
ICDM	Metric Rule Generation with Septic Shock Patient Data.	Jürgen Paetz	2001	In this contribution we present an application of metric rule generation in the domain of medical research. We consider intensive car unit patients developing a septic shockduring their stay at the hospital. To analyse the patient data, rule generation is embedded in a medical data mining cycle. For rule generation, we improve an architecture basedon a growing trapezoidal basis function network.
ICDM	Applications of Data Mining in Hydrology.	Xu Liang,Yao Liang	2001	Long-term range streamflow forecast plays an invaluable role in water resources planning andmanagement. In this study, the potential applicability and limitations of the time series forecasting approach using neural network with the multiresolution learning paradigm (NNMLP) are investigated. The predictedlongterm range streamflows using the NNMLP are compared with the observations. The results show that the time series forecasting approach of NNMLP has good predicting skill. The NNMLP requires only historicalstreamflow information. The time series forecasting approach of NNMLP has great potential for being used alone in regions with limited available information, and for being combined with other approaches to improve long-term range streamflow forecasts.
ICDM	LPMiner: An Algorithm for Finding Frequent Itemsets Using Length-Decreasing Support Constraint.	Masakazu Seno,George Karypis	2001	Over the years, a variety of algorithms or finding frequentitemsets in very large transaction databases have been developed. The key feature in most to these algorithms is that they use a constant support constraint to control the inherently exponential complexity of the problem. In general, itemsets that contain only a few items will tend to be interesting if they have a high support, whereas long itemsets can still be interesting even if their support is relatively small. Ideally, we desire to have an algorithm that finds all the frequent itemsets whose support decreases as a function of their length. In this paper we present an algorithm called LPMiner, that finds all itemsets that satisfy a length-decreasing support constraint. Our experimental evaluation shows that LPMiner is up to two orders of magnitude faster than the FP-growth algorithm or finding itemsets at a constant support constraint, and that its runtime increasesgradually as the average length of the transactions (and the discovered itemsets) increases.
ICDM	Analyzing the Interestingness of Association Rules from the Temporal Dimension.	Bing Liu,Yiming Ma,Ronnie Lee	2001	Rule discovery is one of the central tasks of data mining. Existing research has produced many algorithms for the purpose. These algorithms, however, often generate too manyrules. In the past few years, rule interestingness techniques were proposed to help the user find interesting rules. These techniques typically employ the dataset as a whole to mine rules, and then filter and/or rank the discovered rules in various ways. In this paper, we argue that this is insufficient. These techniques are unable to answer a question that is of criticalimportance to the application of rules, i.e., can the rules be trusted? In practice, the users are always concerned with the question. They want to know whether the rules indeed represent some true and stable (or reliable)underlying relationships in the domain. If a rule is not stable, does it show any systematic pattern such as a trend? Before any rule can be used, these questions must be answered. This paper proposes a technique to use statistical methods to analyze rules from the temporal dimension to answer these questions. Experimental results show that the proposed technique is very effective.
ICDM	RPCL-Based Local PCA Algorithm.	Zhiyong Liu,Lei Xu	2001	Mining local structure is important in data analysis.Gaussian mixture is able to describe local structurethrough the covariance matrices, but when used on high dimensional data, fitly specifying such a large number of d(d + 1)=2 free elements in each covariance matrix is difficult. In this paper, by constraining the covariance matrixin decomposed orthonormal form, we propos a Local PCAalgorithm to tackle this problem in help of RPCL competitivelearning, which can automatically determine the number of local structure.
ICDM	Closing the Loop: An Agenda- and Justification-Based Framework for Selecting the Next Discovery Task to Perform.	Gary Livingston,John M. Rosenberg,Bruce G. Buchanan	2001	"We propose and evaluate an agenda-and justification-basedarchitecture for discovery systems that selects the next tasks to perform. This framework has manydesirable properties: (1) it facilitates the encoding of general discovery strategies using a variety of backgroundknowledge, (2) t reasons about the appropriateness of the tasks being considered, and (3) it tailors its behavior toward a user 's interests. A prototype discovery program called HAMB demonstrates that both reasons andestimates of interestingness contribute to performance in the domains of protein crystallization and patient rehabilitation."
ICDM	Closing the Loop: Heuristics for Autonomous Discovery.	Gary Livingston,John M. Rosenberg,Bruce G. Buchanan	2001	Closing the Loop: Heuristics for Autonomous Discovery.
ICDM	H-Mine: Hyper-Structure Mining of Frequent Patterns in Large Databases.	Jian Pei,Jiawei Han,Hongjun Lu,Shojiro Nishio,Shiwei Tang,Dongqing Yang	2001	Methods for efficient mining of frequent patterns have been studied extensively by many researchers. However, the previously proposed methods still encounter someperformance bottlenecks when mining databases with different data characteristics, such as dense vs. sparse, long vs. short patterns, memory-based vs. disk-based, etc.In this study, we propose a simple and novel hyper-linkeddata structure, H-struct , and a new mining algorithm, H-mine ,which takes advantage of this data structure anddynamically adjusts links in the mining process. A distinct feature of this method is that it has very limitedand precisely predictable space overhead and runs really fast in memory-based setting. Moreover, it ca be scaled up to very large databases by database partitioning, and whenthe data set becomes dense,(conditional)FP-trees can be constructed dynamically as part of the mining process. Our study shows that H-mine has high performance in various kinds of data, outperforms the previously developedalgorithms in different settings, and is highly scalable in mining large databases. This study also proposes a new datamining methodology, space-preserving mining ,which mayhave strong impact in the future development of efficient and scalable data mining methods.
ICDM	Anchor Text Mining for Translation of Web Queries.	Wen-Hsiang Lu,Lee-Feng Chien,Hsi-Jian Lee	2001	This paper presents an approach to automatically extracting translations of Web query terms through mining of Web anchor texts and link structures. One of the existing difficulties in cross-language information retrieval (CLIR)and Web search is the lack of the appropriate translations of new terminology and proper names. Such a difficult problem can be effectively alleviated by our proposed approach, and the resource of anchor texts in the Web is proven a valuable corpus for this kind of term translation.
ICDM	Mining Mutually Dependent Patterns.	Sheng Ma,Joseph L. Hellerstein	2001	In some domains, such as isolating problems in computer net-worksand discovering stock market irregularities, there is more interest inpatterns consisting of infrequent, but highly correlated items rather thanpatterns that occur frequently (as defined by minsup, the minimum supportlevel). Herein, we describe the m-pattern, a new pattern that is definedin terms of minp, the minimum probability of mutual dependence of itemsin the pattern. We show that all infrequent m-pattern can be discovered byan efficient algorithm that makes use of: (a) a linear algorithm to qualifyan m-pattern; (b) an effective technique for candidate pruning based on anecessary condition for the presence of an m-pattern; and (c) a level-wisesearch for m-pattern discovery (which is possible because m-patterns aredownward closed). Further, we consider frequent m-patterns, which aredefined in terms of both minp and minsup. Using synthetic data, we studythe scalability of our algorithm. Then, we apply our algorithm to data froma production computer network both to show the m-patterns present andto contrast with frequent patterns. We show that when minp_0, our algorithmis equivalent to finding frequent patterns. However, with a larger minp, our algorithm yields a modest number of highly correlated items, which makes it possible to mine for infrequent but highly correlated item-sets. To date, many actionable m-patterns have been discovered in production systems.
ICDM	FARM: A Framework for Exploring Mining Spaces with Multiple Attributes.	Chang-Shing Perng,Haixun Wang,Sheng Ma,Joseph L. Hellerstein	2001	Mining for frequent itemsets typically involves a preprocessing step in which data with multiple attributes are grouped into transactions, and item are defined based on attribute values. We have observed that such fixed attribute mining can severely constrain the pattern that are discovered. Herein, we introduce mining paces, a new framework for mining multi-attribute data that include the discovery of transaction and item definition (with the exploitation of taxonomies and functional dependenciesif they are available).We prove that special downward closure properties (or anti-monotonic property) hold for mining paces, aresult that allows us to construct efficient algorithms for mining pattern without the constraint of fixed attribute mining. We apply our algorithm to real world data collected from a production computer network. The result how that by exploiting the special kind of downward closure in mining paces, execution times for mining can be reduced by a factor of three to four.
ICDM	The Representative Basis for Association Rules.	Viet Phan Luong	2001	We define the concept of the representative basic for interesting association rules, and an inference system which is purely qualitative. The representative basis is unique, and minimal with respect to (wrt) the inference system. On the representative basis, the inference system is correct and complete. Experimental results show that the number of rule in the representative basis is significantly reduced wrt the number of rules generated by other existing approaches.
ICDM	A Fast Algorithm to Cluster High Dimensional Basket Data.	Carlos Ordonez,Edward Omiecinski,Norberto F. Ezquerra	2001	Clustering is a data mining problem that has received significant attention by the database community. Data set size, dimensionality and sparsity have been identified as aspectsthat make clustering more difficult. This work introduces a fast algorithm to cluster large binary data sets where data points have high dimensionality and most o their coordinates are zero. This is the case with basket data transactions containing items, that can be represented as sparse binary vectors with very high dimensionality. An experimental section shows performance, advantages and limitations of the proposed approach.
ICDM	Learning Automatic Acquisition of Subcategorization Frames Using Bayesian Inference and Support Vector Machines.	Manolis Maragoudakis,Katia Kermanidis,Nikos Fakotakis,George K. Kokkinakis	2001	Learning Bayesian Belief Network (BBN) from corpora and Support Vector Machines (SVM) have been applied to the automatic acquisition of verb subcategorization frames for Modern Greek.We are incorporating minimal linguistic resources, i.e. basic morphological tagging and phrase chunking, to demonstrate that verb subcategorization, which is of great significance for developing robust natural language human computer interaction systems, could be achieved using large corpora, without having any general-purpose syntactic parser at all.
ICDM	Neural Analysis of Mobile Radio Access Network.	Kimmo Raivio,Olli Simula,Jaana Laiho	2001	The Self-Organizing Map (SOM) is an efficient tool for visualization and clustering of multidimensional data. It transforms the input vectors on two-dimensional grid of prototype vectors and orders them. The ordered prototype vectors are easier to visualize and explore than the original data. Mobile networks produce a huge amount of spatio-temporaldata. The data consists of parameters of base stations (BS)and quality information of calls. There are two alternatives in starting the data analysis. We can build either a general one-cell-model trained using state vectors from all cells, or a model of the network using state vectors with parameters from all mobile cells. In both methods,further analysis is needed to understand the reasons for various operational states of the entire network.
ICDM	Discovery of Association Rules in Tabular Data.	Graeme Richards,Victor J. Rayward-Smith	2001	In this paper we address the problem of finding all association rules in tabular data. An Algorithm, ARA, for finding rules, that satisfy clearly specified constraints, in tabular data is presented. ARA is based on the Dense Miner algorithm but includes an additional constraintand an improved method of calculating support. ARA is tested and compared with our implementation of Dense Miner ;it is conclude that ARA is usually more efficient than Dense Miner and is often considerably more so.We also consider the potential for modifying the constraints used in ARA in order to find more generalrules.
ICDM	Theory and Applications of Attribute Decomposition.	Lior Rokach,Oded Maimon	2001	This paper examines the Attribute Decomposition Approach with simple Bayesian combination for dealing with classi£cation problems that contain high number ofattributes and moderate numbers of records. According to the attribute Decomposition approach, the set of input attributes is automatically decomposed into several subsets. classi£cation model is built for each subset, then all the models are combined using simple Bayesian combination.This paper presents theoretical and practical foundation for the Attribute Decomposition approach. A greedyprocedure, called D-IFN, is developed to decompose the input attributes set into subsets and build a classi£cation model for each subset separately. The results achieved in theempirical comparison testing with well-known classi£cationmethods (like C4.5)indicate the superiority of the decomposition approach.
ICDM	FlExPat: Flexible Extraction of Sequential Patterns.	Pierre-Yves Rolland	2001	"This paper addresses sequential data mining, a sub-area of data mining where the data to be analyzed is organized in sequences. In many problem domains a natural ordering exists over data. Examples of sequential databases (SDBs) include: (a)collections of temporal data sequences, such as chronologicalseries of daily stock indices or multimedia data (sound, music, video..); and (b) macromolecule banks, where aminoacid or proteic sequences are represented as strings.In a SDB it is often valuable to detect regularities through one or several sequences. In particular, finding exact or approximate repetitions of segments ca be utilized directly (e.g.for determining the biochemical activity of a protein region) or indirectly, e.g. for prediction in finance. To this end, we present concepts and an algorithm for automatically extracting sequential patterns from a sequential database. Such a patter is defined as a group of significantly similar segments from one or several sequences. Appropriate functions for measuringsimilarity between sequence segments are proposed, generalizing the edit distance framework. There is a trade off here between flexibility, particularly in sequence data representation and in associated similarity metrics, and computational efficiency. Wedesigned the FlExPat algorithm to satisfactorily cope with this trade-off. FlExPat's complexity is in practice lesser than quadratic in the total length of the SDB analyzed, while allowinghigh flexibility. Some experimental results obtained with FlExPat on music data are presented and commented."
ICDM	Incremental Learning with Support Vector Machines.	Stefan Rüping	2001	Support Vector Machines (SVMs) have become a popular tool for machine learning with large amounts of high dimensional data. In this paper an approach for incremental learning with Support Vector Machines is presented, that improves the existing approach of [3 ]. Also, some insight into the interpretability of support vectors s given.
ICDM	Interestingness PreProcessing.	Sigal Sahar	2001	As the size of databases increases, the number of rules mined from them also increases, often to a extent that overwhelms users. To address this problem, an important part of the KDD process is dedicated to determining which of these patterns is interesting. In this paper we define the Interestingness PreProcessing Step, and introduce a new framework for interestingness analysis. In asimilar fashion to data-preprocessing, this preprocessing should always be applied prior to interestingness processing. A strictrequirement, and the biggest challenge, in defining Interestingness PreProcessing techniques is that the preprocessing will not eliminate any potentially interesting patterns. That is, the preprocessing methods must be domain-,task-and user-independent. This property differentiates the preprocessing methods from existing interestingness criteria, and, since they can be applied automatically, makes them very useful. This generic nature also makes them rare: PreProcessing methods are very challenging to define.We also define in this paper the first two preprocessing techniques, and present the empirical results of applying them to six databases. The results indicate that Interestingness PreProcessing Step is very powerful: in most cases, an average of half the rules mined were eliminated by the application of the two Interestingness PreProcessing techniques. These results are Particularly significant since no user-interaction is required to achieve them.
ICDM	Data Analysis and Mining in Ordered Information Tables.	Ying Sai,Yiyu Yao,Ning Zhong	2001	"Many real world problems deal with ordering objects instead of classifying objects, although majority of research in machine learning and data mining has been focused on the latter. For modeling ordering problems, we generalize the notion of information tables to ordered information tables by adding order relations on attribute values. The problem of mining ordering rules is formulated as findingassociation between orderings of attribute values and the overall ordering of objects. An ordering rules ay state that ""if the value of an object x on an attribute a is ordered ahead of the value of another object y on the same attribute, then x is ordered ahead of y"" For mining ordering rules, we first transform an ordered information table into a binaryinformation, and then apply any standard machine learning and data mining algorithms. As an illustration, we analyze in detail MacLean's universities ranking for the year 2000."
ICDM	The EQ Framework for Learning Equivalence Classes of Bayesian Networks.	Paul Munteanu,Mohamed Bendou	2001	This paper proposes a theoretical and an algorithmic framework for the analysis and the design of efficient learning algorithms which explore the space of equivalence classes of Bayesian network structures.This framework is composed of a generic learning model which uses essential graphs and more general partially directed graphs i order to represent the equivalence classes evaluated during search, operational characterizations of these graphs, processing procedures and formulas for directly calculating their score.The experimental results of the algorithms designed within this framework show that the space of equivalence classes may be explored efficiently and with better results than the classical search in the space of Bayesian network structures.
ICDM	Mining Constrained Association Rules to Predict Heart Disease.	Carlos Ordonez,Edward Omiecinski,Levien de Braal,Cesar A. Santana,Norberto F. Ezquerra,José A. Taboada,C. David Cooke,Elizabeth Krawczynska,Ernest V. Garcia	2001	This work describes our experiences on discovering association rules in medical data to predict heart disease. We focus on two aspects in this work: mapping medical data toa transaction format suitable for mining association rules and identifying useful constraints. Based on these aspects we introduce an improved algorithm to discover constrainedassociation rules. We present an experimental sectionexplaining several interesting discovered rules.
ICDM	Bayesian Data Mining on the Web with B-Course.	Petri Myllymäki,Tomi Silander,Henry Tirri,Pekka Uronen	2001	B-Course is a free 1 web-based Bayesian data mining service. This service allows the users to analyze their own data for multivariate probabilistic dependencies represented as Bayesian network models. In addition to this, B-Course also offers facilities for inferring certain type of causal dependencies from the data. The software is especially suitable for educational purposes as the tutorial style user-friendly interface intertwines the steps in the data analysiswith support material that gives an informal introduction to the Bayesian approach adopted. Nevertheless, although the analysis methods, modeling assumptions and restrictionsare totally transparent to the user, this transparency is not achieved at the expense of analysis power: with the restrictions stated in the support material, B-Course is a powerful analysis tool exploiting several theoretically elaborate results developed recently in the fields of Bayesian and causal modeling.
ICDM	A Clustering Method for Very Large Mixed Data Sets.	Guillermo Sánchez-Díaz,José Ruiz-Shulcloper	2001	In the developed countries, especially over the last decade, there has been an explosive growth in the capability to generate, collect and use very large data sets. The objects of these data sets could be simultaneously described by quantitative and qualitative attributes. At present, algorithms able to process either very large data sets (in metric spaces) or mixed(qualitative and quantitative) incomplete data (missing value) sets have been developed, but not for very large mixed incomplete data sets. In this paper we introduce a new clustering method named GLC+to process very large mixed incomplete data sets in order to obtain apartition in connected sets.
ICDM	Mining the Web with Active Hidden Markov Models.	Tobias Scheffer,Christian Decomain,Stefan Wrobel	2001	Mining the Web with Active Hidden Markov Models.
ICDM	Visualizing Association Mining Results through Hierarchical Clusters.	Steven Noel,Vijay V. Raghavan,Chee-Hung Henry Chu	2001	"We propose a new methodology for visualizing association mining results. Inter-item distances are computed from combinations of item set supports. The new distances retain a simple pairwise structure, and are consistent with important frequently occurring item sets. Thus standard tools of visualization, e.g. hierarchical clustering dendrograms can still be applied, while the distance information upon which they are based is richer. Our approach is applicable to general association mining applications, as well as applications involving information spaces modeled by directed graphs, e.g. the Web. In the context of collections of hypertext documents, the inter-document distances capture the information inherent in a collection's link structure, a for of link mining. We demonstrate our methodology with document sets extracted fro the Science Citation Index, applying a metric that measures consistency between clusters and frequent itemsets."
ICDM	An Experimental Comparison of Supervised and Unsupervised Approaches to Text Summarization.	Tadashi Nomoto,Yuji Matsumoto	2001	The paper presents a direct comparison of supervised and unsupervised approaches to text summarization. As a representative supervised method, we use the C4.5 decision tree algorithm, extended with the Minimum Description Length Principle (MDL), and compare it against several unsupervised methods. It is found that a particular un-supervised method based on an extension of the K-means clustering algorithm, performs equal to and in some cases superior to the decision tree based method.
ICDM	Document Clustering and Cluster Topic Extraction in Multilingual Corpora.	Joaquim Ferreira da Silva,João Mexia,Carlos Agra Coelho,José Gabriel Pereira Lopes	2001	A statistics-based approach for clustering documents and for extracting cluster topics is described. Relevant (meaningful) Expressions (REs) automatically extracted from corpora are used as clustering base features. These features are transformed and its number is strongly reduced in order to obtain a small set of document classificationfeatures. This is achieved on the basis of PrincipalComponents Analysis. Model-Based Clustering Analysis finds thebest number of clusters. Then, the most important REs are extracted from each cluster and taken as document cluster topics.
ICDM	A Simple KNN Algorithm for Text Categorization.	Pascal Soucy,Guy W. Mineau	2001	Text categoriztion (also called text classification) is the process of identifying the class to which a text document belongs. This paper proposes to use a simple non-weighted feature KNN algorithm for text caegoriztion. We propose to use a feature selection method that finds the relevant features for the learning task at hand using feature interaction (based on word interdependencies).
ICDM	Measuring Real-Time Predictive Models.	Sam Steingold,Richard Wherry,Gregory Piatetsky-Shapiro	2001	In this paper we examine the problem of comparing real-time predictive models and propose a number of measures for selecting the best model, based on a combination of accuracy, timeliness, and cost. We apply the measure to the real-time attrition problem.
ICDM	Hierarchical Text Classification and Evaluation.	Aixin Sun,Ee-Peng Lim	2001	Hierarchical Classification refers to assigning of one or more suitable categories from a hierarchical category space to a document. While previous work in hierarchical classification focused on virtual category trees where documents are assigned only to the leaf categories, we propose atop-down level-based classification method that can classify documents to both leaf and internal categories. As the standard performance measures assume independence between categories, they have not considered the documents incorrectly classified into categories that are similar or not far from the correct ones in the category tree. We therefore propose the Category-Similarity Measures and Distance-Based Measures to consider the degree of misclassification in measuring the classification performance. An experiment has been carried out to measure the performance four proposed hierarchical classification method. The results showed that our method performs well for Reuters text collection when enough training documents are given andthe new measures have indeed considered the contributions of misclassified documents.
ICDM	Incremental Learning of Bayesian Networks with Hidden Variables.	Fengzhan Tian,Hongwei Zhang,Yuchang Lu,Chunyi Shi	2001	In this paper, an incremental method for learning Bayesian networks based on evolutionary computing, IEMA, is put forward. IEMA introduces the evolutionary algorithm and EM algorithm into the process of incremental learning, can not only avoid getting into local maxima, but also incrementally learn Bayesian networks with high accuracy in presence of missing values andhidden variables. In addition, we improved the incremental learning process by Friedman et al. The experimental results verified the validity of IEMA. In terms of storage cost, IEMA is comparable with the incremental learning method of Friedman et al, while it is ore accurate.
ICDM	Mining Frequent Closed Itemsets with the Frequent Pattern List.	Fan-Chen Tseng,Ching-Chi Hsu,Henry Chen	2001	The mining of the complete set of frequent itemsets willlead to a huge number of itemsets. Fortunately, thisproblem can be reduced to the mining of frequent closeditemsets (FCIs), which results in a much smaller number ofitemsets. The approaches to mining frequent closeditemsets can be categorized into two groups: those withcandidate generation and those without. In this paper, wepropose an approach to mining frequent closed itemsetswithout candidate generation: with a data structure calledthe Frequent Pattern List (FPL). We designed thealgorithm FPLC -Mining to mine the frequent closeditemsets (FCIs). Experimental result shows that our methodis faster than the previously existing ones.
ICDM	Web Cartography for Online State Promotion: An Algorithm for Clustering Web Resources.	François Velin,Pascale Kuntz,Henri Briand	2001	This paper presents an approach of Web cartography to be used in the context of online site promotion.The overall objective is to provide users with handy maps offering information about candidate sites for the creation of hyperlinks that enable a large flow of targeted visitors.Two main types of data must be considered; texts and hyperlinks.We propose to exploit the latter to construct a relevant corpus on which semantic as well as graph analysis can be applied.The stress is put on theclustering of Web resources based on the link network,which makes it possible to highlight groups of strongly connected sites which are of the utmost interest for our application.To tackle the site graph partitioning problem, we turn to a promising iterative approach initially developedin the context of computer-aided design.It uses spectral decomposition of the Laplacian matrix to embed theconsidered graph in a geometric space where efficientmethods can be applied.An algorithm that was adaptedfrom an existing one implements the method.Experimentswere conductedon a real application case concerning the promotion of a site dealing with Cognac.We present the obtained map as well as leads to exploit it.
ICDM	alpha-Surface and Its Application to Mining Protein Data.	Xiong Wang	2001	Given a finite set of points in three dimensional Euclidean space R3, the subset that forms its surface could bedifferent when observed in different levels of details. In thispaper, we introduce a notion called a-surface. We presentan algorithm that extracts the a-surface from a finite set ofpoints in R3. We apply the algorithm to extracting the a-surfaces of proteins and discover patterns from these surface structures, using the pattern discovery algorithm wedeveloped earlier. We then use these patterns to classify theproteins. Experimental results show the good performanceof the proposed approach.
ICDM	Classification through Maximizing Density.	Hui Wang,Ivo Düntsch,David A. Bell,Dayou Liu	2001	This paper presents a novel method for classification, which makes use of the models builtby the lattice machine (LM) [1,3 ]. The LM approximates data resulting in, as a model of data, a set of hyper tuples that are equilabelled, supported and maximal . The method presentedin this paper uses the LM model of data to classify new data with a view to maximising the density of the model. Experiments show that this method, when used with the LM, outperforms the C2 algorithm in [3 ] and it is comparable to the C5.0 classification algorithm.
ICDM	Maintenance of Sequential Patterns for Record Deletion.	Ching-Yao Wang,Tzung-Pei Hong,Shian-Shyong Tseng	2001	In the past, we proposed an incremental mining algorithm for maintenance of sequential patterns based on the concept of pre-large sequences as new records were inserted. In this paper, we attempt to apply the concept of pre-large sequences to maintain sequentialpatterns as records are deleted. Pre-large sequences are defined by a lower support threshold and an upper support threshold. They act as buffers to avoid the movements of sequential patterns directly from large to small and vice-versa. Our proposed algorithm does notrequire rescanning original databases until the accumulative amount of deleted customer sequences exceeds a safety bound, which depends on database size. As databases grow larger, the numbers of deleted customer sequences allowed before database rescanningis required also grow. The proposed approach is thus efficient for a large database.
ICDM	An Immune Neural Network Used for Classification.	Lei Wang,Licheng Jiao	2001	Based on analyzing the immune phenomena in nature and utilizing performances of ANN, a novel network model, i.e., an immune neural network (INN), is proposed which integrates the immune mechanism and the function of neural information processing. The learning algorithm of INN is mainly about the selection of an excitation function and an adaptive algorithm of the network.
ICDM	SSDT: A Scalable Subspace-Splitting Classifier for Biased Data.	Haixun Wang,Philip S. Yu	2001	Decision trees are one of the most extensively used data mining models. Recently, a number of efficient, scalable algorithms for constructing decision trees on large disk-resident dataset have been introduced. In this paper, we study the problem of learning scalable decision trees from datasets with biased class distribution. Our objective is to build decision trees that are ore concise and oreinterpretable while maintaining the scalability of the model.To achieve this, our approach searches for subspace clusters of data cases of the biased class to enable multivariate splittings based on weighted distances to such clusters. In orderto build concise and interpretable models, other approaches including multivariate decision trees and association rules, often introduce scalability and performance issues. The SSDT algorithm we present achieves the objective without loss in efficiency, scalability, and accuracy.
ICDM	Meta-patterns: Revealing Hidden Periodic Patterns.	Wei Wang,Jiong Yang,Philip S. Yu	2001	Discovery of periodic patterns in time series data has become an active research area with many applications. These patterns can be hierarchical in nature, where higher level pattern may consist of repetitions of lower level patterns.Unfortunately, the presence of noise m y prevent these higher level patterns from being recognized in the sense that two portions (of data sequence) that support the same (high level) pattern may have different layouts of occurrences of basic symbols. There may not exist any common representation in terms of raw symbol combinations; and hence such (high level) pattern may not be expressed by any previous model (defined on raw symbols or symbol combinations) and would not be properly recognized by any existing method. In this paper, we propose novel model, namely meta-pattern, to capture these high level patterns. As more flexible model, the number of potential meta-patterns could be very large. A substantial difficulty lies on how to identify the proper pattern candidates. However, the well-known Apriori property is not able to provide sufficient pruning power. A new property, namely component location property, is identified and used to conduct the candidate generation so that an efficient computation-based mining algorithm can be developed. Last but not least, we apply our algorithm to some real and synthetic sequences and some interesting patterns are discovered.
ICDM	Using Boosting to Simplify Classification Models.	Virginia Wheway	2001	Ensemble classification techniques such as bagging ,boosting and arcingalgorithms have been shown to lead to reduced classification error on unseencases and seem immune t the problem of overfitting. Several explanations forthe reduction in generalisation error have been presented, with authors morerecently defining and applying diagnostics such as edge and margin [4,9,10 ].These measures pr vide insight into the behaviour of ensemble classifiers but can they be exploited further?
ICDM	Fast Parallel Association Rule Mining without Candidacy Generation.	Osmar R. Zaïane,Mohammad El-Hajj,Paul Lu	2001	In this paper we introduce a new parallel algorithm MLFPT (Multiple Local Frequent Pattern Tree) [11] for parallel mining of frequent patterns, based on FP-growth mining, that uses only two full I/O scans of the database, eliminating the need for generating the candidate items and distributing the work fairly among processors. We have devised partitioning strategies at different stages of the mining process to achieve near optimal balancing between processors.We have successfully tested our algorithm on datasets larger than 50 million transactions.
ICDM	A Comparison of Stacking with Meta Decision Trees to Bagging, Boosting, and Stacking with other Methods.	Bernard Zenko,Ljupco Todorovski,Saso Dzeroski	2001	Abstract. Meta decision trees (MTs) are a method for combining multiple classifiers. We present an integration of the algorithm MLC4.5 for learning MTs into the Weka data mining suite. We compare classifier ensembles combined with MDTs to bagged and boosted decision trees, and to classifier ensembles combined with other methods: voting and stacking with three different meta-level classifiers (ordinary decision trees, naive Bayes, and multi-response linear regression -MLR).
ICDM	Mining California Vital Statistics Data.	Du Zhang,Quoc Luan Ha,Meiliu Lu	2001	Vital statistics data offer a fertile ground for data mining. In this paper, we discuss the results of a data-mining project on the causes of death aspect of the vital statistics data in the state of California. A data-mining tool called Cubist is used to build predictive models out of two million cases over a nine-year period. The objective of our study is to discover knowledge (trends, correlations or patterns) that may not be gleaned through standard techniques. The generated predictive models allow pertinent state agencies to gain insight into various aspects of the death rates in the state of California, to predict health issues related to the causes of death, to offer an aid to decision or policy-making process and to provide useful information services to the customers. The results obtained in our study contain valuable new information.
ICDM	An Efficient Data Mining Technique for Discovering Interesting Sequential Patterns.	Show-Jane Yen,Yue-Shi Lee	2001	Mining sequential patterns is to discover sequentialpurchasing behaviors of most customers from a largeamount of customer transactions. In this paper, a datamining language is presented. From the data mininglanguage, use s can specify the interested items and thecriteria of the sequential patterns to be discovered. Also,an efficient data mining technique is proposed to ext actthe sequential patterns according to the uses` requests.
ICDM	A Pattern Decomposition (PD) Algorithm for Finding All Frequent Patterns in Large Datasets.	Qinghua Zou,Wesley W. Chu,David B. Johnson,Henry Chiu	2001	Efficient algorithms to mine frequent patterns are crucial to many tasks in data mining. Since the Apriori algorithm was proposed in 1994, there have been several methods proposed to improve its performance. However, most still adopt its candidate set generation-and-testapproach. We propose a pattern decomposition (PD) algorithm that can significantly reduce the size of the dataset on each pass making it more efficient to mine frequent patterns in a large dataset. The proposed algorithm avoids the costly process of candidate set generation and saves time by reducing dataset. Our empirical evaluation shows that the algorithmoutperforms Apriori by one order of magnitude and is faster than FP-tree. Further, PD is more scalable than both Apriori and FP-tree.
ICDM	Interestingness, Peculiarity, and Multi-Database Mining.	Ning Zhong,Y. Y. Yao,Muneaki Ohshima,Setsuo Ohsuga	2001	Interestingness, Peculiarity, and Multi-Database Mining.
KDD	Random projection in dimensionality reduction: applications to image and text data.	Ella Bingham,Heikki Mannila	2001	Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.
KDD	Data mining case study: modeling the behavior of offenders who commit serious sexual assaults.	Richard Adderley,Peter B. Musgrove	2001	This paper looks at the use of a Self Organizing Map (SOM), to link of records of crimes of serious sexual attacks. Once linked a profile can be derived of the offender(s) responsible.The data was drawn from the major crimes database at the National Crime Faculty of the National Police Staff College Bramshill UK. The data was encoded from text by a small team of specialists working to a well-defined protocol. The encoded data was analyzed using SOMs. Two exercises were conducted. These resulted in the linking of several offences in to clusters each of which were sufficiently similar to have possibly been committed by the same offender(s). A number of clusters were used to form profiles of offenders. Some of these profiles were confirmed by independent analysts as either belonging to known offenders or appeared sufficiently interesting to warrant further investigation.The prototype was developed over 10 weeks. This contrasts with an in-house study using a conventional approach, which took 2 years to reach similar results. As a consequence of this study the NCF intends to pursue an in-depth follow up study.
KDD	Evaluating the novelty of text-mined rules using lexical knowledge.	Sugato Basu,Raymond J. Mooney,Krupakar V. Pasupuleti,Joydeep Ghosh	2001	In this paper, we present a new method of estimating the novelty of rules discovered by data-mining methods using WordNet, a lexical knowledge-base of English words. We assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance, more is the novelty of the rule. The novelty of rules extracted by the DiscoTEX text-mining system on Amazon.com book descriptions were evaluated by both human subjects and by our algorithm. By computing correlation coefficients between pairs of human ratings and between human and automatic ratings, we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another. @Text mining
KDD	A human-computer cooperative system for effective high dimensional clustering.	Charu C. Aggarwal	2001	High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Therefore, techniques have recently been proposed to find clusters in hidden subspaces of the data. However, since the behavior of the data may vary considerably in different subspaces, it is often difficult to define the notion of a cluster with the use of simple mathematical formalizations. In fact, the meaningfulness and definition of a cluster is best characterized with the use of human intuition. In this paper, we propose a system which performs high dimensional clustering by effective cooperation between the human and the computer. The complex task of cluster creation is accomplished by a combination of human intuition and the computational support provided by the computer. The result is a system which leverages the best abilities of both the human and the computer in order to create very meaningful sets of clusters in high dimensionality.
KDD	Mining massively incomplete data sets by conceptual reconstruction.	Charu C. Aggarwal,Srinivasan Parthasarathy	2001	Incomplete data sets have become almost ubiquitous in a wide variety of application domains. Common examples can be found in climate and image data sets, sensor data sets and medical data sets. The incompleteness in these data sets may arise from a number of factors: in some cases it may simply be a reflection of certain measurements not being available at the time; in others the information may be lost due to partial system failure; or it may simply be a result of users being unwilling to specify attributes due to privacy concerns. When a significant fraction of the entries are missing in all of the attributes, it becomes very difficult to perform any kind of reasonable extrapolation on the original data. For such cases, we introduce the novel idea of conceptual reconstruction, in which we create effective conceptual representations on which the data mining algorithms can be directly applied. The attraction behind the idea of conceptual reconstruction is to use the correlation structure of the data in order to express it in terms of concepts rather the original dimensions. As a result, the reconstruction procedure estimates only those conceptual aspects of the data which can be mined from the incomplete data set, rather than force errors created by extrapolation. We demonstrate the effectiveness of the approach on a variety of real data sets.
KDD	Applications of generalized support vector machines to predictive modeling.	Nitin Agrarwal	2001	"The work of the Russian mathematician Vladimir Vapnik (AT&T Labs) enables us to go back to the roots of theoretical statistics, leaving behind Fisher's parameters in favor of the general approaches started in the 1930s by Glivenko-Cantelli-Kolmogorov. Nowadays, it has become possible to model millions of events described by thousands of variables, within a reasonable time for a specific application. The SRM approach works with a family of models and calibrates the family of models to a point which is the best compromise between accuracy and robustness. It also measures the complexity of the model using VC dimension which is not plagued by number of parameters. Hence models for large events described by several parameters can be generalized. This opens up great prospects in numerous fields like Customer Relationship Management, Network Optimization, Risk Management, Manufacturing Yield Management, and a number of other data-rich problems."
KDD	Interactive path analysis of web site traffic.	Pavel Berkhin,Jonathan D. Becher,Dee Jay Randall	2001	"The goal of Path Analysis is to understand visitors' navigation of a Web site. The fundamental analysis component is a path. A path is a finite sequence of elements, typically representing URLs or groups of URLs. A full path is an abstraction of a visit or a session, which can contain attributes described below. Subpaths represent interesting subsequences of the full paths.Path Analysis provides user-configurable extraction, filtering, preprocessing, noise reduction, descriptive statistics and detailed analysis of three basic specific objects: elements, (sub)paths, and couples of elements. In each case, lists of frequent objects --- subject to particular filtering and sorting --- are available. We call the corresponding interactive tools Element, Path, and Couple Analyzers.We also allow in-depth exploration of individual elements, paths, and couples: Element Explorer investigates composition and convergence of traffic through an element and allows conditioning based on the number of preceding/succeeding steps. Path Explorer visualizes in and out flows of a path and attrition rate along the path. Couple Explorer presents distinct paths connecting couple elements, along with measures of their association and some additional statistics."
KDD	Fast ordering of large categorical datasets for better visualization.	Alina Beygelzimer,Chang-Shing Perng,Sheng Ma	2001	An important issue in visualizing categorical data is how to order categorical values. The focus of this paper is on constructing such orderings efficiently without compromising their visual quality.
KDD	Challenges for knowledge discovery in biology.	Russ B. Altman	2001	Bioinformatics is the study of information flow in biology. Interest in the field has exploded in the last 10 years with the emergence of techniques for large scale experimental data collection-including genome sequencing, gene expression analysis, protein interaction detection, high-throughput structure determination and others. These techniques, in the context of a large online published literature, have created relatively large data sets (at least by biological standards) that are not possible to analyze manually. There is therefore a critical need for methods to analyze these data and reduce them to new knowledge. The principle challenges to the field include the great diversity of data types and questions that are asked of the data, and the communication difficulties that can exist between experts in biology and experts in machine learning. In this talk, I will provide an introduction to the major biological questions that are being addressed, why they are important, and how the field is trying to address them with technical approaches.
KDD	Segmentation-based modeling for advanced targeted marketing.	Chidanand Apté,E. Bibelnieks,Ramesh Natarajan,Edwin P. D. Pednault,Fateh Tipu,Deb Campbell,Bryan Nelson	2001	"Fingerhut Business Intelligence (BI) has a long and successful history of building statistical models to predict consumer behavior. The models constructed are typically segmentation-based models in which the target audience is split into subpopulations (i.e., customer segments) and individually tailored statistical models are then developed for each segment. Such models are commonly employed in the direct-mail industry; however, segmentation is often performed on an ad-hoc basis without directly considering how segmentation affects the accuracy of the resulting segment models. Fingerhut BI approached IBM Research with the problem of how to build segmentation-based models more effectively so as to maximize predictive accuracy. The IBM Advanced Targeted Marketing-Single EventsTM (IBM ATM-SETM) solution is the result of IBM Research and Fingerhut BI directing their efforts jointly towards solving this problem. This paper presents an evaluation of ATM-SE's modeling capabilities using data from Fingerhut's catalog mailings."
KDD	"The ""DGX"" distribution for mining massive, skewed data."	Zhiqiang Bi,Christos Faloutsos,Flip Korn	2001	Skewed distributions appear very often in practice. Unfortunately, the traditional Zipf distribution often fails to model them well. In this paper, we propose a new probability distribution, the Discrete Gaussian Exponential (DGX), to achieve excellent fits in a wide variety of settings; our new distribution includes the Zipf distribution as a special case. We present a statistically sound method for estimating the DGX parameters based on maximum likelihood estimation (MLE). We applied DGX to a wide variety of real world data sets, such as sales data from a large retailer chain, us-age data from AT&T, and Internet clickstream data; in all cases, DGX fits these distributions very well, with almost a 99% correlation coefficient in quantile-quantile plots. Our algorithm also scales very well because it requires only a single pass over the data. Finally, we illustrate the power of DGX as a new tool for data mining tasks, such as outlier detection.
KDD	A robust and scalable clustering algorithm for mixed type attributes in large database environment.	Tom Chiu,DongPing Fang,John Chen,Yao Wang,Christopher Jeris	2001	Clustering is a widely used technique in data mining applications to discover patterns in the underlying data. Most traditional clustering algorithms are limited to handling datasets that contain either continuous or categorical attributes. However, datasets with mixed types of attributes are common in real life data mining problems. In this paper, we propose a distance measure that enables clustering data with both continuous and categorical attributes. This distance measure is derived from a probabilistic model that the distance between two clusters is equivalent to the decrease in log-likelihood function as a result of merging. Calculation of this measure is memory efficient as it depends only on the merging cluster pair and not on all the other clusters. Zhang et al [8] proposed a clustering method named BIRCH that is especially suitable for very large datasets. We develop a clustering algorithm using our distance measure based on the framework of BIRCH. Similar to BIRCH, our algorithm first performs a pre-clustering step by scanning the entire dataset and storing the dense regions of data records in terms of summary statistics. A hierarchical clustering algorithm is then applied to cluster the dense regions. Apart from the ability of handling mixed type of attributes, our algorithm differs from BIRCH in that we add a procedure that enables the algorithm to automatically determine the appropriate number of clusters and a new strategy of assigning cluster membership to noisy data. For data with mixed type of attributes, our experimental results confirm that the algorithm not only generates better quality clusters than the traditional k-means algorithms, but also exhibits good scalability properties and is able to identify the underlying number of clusters in the data correctly. The algorithm is implemented in the commercial data mining tool Clementine 6.0 which supports the PMML standard of data mining model deployment.
KDD	Data mining criteria for tree-based regression and classification.	Andreas Buja,Yung-Seop Lee	2001	"This paper is concerned with the construction of regression and classification trees that are more adapted to data mining applications than conventional trees. To this end, we propose new splitting criteria for growing trees. Conventional splitting criteria attempt to perform well on both sides of a split by attempting a compromise in the quality of fit between the left and the right side. By contrast, we adopt a data mining point of view by proposing criteria that search for interesting subsets of the data, as opposed to modeling all of the data equally well. The new criteria do not split based on a compromise between the left and the right bucket; they effectively pick the more interesting bucket and ignore the other.As expected, the result is often a simpler characterization of interesting subsets of the data. Less expected is that the new criteria often yield whole trees that provide more interpretable data descriptions. Surprisingly, it is a ""flaw"" that works to their advantage: The new criteria have an increased tendency to accept splits near the boundaries of the predictor ranges. This so-called ""end-cut problem"" leads to the repeated peeling of small layers of data and results in very unbalanced but highly expressive and interpretable trees."
KDD	Probabilistic modeling of transaction data with applications to profiling, visualization, and prediction.	Igor V. Cadez,Padhraic Smyth,Heikki Mannila	2001	"Transaction data is ubiquitous in data mining applications. Examples include market basket data in retail commerce, telephone call records in telecommunications, and Web logs of individual page-requests at Web sites. Profiling consists of using historical transaction data on individuals to construct a model of each individual's behavior. Simple profiling techniques such as histograms do not generalize well from sparse transaction data. In this paper we investigate the application of probabilistic mixture models to automatically generate profiles from large volumes of transaction data. In effect, the mixture model represents each individual's behavior as a linear combination of ""basis transactions."" We evaluate several variations of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram-based techniques, as well as being relatively scalable, interpretable, and flexible. In addition we point to applications in outlier detection, customer ranking, interactive visualization, and so forth. The paper concludes by comparing and relating the proposed framework to other transaction-data modeling techniques such as association rules."
KDD	Gaining insights into support vector machine pattern classifiers using projection-based tour methods.	Doina Caragea,Dianne Cook,Vasant Honavar	2001	This paper discusses visual methods that can be used to understand and interpret the results of classification using support vector machines (SVM) on data with continuous real-valued variables. SVM induction algorithms build pattern classifiers by identifying a maximal margin separating hyperplane from training examples in high dimensional pattern spaces or spaces induced by suitable nonlinear kernel transformations over pattern spaces. SVM have been demonstrated to be quite effective in a number of practical pattern classification tasks. Since the separating hyperplane is defined in terms of more than two variables it is necessary to use visual techniques that can navigate the viewer through high-dimensional spaces. We demonstrate the use of projection-based tour methods to gain useful insights into SVM classifiers with linear kernels on 8-dimensional data.
KDD	Estimating business targets.	Piew Datta,James Drew,Andrew Betz,D. R. Mani,Jeffery Howard	2001	Determining and setting maximal revenue expectations or other business performance targets---whether it is for regional company divisions or individual customers---can have profound financial implications. Operational techniques are changed, staffing levels are altered and management attention is re-focused---all in the name of expectations. In practice these expectations are often derived in an ad hoc manner. To address this unsupervised task, we combine nearest neighbor methods and classical statistical methods and derive a new solution to the classical econometric task of frontier analysis. We apply our methodology to two real world business problems in Verizon, a major telecommunications provider in the United States, more specifically in the print yellow page division Verizon Information Services: (1) identifying under marketed customers for targeted upselling campaigns and focused sales attention, and (2) benchmarking regional directory divisions to incent performance improvements. Our analysis uncovers some commercially useful aspects of these domains and by conservative estimates can increase revenue by several million dollars in each domain.
KDD	Co-clustering documents and words using bipartite spectral graph partitioning.	Inderjit S. Dhillon	2001	Both document clustering and word clustering are well studied problems. Most existing algorithms cluster documents and words separately but not simultaneously. In this paper we present the novel idea of modeling the document collection as a bipartite graph between documents and words, using which the simultaneous clustering problem can be posed as a bipartite graph partitioning problem. To solve the partitioning problem, we use a new spectral co-clustering algorithm that uses the second left and right singular vectors of an appropriately scaled word-document matrix to yield good bipartitionings. The spectral algorithm enjoys some optimality properties; it can be shown that the singular vectors solve a real relaxation to the NP-complete graph bipartitioning problem. We present experimental results to verify that the resulting co-clustering algorithm works well in practice.
KDD	PVA: a self-adaptive personal view agent system.	Chien Chin Chen,Meng Chang Chen,Yeali S. Sun	2001	"In this paper, we present PVA, an adaptive personal view information agent system to track, learn and manage, user's interests in Internet documents. When user's interests change, PVA, in not only the contents, but also in the structure of user profile, is modified to adapt to the changes. Experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems."
KDD	A spectral method to separate disconnected and nearly-disconnected web graph components.	Chris H. Q. Ding,Xiaofeng He,Hongyuan Zha	2001	Separation of connected components from a graph with disconnected graph components mostly use breadth-first search (BFS) or depth-first search (DFS) graph algorithms. Here we propose a new algebraic method to separate disconnected and nearly-disconnected components. This method is based on spectral graph partitioning, following a key observation that disconnected components will show up, after properly sorted, as step-function like curve in the lowest eigenvectors of the Laplacian matrix of the graph. Following an perturbative analysis framework, we systematically analyzed the graph structures, first on the disconnected subgraph case, and second on the effects of adding edges sparsely connecting different subgraphs as a perturbation. Several new results are derived, providing insights to spectral methods and related clustering objective function. Examples are given illustrating the concepts and results our methods. Comparing to the standard graph algorithms, this method has the same O(&Verbar;E &Verbar; + &Verbar;V&Verbar;log(&Verbar;V&Verbar;)) complexity, but is easier to implement (using readily available eigensolvers). Further more the method can easily identify articulation points and bridges on nearly-disconnected graphs. Segmentation of a real example of Web graph for query amazon is given. We found that each disconnected or nearly-disconnected components forms a cluster on a clear topic.
KDD	GESS: a scalable similarity-join algorithm for mining large data sets in high dimensional spaces.	Jens-Peter Dittrich,Bernhard Seeger	2001	The similarity join is an important operation for mining high-dimensional feature spaces. Given two data sets, the similarity join computes all tuples (x, y) that are within a distance &egr;.One of the most efficient algorithms for processing similarity-joins is the Multidimensional-Spatial Join (MSJ) by Koudas and Sevcik. In our previous work --- pursued for the two-dimensional case --- we found however that MSJ has several performance shortcomings in terms of CPU and I/O cost as well as memory-requirements. Therefore, MSJ is not generally applicable to high-dimensional data.In this paper, we propose a new algorithm named Generic External Space Sweep (GESS). GESS introduces a modest rate of data replication to reduce the number of expensive distance computations. We present a new cost-model for replication, an I/O model, and an inexpensive method for duplicate removal. The principal component of our algorithm is a highly flexible replication engine.Our analytical model predicts a tremendous reduction of the number of expensive distance computations by several orders of magnitude in comparison to MSJ (factor 107). In addition, the memory requirements of GESS are shown to be lower by several orders of magnitude. Furthermore, the I/O cost of our algorithm is by factor 2 better (independent from the fact whether replication occurs or not). Our analytical results are confirmed by a large series of simulations and experiments with synthetic and real high-dimensional data sets.
KDD	Mining the network value of customers.	Pedro Domingos,Matthew Richardson	2001	"One of the major applications of data mining is in helping companies determine which potential customers to market to. If the expected profit from a customer is greater than the cost of marketing to her, the marketing action for that customer is executed. So far, work in this area has considered only the intrinsic value of the customer (i.e, the expected profit from sales to her). We propose to model also the customer's network value: the expected profit from sales to other customers she may influence to buy, the customers those may influence, and so on recursively. Instead of viewing a market as a set of independent entities, we view it as a social network and model it as a Markov random field. We show the advantages of this approach using a social network mined from a collaborative filtering database. Marketing that exploits the network value of customers---also known as viral marketing---can be extremely effective, but is still a black art. Our work can be viewed as a step towards providing a more solid foundation for it, taking advantage of the availability of large relevant databases."
KDD	Proximal support vector machine classifiers.	Glenn Fung,Olvi L. Mangasarian	2001	Instead of a standard support vector machine (SVM) that classifies points by assigning them to one of two disjoint half-spaces, points are classified by assigning them to the closest of two parallel planes (in input or feature space) that are pushed apart as far as possible. This formulation, which can also be interpreted as regularized least squares and considered in the much more general context of regularized networks [8, 9], leads to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equations. In contrast, standard SVMs solve a quadratic or a linear program that require considerably longer computational time. Computational results on publicly available datasets indicate that the proposed proximal SVM classifier has comparable test set correctness to that of standard SVM classifiers, but with considerably faster computational time that can be an order of magnitude faster. The linear proximal SVM can easily handle large datasets as indicated by the classification of a 2 million point 10-attribute set in 20.8 seconds. All computational results are based on 6 lines of MATLAB code.
KDD	Empirical bayes screening for multi-item associations.	William DuMouchel,Daryl Pregibon	2001	"This paper considers the framework of the so-called ""market basket problem"", in which a database of transactions is mined for the occurrence of unusually frequent item sets. In our case, ""unusually frequent"" involves estimates of the frequency of each item set divided by a baseline frequency computed as if items occurred independently. The focus is on obtaining reliable estimates of this measure of interestingness for all item sets, even item sets with relatively low frequencies. For example, in a medical database of patient histories, unusual item sets including the item ""patient death"" (or other serious adverse event) might hopefully be flagged with as few as 5 or 10 occurrences of the item set, it being unacceptable to require that item sets occur in as many as 0.1% of millions of patient reports before the data mining algorithm detects a signal. Similar considerations apply in fraud detection applications. Thus we abandon the requirement that interesting item sets must contain a relatively large fixed minimal support, and adopt a criterion based on the results of fitting an empirical Bayes model to the item set counts. The model allows us to define a 95% Bayesian lower confidence limit for the ""interestingness"" measure of every item set, whereupon the item sets can be ranked according to their empirical Bayes confidence limits. For item sets of size J > 2, we also distinguish between multi-item associations that can be explained by the observed J(J-1)/2 pairwise associations, and item sets that are significantly more frequent than their pairwise associations would suggest. Such item sets can uncover complex or synergistic mechanisms generating multi-item associations. This methodology has been applied within the U.S. Food and Drug Administration (FDA) to databases of adverse drug reaction reports and within AT&T to customer international calling histories. We also present graphical techniques for exploring and understanding the modeling results."
KDD	Data mining: are we there yet?	Herb Edelstein	2001	"Data mining started its move out of the statistics and machine learning ghettos and into the mainstream almost 10 years ago. With great fanfare and a large influx of venture capital, data mining was going to change the very nature of business. Yet data mining products have had relatively modest success in the marketplace. The reasons include limitations and misplaced emphasis in the products' features and functions, unrealistic expectations set by messages from the data mining community, and a lack of readiness by many prospective users. This session will look at where vendors have succeeded and failed with their products, what expectations users should have, and suggestions for achieving the potential of this exciting and valuable technology."
KDD	Magical thinking in data mining: lessons from CoIL challenge 2000.	Charles Elkan	2001	CoIL challenge 2000 was a supervised learning contest that attracted 43 entries. The authors of 29 entries later wrote explanations of their work. This paper discusses these reports and reaches three main conclusions. First, naive Bayesian classifiers remain competitive in practice: they were used by both the winning entry and the next best entry. Second, identifying feature interactions correctly is important for maximizing predictive accuracy: this was the difference between the winning classifier and all others. Third and most important, too many researchers and practitioners in data mining do not appreciate properly the issue of statistical significance and the danger of overfitting. Given a dataset such as the one for the CoIL contest, it is pointless to apply a very complicated learning algorithm, or to perform a very time-consuming model search. In either ease, one is likely to overfit the training data and to fool oneself in estimating predictive accuracy and in discovering useful correlations.
KDD	Data mining with sparse grids using simplicial basis functions.	Jochen Garcke,Michael Griebel	2001	Recently we presented a new approach [18] to the classification problem arising in data mining. It is based on the regularization network approach but, in contrast to other methods which employ ansatz functions associated to data points, we use a grid in the usually high-dimensional feature space for the minimization process. To cope with the curse of dimensionality, we employ sparse grids [49]. Thus, only O(hn-1nd-1) instead of O(hn-d) grid points and unknowns are involved. Here d denotes the dimension of the feature space and hn = 2-n gives the mesh size. We use the sparse grid combination technique [28] where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension. The sparse grid solution is then obtained by linear combination. In contrast to our former work, where d-linear functions were used, we now apply linear basis functions based on a simplicial discretization. This allows to handle more dimensions and the algorithm needs less operations per data point.We describe the sparse grid combination technique for the classification problem, give implementational details and discuss the complexity of the algorithm. It turns out that the method scales linearly with the number of given data points. Finally we report on the quality of the classifier built by our new method on data sets with up to 10 dimensions. It turns out that our new method achieves correctness rates which are competitive to that of the best existing methods.
KDD	Funnel report mining for the MSN network.	Teresa Mah,Hank Hoek,Ying Li	2001	"Data mining research has long concentrated on the five main areas: clustering, association discovery, classification, forecasting and sequential patterns. Web data mining projects are concerned mainly with text mining, user segmentation, forecasting web usage and analyzing users' clickstream patterns. We present a new type of web usage mining called funnel analysis or funnel report mining. A funnel report is a study of the retention behavior among a series of pages or sites. For example, of all hits on the home page of www.msn.com, what percentages of those are followed by hits to moneycentral.msn.com? What percentage of www.msn.com hits are followed by moneycentral.msn.com, and then www.msnbc.com? What are the most interesting funnels starting with www.msn.com? Where does the greatest drop off rate occur after a user has hit MSNBC? Funnel reports are extremely useful in e-business because they give product planners an idea of how usable and well-structured their site is. From our experience performing web usage mining for the MSN network of sites, funnel reports are requested even more than user segmentation analyses, site affiliation studies and classification exercises. In this paper, we define a framework for funnel analysis and provide a tree-based solution we have been using successfully to extract all relevant funnels using only one scan of the data file."
KDD	Mining from open answers in questionnaire data.	Hang Li,Kenji Yamanishi	2001	Surveys are an important part of marketing and customer relationship management, and open answers (i.e., answers to open questions) in particular may contain valuable information and provide an important basis for making business decisions. We have developed a text mining system that provides a new way for analyzing open answers in questionnaire data. The product is able to perform the following two functions: (A) accurate extraction of characteristics for individual analysis targets, (B) accurate extraction of the relationships among characteristics of analysis targets. In this paper, we describe the working of our text mining system. It employs two statistical learning techniques: rule analysis and Correspondence Analysis for performing the two functions. Our text mining system has already been put into use by a number of large corporations in Japan in the performance of text mining on various types of survey data, including open answers about brand images, open answers about company images, complaints about products, comments written on home pages, business reports, and help desk records. In this it has been found to be useful in forming a basis for effective business decisions.
KDD	Clustering spatial data using random walks.	David Harel,Yehuda Koren	2001	Discovering significant patterns that exist implicitly in huge spatial databases is an important computational task. A common approach to this problem is to use cluster analysis. We propose a novel approach to clustering, based on the deterministic analysis of random walks on a weighted graph generated from the data. Our approach can decompose the data into arbitrarily shaped clusters of different sizes and densities, overcoming noise and outliers that may blur the natural decomposition of the data. The method requires only O(n log n) time, and one of its variants needs only constant space.
KDD	REVI-MINER, a KDD-environment for deviation detection and analysis of warranty and goodwill cost statements in automotive industry.	Edgar Hotz,Udo Grimmer,W. Heuser,Gholamreza Nakhaeizadeh,M. Wieczorek	2001	REVI-MINER is a KDD-environment which supports the detection and analysis of deviations in warranty and goodwill cost statements. The system was developed within the framework of a cooperation between DaimlerChrysler Research & Technology and Global Service and Parts (GSP) and is based upon the CRISP-DM methodology as a widely accepted process model for the solution of Data Mining problems. Also, we have implemented different approaches based on Machine learning and statistics which can be utilized for data cleaning in the preprocessing phase. The Data Mining models applied have been developed by using a statistical deviation detection approach. The tool supports controllers in their task of auditing the authorized repair shops. In this paper we describe the development phases which have led to REVI-MINER.
KDD	Data mining techniques to improve forecast accuracy in airline business.	Christoph Hueglin,Francesco Vannotti	2001	Predictive models developed by applying Data Mining techniques are used to improve forecasting accuracy in the airline business. In order to maximize the revenue on a flight, the number of seats available for sale is typically higher than the physical seat capacity (overbooking). To optimize the overbooking rate, an accurate estimation of the number of no-show passengers (passengers who hold a valid booking but do not appear at the gate to board for the flight) is essential. Currently, no-shows on future flights are estimated from the number of no-shows on historical flights averaged on booking class level. In this work, classification trees and logistic regression models are applied to estimate the probability that an individual passenger turns out to be a no-show. Passenger information stored in the reservation system of the airline is either directly used as explanatory variable or used to create attributes that have an impact on the probability of a passenger to be a no-show. The total number of no-shows in each booking class or on the total flight is then obtained by accumulating the individual no-show probabilities over the entity of interest. We show that this forecasting approach is more accurate than the currently used method. In addition, the selected models lead to a deepened insight into passenger behavior.
KDD	Mining time-changing data streams.	Geoff Hulten,Laurie Spencer,Pedro Domingos	2001	Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
KDD	Solving regression problems with rule-based ensemble classifiers.	Nitin Indurkhya,Sholom M. Weiss	2001	We describe a lightweight learning method that induces an ensemble of decision-rule solutions for regression problems. Instead of direct prediction of a continuous output variable, the method discretizes the variable by k-means clustering and solves the resultant classification problem. Predictions on new examples are made by averaging the mean values of classes with votes that are close in number to the most likely class. We provide experimental evidence that this indirect approach can often yield strong results for many applications, generally outperforming direct approaches such as regression trees and rivaling bagged regression trees.
KDD	Mining top-n local outliers in large databases.	Wen Jin,Anthony K. H. Tung,Jiawei Han	2001	"Outlier detection is an important task in data mining with numerous applications, including credit card fraud detection, video surveillance, etc. A recent work on outlier detection has introduced a novel notion of local outlier in which the degree to which an object is outlying is dependent on the density of its local neighborhood, and each object can be assigned a Local Outlier Factor (LOF) which represents the likelihood of that object being an outlier. Although the concept of local outliers is a useful one, the computation of LOF values for every data objects requires a large number of &kgr;-nearest neighbors searches and can be computationally expensive. Since most objects are usually not outliers, it is useful to provide users with the option of finding only n most outstanding local outliers, i.e., the top-n data objects which are most likely to be local outliers according to their LOFs. However, if the pruning is not done carefully, finding top-n outliers could result in the same amount of computation as finding LOF for all objects. In this paper, we propose a novel method to efficiently find the top-n local outliers in large databases. The concept of ""micro-cluster"" is introduced to compress the data. An efficient micro-cluster-based local outlier mining algorithm is designed based on this concept. As our algorithm can be adversely affected by the overlapping in the micro-clusters, we proposed a meaningful cut-plane solution for overlapping data. The formal analysis and experiments show that this method can achieve good performance in finding the most outstanding local outliers."
KDD	Generalized clustering, supervised learning, and data assignment.	Annaka Kalton,Pat Langley,Kiri Wagstaff,Jungsoon P. Yoo	2001	Clustering algorithms have become increasingly important in handling and analyzing data. Considerable work has been done in devising effective but increasingly specific clustering algorithms. In contrast, we have developed a generalized framework that accommodates diverse clustering algorithms in a systematic way. This framework views clustering as a general process of iterative optimization that includes modules for supervised learning and instance assignment. The framework has also suggested several novel clustering methods. In this paper, we investigate experimentally the efficacy of these algorithms and test some hypotheses about the relation between such unsupervised techniques and the supervised methods embedded in them.
KDD	Visualizing multi-dimensional clusters, trends, and outliers using star coordinates.	Eser Kandogan	2001	Interactive visualizations are effective tools in mining scientific, engineering, and business data to support decision-making activities. Star Coordinates is proposed as a new multi-dimensional visualization technique, which supports various interactions to stimulate visual thinking in early stages of knowledge discovery process. In Star Coordinates, coordinate axes are arranged on a two-dimensional surface, where each axis shares the same origin point. Each multi-dimensional data element is represented by a point, where each attribute of the data contributes to its location through uniform encoding. Interaction features of Star Coordinates provide users the ability to apply various transformations dynamically, integrate and separate dimensions, analyze correlations of multiple dimensions, view clusters, trends, and outliers in the distribution of data, and query points based on data ranges. Our experience with Star Coordinates shows that it is particularly useful for the discovery of hierarchical clusters, and analysis of multiple factors providing insight in various real datasets including telecommunications churn.
KDD	Ensemble-index: a new approach to indexing large databases.	Eamonn J. Keogh,Selina Chu,Michael J. Pazzani	2001	The problem of similarity search (query-by-content) has attracted much research interest. It is a difficult problem because of the inherently high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier Transform (DFT), the Discrete Wavelet Transform (DWT) and Piecewise Polynomial Approximation. In this work, we introduce a novel framework for using ensembles of two or more representations for more efficient indexing. The basic idea is that instead of committing to a single representation for an entire dataset, different representations are chosen for indexing different parts of the database. The representations are chosen based upon a local view of the database. For example, sections of the data that can achieve a high fidelity representation with wavelets are indexed as wavelets, but highly spectral sections of the data are indexed using the Fourier transform. At query time, it is necessary to search several small heterogeneous indices, rather than one large homogeneous index. As we will theoretically and empirically demonstrate this results in much faster query response times.
KDD	Robust space transformations for distance-based operations.	Edwin M. Knorr,Raymond T. Ng,Ruben H. Zamar	2001	"For many KDD operations, such as nearest neighbor search, distance-based clustering, and outlier detection, there is an underlying &kgr;-D data space in which each tuple/object is represented as a point in the space. In the presence of differing scales, variability, correlation, and/or outliers, we may get unintuitive results if an inappropriate space is used.The fundamental question that this paper addresses is: ""What then is an appropriate space?"" We propose using a robust space transformation called the Donoho-Stahel estimator. In the first half of the paper, we show the key properties of the estimator. Of particular importance to KDD applications involving databases is the stability property, which says that in spite of frequent updates, the estimator does not: (a) change much, (b) lose its usefulness, or (c) require re-computation. In the second half, we focus on the computation of the estimator for high-dimensional databases. We develop randomized algorithms and evaluate how well they perform empirically. The novel algorithm we develop called the Hybrid-random algorithm is, in most cases, at least an order of magnitude faster than the Fixed-angle and Subsampling algorithms."
KDD	Mining e-commerce data: the good, the bad, and the ugly.	Ron Kohavi	2001	Organizations conducting Electronic Commerce (e-commerce) can greatly benefit from the insight that data mining of transactional and clickstream data provides. Such insight helps not only to improve the electronic channel (e.g., a web site), but it is also a learning vehicle for the bigger organization conducting business at brick-and-mortar stores. The e-commerce site serves as an early alert system for emerging patterns and a laboratory for experimentation. For successful data mining, several ingredients are needed and e-commerce provides all the right ones (the Good). Web server logs, which are commonly used as the source of data for mining e-commerce data, were designed to debug web servers, and the data they provide is insufficient, requiring the use of heuristics to reconstruct events. Moreover, many events are never logged in web server logs, limiting the source of data for mining (the Bad). Many of the problems of dealing with web server log data can be resolved by properly architecting the e-commerce sites to generate data needed for mining. Even with a good architecture, however, there are challenging problems that remain hard to solve (the Ugly). Lessons and metrics based on mining real e-commerce data are presented.
KDD	Molecular feature mining in HIV data.	Stefan Kramer,Luc De Raedt,Christoph Helma	2001	"We present the application of Feature Mining techniques to the Developmental Therapeutics Program's AIDS antiviral screen database. The database consists of 43576 compounds, which were measured for their capability to protect human cells from HIV-1 infection. According to these measurements, the compounds were classified as either active, moderately active or inactive. The distribution of classes is extremely skewed: Only 1.3 % of the molecules is known to be active, and 2.7 % is known to be moderately active.Given this database, we were interested in molecular substructures (i.e., features) that are frequent in the active molecules, and infrequent in the inactives. In data mining terms, we focused on features with a minimum support in active compounds and a maximum support in inactive compounds. We analyzed the database using the levelwise version space algorithm that forms the basis of the inductive query and database system MOLFEA (Molecular Feature Miner). Within this framework, it is possible to declaratively specify the features of interest, such as the frequency of features on (possibly different) datasets as well as on the generality and syntax of them. Assuming that the detected substructures are causally related to biochemical mechanisms, it should be possible to facilitate the development of new pharmaceuticals with improved activities."
KDD	Mining a stream of transactions for customer patterns.	Diane Lambert,José C. Pinheiro	2001	Transaction data can arrive at a ferocious rate in the order that transactions are completed. The data contain an enormous amount of information about customers, not just transactions, but extracting up-to-date customer information from an ever changing stream of data and mining it in real-time is a challenge. This paper describes a statistically principled approach to designing short, accurate summaries or signatures of high dimensional customer behavior that can be kept current with a stream of transactions. A signature database can then be used for data mining and to provide approximate answers to many kinds of queries about current customers quickly and accurately, as an empirical study of the calling patterns of 96,000 wireless customers who made about 18 million wireless calls over a three month period shows.
KDD	The distributed boosting algorithm.	Aleksandar Lazarevic,Zoran Obradovic	2001	In this paper, we propose a general framework for distributed boosting intended for efficient integrating specialized classifiers learned over very large and distributed homogeneous databases that cannot be merged at a single location. Our distributed boosting algorithm can also be used as a parallel classification technique, where a massive database that cannot fit into main computer memory is partitioned into disjoint subsets for a more efficient analysis. In the proposed method, at each boosting round the classifiers are first learned from disjoint datasets and then exchanged amongst the sites. Finally the classifiers are combined into a weighted voting ensemble on each disjoint data set. The ensemble that is applied to an unseen test set represents an ensemble of ensembles built on all distributed sites. In experiments performed on four large data sets the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring less memory and less computational time. In addition, the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large-scale databases.
KDD	Induction of semantic classes from natural language text.	Dekang Lin,Patrick Pantel	2001	Many applications dealing with textual information require classification of words into semantic classes (or concepts). However, manually constructing semantic classes is a tedious task. In this paper, we present an algorithm, UNICON, for UNsupervised Induction of CONcepts. Some advantages of UNICON over previous approaches include the ability to classify words with low frequency counts, the ability to cluster a large number of elements in a high-dimensional space, and the ability to classify previously unknown words into existing clusters. Furthermore, since the algorithm is unsupervised, a set of concepts may be constructed for any corpus.
KDD	DIRT @SBT@discovery of inference rules from text.	Dekang Lin,Patrick Pantel	2001	"In this paper, we propose an unsupervised method for discovering inference rules from text, such as ""X is author of Y &ap; X wrote Y"", ""X solved Y &ap; X found a solution to Y"", and ""X caused Y &ap; Y is triggered by X"". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."
KDD	Identifying non-actionable association rules.	Bing Liu,Wynne Hsu,Yiming Ma	2001	Building predictive models and finding useful rules are two important tasks of data mining. While building predictive models has been well studied, finding useful rules for action still presents a major problem. A main obstacle is that many data mining algorithms often produce too many rules. Existing research has shown that most of the discovered rules are actually redundant or insignificant. Pruning techniques have been developed to remove those spurious and/or insignificant rules. In this paper, we argue that being a significant rule (or a non-redundant rule), however, does not mean that it is a potentially useful rule for action. Many significant rules (unpruned rules) are in fact not actionable. This paper studies this issue and presents an efficient algorithm to identify these non-actionable rules. Experiment results on many real-life datasets show that the number of non-actionable rules is typically quite large. The proposed technique thus enables the user to focus on fewer rules and to be assured that the remaining rules are non-redundant and potentially useful for action.
KDD	Discovering the set of fundamental rule changes.	Bing Liu,Wynne Hsu,Yiming Ma	2001	The world around us changes constantly. Knowing what has changed is an important part of our lives. For businesses, recognizing changes is also crucial. It allows businesses to adapt themselves to the changing market needs. In this paper, we study changes of association rules from one time period to another. One approach is to compare the supports and/or confidences of each rule in the two time periods and report the differences. This technique, however, is too simplistic as it tends to report a huge number of rule changes, and many of them are, in fact, simply the snowball effect of a small subset of fundamental changes. Here, we present a technique to highlight the small subset of fundamental changes. A change is fundamental if it cannot be explained by some other changes. The proposed technique has been applied to a number of real-life datasets. Experiments results show that the number of rules whose changes are unexplainable is quite small (about 20% of the total number of changes discovered), and many of these unexplainable changes reflect some fundamental shifts in the application domain.
KDD	"Discovering unexpected information from your competitors' web sites."	Bing Liu,Yiming Ma,Philip S. Yu	2001	"Ever since the beginning of the Web, finding useful information from the Web has been an important problem. Existing approaches include keyword-based search, wrapper-based information extraction, Web query and user preferences. These approaches essentially find information that matches the user's explicit specifications. This paper argues that this is insufficient. There is another type of information that is also of great interest, i.e., unexpected information, which is unanticipated by the user. Finding unexpected information is useful in many applications. For example, it is useful for a company to find unexpected information bout its competitors, e.g., unexpected services and products that its competitors offer. With this information, the company can learn from its competitors and/or design counter measures to improve its competitiveness. Since the number of pages of a typical commercial site is very large and there are also many relevant sites (competitors), it is very difficult for a human user to view each page to discover the unexpected information. Automated assistance is needed. In this paper, we propose a number of methods to help the user find various types of unexpected information from his/her competitors' Web sites. Experiment results show that these techniques are very useful in practice and also efficient."
KDD	Finding simple intensity descriptions from event sequence data.	Heikki Mannila,Marko Salmenkivi	2001	Sequences of events are an important type of data arising in various applications, including telecommunications, bio-statistics, web access analysis, etc. A basic approach to modeling such sequences is to find the underlying intensity functions describing the expected number of events per time unit. Typically, the intensity functions are assumed to be piecewise constant. We therefore consider different ways of fitting intensity models to event sequence data. We start by considering a Bayesian approach using Markov chain Monte Carlo (MCMC) methods with varying number of pieces. These methods can be used to produce posterior distributions on the intensity functions and they can also accomodate covariates. The drawback is that they are computationally intensive and thus are not very suitable for data mining applications in which large numbers of intensity functions have to be estimated. We consider dynamic programming approaches to finding the change points in the intensity functions. These methods can find the maximum likelihood intensity function in O(n2k) time for a sequence of n events and k different pieces of intensity. We show that simple heuristics can be used to prune the number of potential change points, yielding speedups of several orders of magnitude. The results of the improved dynamic programming method correspond very closely with the posterior averages produced by the MCMC methods.
KDD	Extracting targeted data from the web.	Tom M. Mitchell	2001	"Tom M. Mitchell is author of the textbook ""Machine Learning"" (McGraw Hill, 1997), President of the American Association for Artificial Intelligence and a member of the National Research Council's Computer Science and Telecommunications Board. He is Vice President and Chief Scientist at WhizBang Labs and is currently on a two-year leave of absence from Carnegie Mellon University where he is the Fredkin Professor of Learning and AI in the School of Computer Science and founding Director of CMU's Center for Automated Learning and Discovery. Mitchell's research interests span many areas of Machine Learning theory and practice. His current work at WhizBang Labs involves developing machine learning methods for extracting information from text. For example, WhizBang has developed the world's largest database of job openings by training its software to automatically locate and extract detailed information from job postings on corporate web sites (see www.flipdog.com)."
KDD	Data filtering for automatic classification of rocks from reflectance spectra.	Jonathan Moody,Ricardo Bezerra de Andrade e Silva,Joseph Vanderwaart	2001	The ability to identify the mineral composition of rocks and soils is an important tool for the exploration of geological sites. For instance, NASA intends to design robots that are sufficiently autonomous to perform this task on planetary missions. Spectrometer readings provide one important source of data for identifying sites with minerals of interest. Reflectance spectrometers measure intensities of light reflected from surfaces over a range of wavelengths. Spectral intensity patterns may in some cases be sufficiently distinctive for proper identification of minerals or classes of minerals. For some mineral classes, carbonates for example, specific short spectral intervals are known to carry a distinctive signature. Finding similar distinctive spectral ranges for other mineral classes is not an easy problem. We propose and evaluate data-driven techniques that automatically search for spectral ranges optimized for specific minerals. In one set of studies, we partition the whole interval of wavelengths available in our data into sub-intervals, or bins, and use a genetic algorithm to evaluate a candidate selection of subintervals. As alternatives to this computationally expensive search technique, we present an entropy-based heuristic that gives higher scores for wavelengths more likely to distinguish between classes, as well as other greedy search procedures. Results are presented for four different classes, showing reasonable improvements in identifying some, but not all, of the mineral classes tested.
KDD	Mining frequent neighboring class sets in spatial databases.	Yasuhiko Morimoto	2001	"We consider the problem of finding neighboring class sets. Objects of each instance of a neighboring class set are grouped using their Euclidean distances from each other. Recently, location-based services are growing along with mobile computing infrastructure such as cellular phones and PDAs. Therefore, we expect to see the development of spatial databases that contains very large number of access records including location information. The most typical type would be a database of point objects. Records of the objects may consist of ""requested service name,"" ""number of packet transmitted"" in addition to x and y coordinate values indicating where the request came from. The algorithm presented here efficiently finds sets of ""service names"" that were frequently close to each other in the spatial database. For example, it may find a frequent neighboring class set, where ""ticket"" and ""timetable"" are frequently requested close to each other. By recognizing this, location-based service providers can promote a ""ticket"" service for customers who access the ""timetable."""
KDD	Data mining platform for database developers.	Amir Netz	2001	Data mining platform for database developers.
KDD	Experimental comparisons of online and batch versions of bagging and boosting.	Nikunj C. Oza,Stuart J. Russell	2001	Bagging and boosting are well-known ensemble learning methods. They combine multiple learned base models with the aim of improving generalization performance. To date, they have been used primarily in batch mode, i.e., they require multiple passes through the training data. In previous work, we presented online bagging and boosting algorithms that only require one pass through the training data and presented experimental results on some relatively small datasets. Through additional experiments on a variety of larger synthetic and real datasets, this paper demonstrates that our online versions perform comparably to their batch counterparts in terms of classification accuracy. We also demonstrate the substantial reduction in running time we obtain with our online algorithms because they require fewer passes through the training data.
KDD	"Personalization from incomplete data: what you don't know can hurt."	Balaji Padmanabhan,Zhiqiang Zheng,Steven Orla Kimbrough	2001	"Clickstream data collected at any web site (site-centric data) is inherently incomplete, since it does not capture users' browsing behavior across sites (user-centric data). Hence, models learned from such data may be subject to limitations, the nature of which has not been well studied. Understanding the limitations is particularly important since most current personalization techniques are based on site-centric data only. In this paper, we empirically examine the implications of learning from incomplete data in the context of two specific problems: (a) predicting if the remainder of any given session will result in a purchase and (b) predicting if a given user will make a purchase at any future session. For each of these problems we present new algorithms for fast and accurate data preprocessing of clickstream data. Based on a comprehensive experiment on user-level clickstream data gathered from 20,000 users' browsing behavior, we demonstrate that models built on user-centric data outperform models built on site-centric data for both prediction tasks."
KDD	Probabilistic query models for transaction data.	Dmitry Pavlov,Padhraic Smyth	2001	We investigate the application of Bayesian networks, Markov random fields, and mixture models to the problem of query answering for transaction data sets. We formulate two versions of the querying problem: the query selectivity estimation (i.e., finding exact counts for tuples in a data set) and the query generalization problem (i.e., computing the probability that a tuple will occur in new data). We show that frequent itemsets are useful for reducing the original data to a compressed representation and introduce a method to store them using an ADTree data structure. In an extension of our earlier work on this topic we propose several new schemes for query answering based on the compressed representation that avoid direct scans of the data at query time. Experimental results on real-world transaction data sets provide insights into various tradeoffs involving the offline time for model-building, the online time for query-answering, the memory footprint of the compressed data, and the accuracy of the estimate provided to the query.
KDD	Extracting collective probabilistic forecasts from web games.	David M. Pennock,Steve Lawrence,Finn Årup Nielsen,C. Lee Giles	2001	"Game sites on the World Wide Web draw people from around the world with specialized interests, skills, and knowledge. Data from the games often reflects the players' expertise and will to win. We extract probabilistic forecasts from data obtained from three online games: the Hollywood Stock Exchange (HSX), the Foresight Exchange (FX), and the Formula One Pick Six (F1P6) competition. We find that all three yield accurate forecasts of uncertain future events. In particular, prices of so-called ""movie stocks"" on HSX are good indicators of actual box office returns. Prices of HSX securities in Oscar, Emmy, and Grammy awards correlate well with observed frequencies of winning. FX prices are reliable indicators of future developments in science and technology. Collective predictions from players in the F1 competition serve as good forecasts of true race outcomes. In some cases, forecasts induced from game data are more reliable than expert opinions. We argue that web games naturally attract well-informed and well-motivated players, and thus offer a valuable and oft-overlooked source of high-quality data with significant predictive value."
KDD	Evaluation of prediction models for marketing campaigns.	Saharon Rosset,Einat Neumann,Uri Eick,Nurit Vatnik,Yizhak Idan	2001	"We consider prediction-model evaluation in the context of marketing-campaign planning. In order to evaluate and compare models with specific campaign objectives in mind, we need to concentrate our attention on the appropriate evaluation-criteria. These should portray the model's ability to score accurately and to identify the relevant target population. In this paper we discuss some applicable model-evaluation and selection criteria, their relevance for campaign planning, their robustness under changing population distributions, and their employment when constructing confidence intervals. We illustrate our results with a case study based on our experience from several projects."
KDD	Recommender systems in commerce and community.	John Riedl	2001	Recommender systems have been revolutionizing the way shoppers and information seekers find what they want. We will study some of the tremendous successes and spectacular failures of recommenders in E-commerce to understand the causes of the success or failure. We will leverage that understanding into a set of principles for successfully applying recommenders to business problems. Finally, we will study the economic and social forces that are shaping the evolution of recommenders, and peer into the crystal ball to glimpse the directions the technology will be going in the future.
KDD	Mass collaboration and data mining.	Raghu Ramakrishnan	2001	"Mass Collaboration is a new ""P2P""-style approach to large-scale knowledge sharing, with applications in customer support, focused community development, and capturing knowledge distributed within large organizations. Effectively supporting this paradigm raises many technical challenges, and offers intriguing opportunities for mining massive amounts of data captured continually from user interactions. Data mining offers the promise of increased business intelligence, and also improved user experiences, leading to increased participation and greater quality in the knowledge that is captured, both of which are central objectives in Mass Collaboration. In this talk, I will introduce Mass Collaboration and discuss some important data mining related issues."
KDD	TreeDT: gene mapping by tree disequilibrium test.	Petteri Sevon,Hannu Toivonen,Vesa Ollikainen	2001	We introduce and evaluate TreeDT, a novel gene mapping method which is based on discovering and assessing tree-like patterns in genetic marker data. Gene mapping aims at discovering a statistical connection from a particular disease or trait to a narrow region in the genome. In a typical case-control setting, data consists of genetic markers typed for a set of disease-associated chromosomes and a set of control chromosomes. A computer scientist would view this data as a set of strings.TreeDT extracts, essentially in the form of substrings and prefix trees, information about the historical recombinations in the population. This information is used to locate fragments potentially inherited from a common diseased founder, and to map the disease gene into the most likely such fragment. The method measures for each chromosomal location the disequilibrium of the prefix tree of marker strings starting from the location, to assess the distribution of disease-associated chromosomes.We evaluate experimentally the performance of TreeDT on realistic, simulated data sets, and comparisons to state of the art methods (TDT, HPM) show that TreeDT is very competitive.
KDD	Detecting graph-based spatial outliers: algorithms and applications (a summary of results).	Shashi Shekhar,Chang-Tien Lu,Pusheng Zhang	2001	Identification of outliers can lead to the discovery of unexpected, interesting, and useful knowledge. Existing methods are designed for detecting spatial outliers in multidimensional geometric data sets, where a distance metric is available. In this paper, we focus on detecting spatial outliers in graph structured data sets. We define statistical tests, analyze the statistical foundation underlying our approach, design several fast algorithms to detect spatial outliers, and provide a cost model for outlier detection procedures. In addition, we provide experimental results from the application of our algorithms on a Minneapolis-St.Paul(Twin Cities) traffic dataset to show their effectiveness and usefulness.
KDD	Knowledge base maintenance using knowledge gap analysis.	W. Scott Spangler,Jeffrey T. Kreulen	2001	Knowledge base maintenance using knowledge gap analysis.
KDD	A streaming ensemble algorithm (SEA) for large-scale classification.	W. Nick Street,YongSeog Kim	2001	Ensemble methods have recently garnered a great deal of attention in the machine learning community. Techniques such as Boosting and Bagging have proven to be highly effective but require repeated resampling of the training data, making them inappropriate in a data mining context. The methods presented in this paper take advantage of plentiful data, building separate classifiers on sequential chunks of training points. These classifiers are combined into a fixed-size ensemble using a heuristic replacement strategy. The result is a fast algorithm for large-scale or streaming data that classifies as well as a single decision tree built on all the data, requires approximately constant memory, and adjusts quickly to concept drift.
KDD	Tri-plots: scalable tools for multidimensional data mining.	Agma J. M. Traina,Caetano Traina Jr.,Spiros Papadimitriou,Christos Faloutsos	2001	We focus on the problem of finding patterns across two large, multidimensional datasets. For example, given feature vectors of healthy and of non-healthy patients, we want to answer the following questions: Are the two clouds of points separable? What is the smallest/largest pair-wise distance across the two datasets? Which of the two clouds does a new point (feature vector) come from?We propose a new tool, the tri-plot, and its generalization, the pq-plot, which help us answer the above questions. We provide a set of rules on how to interpret a tri-plot, and we apply these rules on synthetic and real datasets. We also show how to use our tool for classification, when traditional methods (nearest neighbor, classification trees) may fail.
KDD	Mining user session data to facilitate user interaction with a customer service knowledge base in RightNow Web.	Doug Warner,J. Neal Richter,Stephen D. Durbin,Bikramjit Banerjee	2001	"RightNow Web is an integrated software package for web-based customer service that has, at its core, a database of answers to frequently asked questions (FAQs). One major design goal is to facilitate end-user interaction with this dynamic document collection, i.e. make it as easy and efficient as possible for users to browse the collection and locate desired information. To this end, we perform several types of analysis on the session tracking database that records user navigation histories. First, using both explicit and implicit measures of user satisfaction, we infer a ""solved count"" representing the average utility of an FAQ. Second, using the user navigation patterns we construct a link matrix representing connections between FAQs. The technique of building up the link matrix and using it to advise users on related information amounts to a form of the ""swarm intelligence"" method of finding optimal paths. Both solved count and the link matrix are continuously updated as users interact with the site; furthermore, they are periodically ""aged"" to emphasize recent activity. The synergistic combination of these techniques allows users to learn from the database in a more effective manner, as evidenced by usage statistics."
KDD	Discovering associations with numeric variables.	Geoffrey I. Webb	2001	"This paper further develops Aumann and Lindell's [3] proposal for a variant of association rules for which the consequent is a numeric variable. It is argued that these rules can discover useful interactions with numeric data that cannot be discovered directly using traditional association rules with discretization. Alternative measures for identifying interesting rules are proposed. Efficient algorithms are presented that enable these rules to be discovered for dense data sets for which application of Auman and Lindell's algorithm is infeasible."
KDD	Discovering outlier filtering rules from unlabeled data: combining a supervised learner with an unsupervised learner.	Kenji Yamanishi,Jun-ichi Takeuchi	2001	This paper is concerned with the problem of detecting outliers from unlabeled data. In prior work we have developed SmartSifter, which is an on-line outlier detection algorithm based on unsupervised learning from data. On the basis of SmartSifter this paper yields a new framework for outlier filtering using both supervised and unsupervised learning techniques iteratively in order to make the detection process more effective and more understandable. The outline of the framework is as follows: In the first round, for an initial dataset, we run SmartSifter to give each data a score, with a high score indicating a high possibility of being an outlier. Next, giving positive labels to a number of higher scored data and negative labels to a number of lower scored data, we create labeled examples. Then we construct an outlier filtering rule by supervised learning from them. Here the rule is generated based on the principle of minimizing extended stochastic complexity. In the second round, for a new dataset, we filter the data using the constructed rule, then among the filtered data, we run SmartSifter again to evaluate the data in order to update the filtering rule. Applying of our framework to the network intrusion detection, we demonstrate that 1) it can significantly improve the accuracy of SmartSifter, and 2) outlier filtering rules can help the user to discover a general pattern of an outlier group.
KDD	Efficient discovery of error-tolerant frequent itemsets in high dimensions.	Cheng Yang,Usama M. Fayyad,Paul S. Bradley	2001	We present a generalization of frequent itemsets allowing for the notion of errors in the itemset definition. We motivate the problem and present an efficient algorithm that identifies error-tolerant frequent clusters of items in transactional data (customer-purchase data, web browsing data, text, etc.). The algorithm exploits sparseness of the underlying data to find large groups of items that are correlated over database records (rows). The notion of transaction coverage allows us to extend the algorithm and view it as a fast clustering algorithm for discovering segments of similar transactions in binary sparse data. We evaluate the new algorithm on three real-world applications: clustering high-dimensional data, query selectivity estimation and collaborative filtering. Results show that the algorithm consistently uncovers structure in large sparse databases that other traditional clustering algorithms fail to find.
KDD	Real world performance of association rule algorithms.	Zijian Zheng,Ron Kohavi,Llew Mason	2001	This study compares five well-known association rule algorithms using three real-world datasets and an artificial dataset. The experimental results confirm the performance improvements previously claimed by the authors on the artificial data, but some of these gains do not carry over to the real datasets, indicating overfitting of the algorithms to the IBM artificial dataset. More importantly, we found that the choice of algorithm only matters at support levels that generate more rules than would be useful in practice. For support levels that generate less than 1,000,000 rules, which is much more than humans can handle and is sufficient for prediction purposes where data is loaded into RAM, Apriori finishes processing in less than 10 minutes. On our datasets, we observed super-exponential growth in the number of rules. On one of our datasets, a 0.02% change in the support increased the number of rules from less than a million to over a billion, implying that outside a very narrow range of support values, the choice of algorithm is irrelevant.
KDD	Infominer: mining surprising periodic patterns.	Jiong Yang,Wei Wang,Philip S. Yu	2001	In this paper, we focus on mining surprising periodic patterns in a sequence of events. In many applications, e.g., computational biology, an infrequent pattern is still considered very significant if its actual occurrence frequency exceeds the prior expectation by a large margin. The traditional metric, such as support, is not necessarily the ideal model to measure this kind of surprising patterns because it treats all patterns equally in the sense that every occurrence carries the same weight towards the assessment of the significance of a pattern regardless of the probability of occurrence. A more suitable measurement, information, is introduced to naturally value the degree of surprise of each occurrence of a pattern as a continuous and monotonically decreasing function of its probability of occurrence. This would allow patterns with vastly different occurrence probabilities to be handled seamlessly. As the accumulated degree of surprise of all repetitions of a pattern, the concept of information gain is proposed to measure the overall degree of surprise of the pattern within a data sequence. The bounded information gain property is identified to tackle the predicament caused by the violation of the downward closure property by the information gain measure and in turn provides an efficient solution to this problem. Empirical tests demonstrate the efficiency and the usefulness of the proposed model.
KDD	Mining web logs for prediction models in WWW caching and prefetching.	Qiang Yang,Henry Haining Zhang,Ian Tian Yi Li	2001	Web caching and prefetching are well known strategies for improving the performance of Internet systems. When combined with web log mining, these strategies can decide to cache and prefetch web documents with higher accuracy. In this paper, we present an application of web log mining to obtain web-document access patterns and use these patterns to extend the well-known GDSF caching policies and prefetching policies. Using real web logs, we show that this application of data mining can achieve dramatic improvement to web-access performance.
KDD	Learning and making decisions when costs and probabilities are both unknown.	Bianca Zadrozny,Charles Elkan	2001	"In many machine learning domains, misclassification costs are different for different examples, in the same way that class membership probabilities are example-dependent. In these domains, both costs and probabilities are unknown for test examples, so both cost estimators and probability estimators must be learned. This paper first discusses how to make optimal decisions given cost and probability estimates, and then presents decision tree learning methods for obtaining well-calibrated probability estimates. The paper then explains how to obtain unbiased estimators for example- dependent costs, taking into account the difficulty that in general, probabilities and costs are not independent random variables, and the training examples for which costs are known are not representative of all examples. The latter problem is called sample selection bias in econometrics. Our solution to it is based on Nobel prize-winning work due to the economist James Heckman. We show that the methods we propose are successful in a comprehensive comparison with MetaCost that uses the well-known and difficult dataset from the KDD''98 data mining contest."
KDD	Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, San Francisco, CA, USA, August 26-29, 2001	Doheon Lee,Mario Schkolnick,Foster J. Provost,Ramakrishnan Srikant	2001	Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, San Francisco, CA, USA, August 26-29, 2001
PKDD	Support Vectors for Reinforcement Learning.	Thomas G. Dietterich,Xin Wang	2001	Support Vectors for Reinforcement Learning.
PKDD	Self-Similar Layered Hidden Markov Models.	Jafar Adibi,Wei-Min Shen	2001	Self-Similar Layered Hidden Markov Models.
PKDD	Automatic Text Summarization Using Unsupervised and Semi-supervised Learning.	Massih-Reza Amini,Patrick Gallinari	2001	Automatic Text Summarization Using Unsupervised and Semi-supervised Learning.
PKDD	Biological Sequence Data Mining.	Yuh-Jyh Hu	2001	Biological Sequence Data Mining.
PKDD	Knowledge Discovery in Multi-label Phenotype Data.	Amanda Clare,Ross D. King	2001	Knowledge Discovery in Multi-label Phenotype Data.
PKDD	Computing Association Rules Using Partial Totals.	Frans Coenen,Graham Goulbourne,Paul H. Leng	2001	Computing Association Rules Using Partial Totals.
PKDD	Detecting Temporal Change in Event Sequences: An Application to Demographic Data.	Hendrik Blockeel,Johannes Fürnkranz,Alexia Prskawetz,Francesco C. Billari	2001	Detecting Temporal Change in Event Sequences: An Application to Demographic Data.
PKDD	Gaphyl: A Genetic Algorithms Approach to Cladistics.	Clare Bates Congdon	2001	Gaphyl: A Genetic Algorithms Approach to Cladistics.
PKDD	Parametric Approximation Algorithms for High-Dimensional Euclidean Similarity.	Ömer Egecioglu	2001	Parametric Approximation Algorithms for High-Dimensional Euclidean Similarity.
PKDD	Data Structures for Minimization of Total Within-Group Distance for Spatio-temporal Clustering.	Vladimir Estivill-Castro,Michael E. Houle	2001	Data Structures for Minimization of Total Within-Group Distance for Spatio-temporal Clustering.
PKDD	Non-crisp Clustering by Fast, Convergent, and Robust Algorithms.	Vladimir Estivill-Castro,Jianhua Yang	2001	Non-crisp Clustering by Fast, Convergent, and Robust Algorithms.
PKDD	Temporal Rule Discovery for Time-Series Satellite Images and Integration with RDB.	Rie Honda,Osamu Konishi	2001	Temporal Rule Discovery for Time-Series Satellite Images and Integration with RDB.
PKDD	Using Grammatical Inference to Automate Information Extraction from the Web.	Theodore W. Hong,Keith L. Clark	2001	Using Grammatical Inference to Automate Information Extraction from the Web.
PKDD	Discovery of Temporal Patterns. Learning Rules about the Qualitative Behaviour of Time Series.	Frank Höppner	2001	Discovery of Temporal Patterns. Learning Rules about the Qualitative Behaviour of Time Series.
PKDD	Implication-Based Fuzzy Association Rules.	Eyke Hüllermeier	2001	Implication-Based Fuzzy Association Rules.
PKDD	Pattern Extraction for Time Series Classification.	Pierre Geurts	2001	Pattern Extraction for Time Series Classification.
PKDD	Specifying Mining Algorithms with Iterative User-Defined Aggregates: A Case Study.	Fosca Giannotti,Giuseppe Manco,Franco Turini	2001	Specifying Mining Algorithms with Iterative User-Defined Aggregates: A Case Study.
PKDD	A General Measure of Rule Interestingness.	Szymon Jaroszewicz,Dan A. Simovici	2001	A General Measure of Rule Interestingness.
PKDD	Interesting Fuzzy Association Rules in Quantitative Databases.	Jeannette M. de Graaf,Walter A. Kosters,Jeroen J. W. Witteman	2001	Interesting Fuzzy Association Rules in Quantitative Databases.
PKDD	Interestingness Measures for Fuzzy Association Rules.	Attila Gyenesei	2001	Interestingness Measures for Fuzzy Association Rules.
PKDD	A Data Set Oriented Approach for Clustering Algorithm Selection.	Maria Halkidi,Michalis Vazirgiannis	2001	A Data Set Oriented Approach for Clustering Algorithm Selection.
PKDD	Fusion of Meta-knowledge and Meta-data for Case-Based Model Selection.	Melanie Hilario,Alexandros Kalousis	2001	Fusion of Meta-knowledge and Meta-data for Case-Based Model Selection.
PKDD	Error Correcting Codes with Optimized Kullback-Leibler Distances for Text Categorization.	Jörg Kindermann,Gerhard Paass,Edda Leopold	2001	Error Correcting Codes with Optimized Kullback-Leibler Distances for Text Categorization.
PKDD	Propositionalisation and Aggregates.	Arno J. Knobbe,Marc de Haas,Arno Siebes	2001	Propositionalisation and Aggregates.
PKDD	Discovering Fuzzy Classification Rules with Genetic Programming and Co-evolution.	Roberto R. F. Mendes,Fabricio de B. Voznika,Alex Alves Freitas,Júlio C. Nievola	2001	Discovering Fuzzy Classification Rules with Genetic Programming and Co-evolution.
PKDD	Combining Discrete Algorithmic and Probabilistic Approaches in Data Mining.	Heikki Mannila	2001	Combining Discrete Algorithmic and Probabilistic Approaches in Data Mining.
PKDD	Algorithms for the Construction of Concept Lattices and Their Diagram Graphs.	Sergei O. Kuznetsov,Sergei A. Obiedkov	2001	Algorithms for the Construction of Concept Lattices and Their Diagram Graphs.
PKDD	Sentence Filtering for Information Extraction in Genomics, a Classification Problem.	Claire Nedellec,Mohamed Ould Abdel Vetah,Philippe Bessières	2001	Sentence Filtering for Information Extraction in Genomics, a Classification Problem.
PKDD	Data Reduction Using Multiple Models Integration.	Aleksandar Lazarevic,Zoran Obradovic	2001	Data Reduction Using Multiple Models Integration.
PKDD	Text Categorization and Semantic Browsing with Self-Organizing Maps on Non-euclidean Spaces.	Jörg Ontrup,Helge Ritter	2001	Text Categorization and Semantic Browsing with Self-Organizing Maps on Non-euclidean Spaces.
PKDD	A Study on the Hierarchical Data Clustering Algorithm Based on Gravity Theory.	Yen-Jen Oyang,Chien-Yu Chen,Tsui-Wei Yang	2001	A Study on the Hierarchical Data Clustering Algorithm Based on Gravity Theory.
PKDD	Internet Document Filtering Using Fourier Domain Scoring.	Laurence A. F. Park,Marimuthu Palaniswami,Kotagiri Ramamohanarao	2001	Internet Document Filtering Using Fourier Domain Scoring.
PKDD	Distinguishing Natural Language Processes on the Basis of fMRI-Measured Brain Activation.	Francisco Pereira,Marcel Just,Tom M. Mitchell	2001	Distinguishing Natural Language Processes on the Basis of fMRI-Measured Brain Activation.
PKDD	Automatic Construction and Refinement of a Class Hierarchy over Multi-valued Data.	Nathalie Pernelle,Marie-Christine Rousset,Véronique Ventos	2001	Automatic Construction and Refinement of a Class Hierarchy over Multi-valued Data.
PKDD	Bloomy Decision Tree for Multi-objective Classification.	Einoshin Suzuki,Masafumi Gotoh,Yuta Choki	2001	Bloomy Decision Tree for Multi-objective Classification.
PKDD	Comparison of Three Objective Functions for Conceptual Clustering.	Céline Robardet,Fabien Feschet	2001	Comparison of Three Objective Functions for Conceptual Clustering.
PKDD	Identification of ECG Arrhythmias Using Phase Space Reconstruction.	Felice M. Roberts,Richard J. Povinelli,Kristina M. Ropella	2001	Identification of ECG Arrhythmias Using Phase Space Reconstruction.
PKDD	Discovery of Temporal Knowledge in Medical Time-Series Databases Using Moving Average, Multiscale Matching, and Rule Induction.	Shusaku Tsumoto	2001	Discovery of Temporal Knowledge in Medical Time-Series Databases Using Moving Average, Multiscale Matching, and Rule Induction.
PKDD	Mining Positive and Negative Knowledge in Clinical Databases Based on Rough Set Model.	Shusaku Tsumoto	2001	Mining Positive and Negative Knowledge in Clinical Databases Based on Rough Set Model.
PKDD	Statistification or Mystification? The Need for Statistical Thought in Visual Data Mining.	Antony Unwin	2001	Statistification or Mystification? The Need for Statistical Thought in Visual Data Mining.
PKDD	The TwoKey Plot for Multiple Association Rules Control.	Antony Unwin,Heike Hofmann,Klaus Bernt	2001	The TwoKey Plot for Multiple Association Rules Control.
PKDD	Finding Association Rules That Trade Support Optimally against Confidence.	Tobias Scheffer	2001	When evaluating association rules, rules that differ in both support and confidence have to be compared; a larger support has to be traded against a higher confidence. The solution which we propose for this problem is to maximize the expected accuracy that the association rule will have for future data. In a Bayesian framework, we determine the contributions of confidence and support to the expected accuracy on future data. We present a fast algorithm that finds the n best rules which maximize the resulting criterion. The algorithm dynamically prunes redundant rules and parts of the hypothesis space that cannot contain better solutions than the best ones found so far. We evaluate the performance of the algorithm relative to the Apriori algorithm.
PKDD	Lightweight Collaborative Filtering Method for Binary-Encoded Data.	Sholom M. Weiss,Nitin Indurkhya	2001	Lightweight Collaborative Filtering Method for Binary-Encoded Data.
PKDD	The Musical Expression Project: A Challenge for Machine Learning and Knowledge Discovery.	Gerhard Widmer	2001	The Musical Expression Project: A Challenge for Machine Learning and Knowledge Discovery.
PKDD	Scalability, Search, and Sampling: From Smart Algorithms to Active Discovery.	Stefan Wrobel	2001	Scalability, Search, and Sampling: From Smart Algorithms to Active Discovery.
PKDD	Principles of Data Mining and Knowledge Discovery, 5th European Conference, PKDD 2001, Freiburg, Germany, September 3-5, 2001, Proceedings	Luc De Raedt,Arno Siebes	2001	Principles of Data Mining and Knowledge Discovery, 5th European Conference, PKDD 2001, Freiburg, Germany, September 3-5, 2001, Proceedings
ICDM	Unsupervised Clustering of Symbol Strings and Context Recognition.	John A. Flanagan,Jani Mäntyjärvi,Johan Himberg	2002	The representation of information based on symbolstrings has been applied to the recognition of context. Aframework for approaching the context recognition problemhas been described and interpreted in terms of symbolstring recognition. The Symbol String Clustering Map(SCM) is introduced as an efficient algorithm for the unsupervisedclustering and recognition of symbol string data.The SCM can be implemented in an on line manner usinga computationally simple similarity measure based ona weighted average. It is shown how measured sensor datacan be processed by the SCM algorithm to learn, representand distinguish different user contexts without any user input.
ICDM	Mining Online Users? Access Records for Web Business Intelligence.	Simon Fong,Serena Chan	2002	"This paper discusses about how business intelligence on awebsite could be obtained from users' access recordsinstead of web logs of ""hits"". Users' access records arecaptured by implementing an Access-Control (AC)architectural model on the website. This model requiresusers to register their profiles in an exchange of apassword; and thereafter they have to login before gainingaccess to certain resources on the website. The links tothe resources on the website have been modified such thata record of information about the access would berecorded in the database when clicked. This way, data-miningcan be performed on a relatively clean set ofaccess records about the users. Hence, a good deal ofbusiness intelligence about the users' behaviors,preferences and about the popularities of the resources(products) on the website can be gained. In this paper, wealso discussed how the business intelligence acquired, inturn, can be used to provide e-CRM for the users."
ICDM	A Parameterless Method for Efficiently Discovering Clusters of Arbitrary Shape in Large Datasets.	Andrew Foss,Osmar R. Zaïane	2002	Clustering is the problem of grouping data based on similarityand consists of maximizing the intra-group similaritywhile minimizing the inter-group similarity. The problem ofclustering data sets is also known as unsupervised classification,since no class labels are given. However, all exist-ingclustering algorithms require some parameters to steerthe clustering process, such as the famous k for the numberof expected clusters, which constitutes a supervision ofa sort. We present in this paper a new, efficient, fast andscalable clustering algorithm that clusters over a range ofresolutions and finds a potential optimum clustering withoutrequiring any parameter input. Our experiments showthat our algorithm outperforms most existing clustering algorithmsin quality and speed for large data sets.
ICDM	Empirical Comparison of Various Reinforcement Learning Strategies for Sequential Targeted Marketing.	Naoki Abe,Edwin P. D. Pednault,Haixun Wang,Bianca Zadrozny,Wei Fan,Chidanand Apté	2002	"We empirically evaluate the performance of various re-inforcementlearning methods in applications to sequentialtargeted marketing. In particular, we propose and evaluatea progression of reinforcement learning methods, rangingfrom the ""direct"" or ""batch"" methods to ""indirect"" or""simulation based"" methods, and those that we call ""semi-direct""methods that fall between them. We conduct a num-berof controlled experiments to evaluate the performanceof these competing methods. Our results indicate that whilethe indirect methods can perform better in a situation inwhich nearly perfect modeling is possible, under the morerealistic situations in which the system's modeling parametershave restricted attention, the indirect methods' performancetend to degrade. We also show that semi-directmethods are effective in reducing the amount of computationnecessary to attain a given level of performance, andoften result in more profitable policies."
ICDM	Investigative Profiling with Computer Forensic Log Data and Association Rules.	Tamas Abraham,Olivier Y. de Vel	2002	Investigative profiling is an important activity in computerforensics that can narrow the search for one or morecomputer perpetrators. Data mining is a technique that hasproduced good results in providing insight into large volumesof data. This paper describes how the associationrule data mining technique may be employed to generateprofiles from log data and the methodology used for the interpretationof the resulting rule sets. The process relies onbackground knowledge in the form of concept hierarchiesand beliefs, commonly available from, or attainable by, thecomputer forensic investigative team. Results obtained withthe profiling system has identified irregularities in computerlogs.
ICDM	Learning with Progressive Transductive Support Vector Machine.	Yisong Chen,Guoping Wang,Shihai Dong	2002	"Support Vector Machine (SVM) is a new learningmethod developed in recent years based on thefoundations of statistical learning theory. By taking atransductive approach instead of an inductive one insupport vector classifiers, the test set can be used as anadditional source of information about margins. Intuitively,we would expect transductive learning to yieldimprovements when the training sets are small or whenthere is a significant deviation between the training andworking set subsamples of the total population. In thispaper, a progressive transductive support vector machineis addressed to extend Joachims' Transductive SVM tohandle different class distributions. It solves the problemof having to estimate the ratio of positive/negativeexamples from the working set. The experimental resultsshow that the algorithm is very promising."
ICDM	Towards Automatic Generation of Query Taxonomy: A Hierarchical Query Clustering Approach.	Shui-Lung Chuang,Lee-Feng Chien	2002	"Previous works on automatic query clustering most generatea flat, un-nested partition of query terms. In this work,we are pursuing to organize query terms into a hierarchicalstructure and construct a query taxonomy in an automaticway. The proposed approach is designed based on a hierarchicalagglomerative clustering algorithm to hierarchicallygroup similar queries and generate the cluster hierarchiesby a novel cluster partition technique. The search processesof real-world search engines are combined to obtain highlyranked Web documents as the feature source for each queryterm. Preliminary experiments show that the proposed approachis effective to obtain thesaurus information for queryterms, and is also feasible to construct a query taxonomywhich provides a basis for in-depth analysis of users' searchinterests and domain-specific vocabulary on a larger scale."
ICDM	Evolutionary Time Series Segmentation for Stock Data Mining.	Korris Fu-Lai Chung,Tak-Chung Fu,Robert W. P. Luk,Vincent T. Y. Ng	2002	Stock data in the form of multiple time series aredifficult to process, analyze and mine. However, when theycan be transformed into meaningful symbols like technicalpatterns, it becomes an easier task. Most recent work ontime series queries only concentrates on how to identify agiven pattern from a time series. Researchers do notconsider the problem of identifying a suitable set of timepoints for segmenting the time series in accordance with agiven set of pattern templates (e.g., a set of technicalpatterns for stock analysis). On the other hand, using fixedlength segmentation is a primitive approach to thisproblem; hence, a dynamic approach (with highcontrollability) is preferred so that the time series can besegmented flexibly and effectively according to the needs ofthe users and the applications. In view of the facts that sucha segmentation problem is an optimization problem andevolutionary computation is an appropriate tool to solve it,we propose an evolutionary time series segmentationalgorithm. This approach allows a sizeable set of stockpatterns to be generated for mining or query. In addition,defining the similarity between time series (or time seriessegments) is of fundamental importance in fitnesscomputation. By identifying the perceptually importantpoints directly from the time domain, time series segmentsand templates of different lengths can be compared andintuitive pattern matching can be carried out in an effectiveand efficient manner. Encouraging experimental results arereported from tests that segment the time series of selectedHong Kong stocks.
ICDM	Using functional PCA for cardiac motion exploration.	Denis Clot	2002	Principal component analysis (PCA) [14, 6] is a maintool in multivariate data analysis. Its paradigms are alsoused in the Karhunen-Loeve decomposition [5], a standardtool in image processing. Extensions of PCA to the frameworkof functional data have been proposed. The analy-sisprovided by the functional PCA seems to be a powerfultool to find principal sources of variability in curves or images,but it fails in providing us with easy interpretationsin the case of multifunctional data. Guide lines aiming atspot information from the outputs of PCA applied to functionalswith values in space of continuous functions upona bounded domain are proposed. An application to cardiacmotion analysis illustrates the complexity of the multi-functionalframework and the results provided by functionalPCA. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player
ICDM	Text Document Categorization by Term Association.	Maria-Luiza Antonie,Osmar R. Zaïane	2002	A good text classifier is a classifier that efficiently categorizeslarge sets of text documents in a reasonable timeframe and with an acceptable accuracy, and that providesclassification rules that are human readable for possiblefine-tuning. If the training of the classifier is also quick,this could become in some application domains a good assetfor the classifier. Many techniques and algorithms forautomatic text categorization have been devised. Accordingto published literature, some are more accurate than others,and some provide more interpretable classification modelsthan others. However, none can combine all the beneficialproperties enumerated above. In this paper, we present anovel approach for automatic text categorization that borrowsfrom market basket analysis techniques using associationrule mining in the data-mining field. We focus on twomajor problems: (1) finding the best term association rulesin a textual database by generating and pruning; and (2)using the rules to build a text classifier. Our text categorizationmethod proves to be efficient and effective, and experimentson well-known collections show that the classifierperforms well. In addition, training as well as classificationare both fast and the generated rules are human readable.
ICDM	Unsupervised Segmentation of Categorical Time Series into Episodes.	Paul R. Cohen,Brent Heeringa,Niall M. Adams	2002	"This paper describes an unsupervised algorithm forsegmenting categorical time series into episodes. TheVOTING-EXPERTS algorithm first collects statistics aboutthe frequency and boundary entropy of ngrams, then passesa window over the series and has two ""expert methods"" decidewhere in the window boundaries should be drawn. Thealgorithm successfully segments text into words in four languages.The algorithm also segments time series of robotsensor data into subsequences that represent episodes inthe life of the robot. We claim that VOTING-EXPERTSfinds meaningful episodes in categorical time series becauseit exploits two statistical characteristics of meaningfulepisodes."
ICDM	Speed-up Iterative Frequent Itemset Mining with Constraint Changes.	Gao Cong,Bing Liu	2002	Mining of frequent itemsets is a fundamental datamining task. Past research has proposed many efficientalgorithms for the purpose. Recent work also highlightedthe importance of using constraints to focus the miningprocess to mine only those relevant itemsets. In practice,data mining is often an interactive and iterative process.The user typically changes constraints and runs the miningalgorithm many times before satisfied with the finalresults. This interactive process is very time consuming.Existing mining algorithms are unable to take advantageof this iterative process to use previous mining results tospeed up the current mining process. This results inenormous waste in time and in computation. In this paper,we propose an efficient technique to utilize previousmining results to improve the efficiency of current miningwhen constraints are changed. We first introduce theconcept of tree boundary to summarize the usefulinformation available from previous mining. We then showthat the tree boundary provides an effective and efficientframework for the new mining. The proposed techniquehas been implemented in the contexts of two existingfrequent itemset mining algorithms, FP-tree and TreeProjection. Experiment results on both synthetic and real-lifedatasets show that the proposed approach achievesdramatic saving in computation.
ICDM	Online Algorithms for Mining Semi-structured Data Stream.	Tatsuya Asai,Hiroki Arimura,Kenji Abe,Shinji Kawasoe,Setsuo Arikawa	2002	In this paper, we study an online data mining problemfrom streams of semi-structured data such as XML data.Modeling semi-structured data and patterns as labeled orderedtrees, we present an online algorithm StreamT thatreceives fragments of an unseen possibly infinite semi-structureddata in the document order through a datastream, and can return the current set of frequent patternsimmediately on request at any time. A crucial part of our algorithmis the incremental maintenance of the occurrencesof possibly frequent patterns using a tree sweeping technique.We give modifications of the algorithm to other on-linemining model. We present theoretical and empiricalanalyses to evaluate the performance of the algorithm.
ICDM	Optimal Projections of High Dimensional Data.	Emilio Corchado,Colin Fyfe	2002	In this paper, we compare two artificial neuralnetwork algorithms for performing ExploratoryProjection Pursuit, a statistical technique forinvestigating data by projecting it onto lower dimensionalmanifolds. The neural networks are extensions of anetwork which performs Principal Component Analysis.We illustrate the technique on artificial data beforeapplying it to real data.
ICDM	Generating an informative cover for association rules.	Laurentiu Cristofor,Dan A. Simovici	2002	Mining association rules may generate a large numbersof rules making the results hard to analyze manually.Pasquier et al. have discussed the generation of Guigues-Duquenne-Luxenburger basis (GD-L basis). Using a similarapproach, we introduce a new rule of inference anddefine the notion of association rules cover as a minimalset of rules that are non-redundant with respect to this newrule of inference. Our experimental results (obtained usingboth synthetic and real data sets) show that our coversare smaller than the GD-L basis and they are computed intime that is comparable to the classic Apriori algorithm forgenerating rules.
ICDM	A Lazy Approach to Pruning Classification Rules.	Elena Baralis,Paolo Garza	2002	Associative classification is a promising technique forthe generation of highly precise classifiers. Previous workspropose several clever techniques to prune the huge set ofgenerated rules, with the twofold aim of selecting a smallset of high quality rules, and reducing the chance of overfitting.In this paper, we argue that pruning should be reducedto a minimum and that the availability of a large rule basemay improve the precision of the classifier, without affectingits performance. In L3 (Live and Let Live), a new algorithmfor associative classification, a lazy pruning technique iterativelydiscards all rules that only yield wrong case classifications.Classification is performed in two steps. Initially, ruleswhich have already correctly classified at least one trainingcase, sorted by confidence, are considered. If the caseis still unclassified, the remaining rules (unused during thetraining phase) are considered, again sorted by confidence.Extensive experiments on 26 databases from the UCImachine learning database repository show that L3 improvesthe classification precision with respect to previousapproaches.
ICDM	Feature Selection for Clustering - A Filter Solution.	Manoranjan Dash,Kiseok Choi,Peter Scheuermann,Huan Liu	2002	"Processing applications with a large number of dimensionshas been a challenge to the KDD community. Featureselection, an effective dimensionality reduction technique,is an essential pre-processing method to remove noisy features.In the literature there are only a few methods proposedfor feature selection for clustering. And, almost all ofthose methods are wrapper' techniques that require a clusteringalgorithm to evaluate the candidate feature subsets.The wrapper approach is largely unsuitable in real-worldapplications due to its heavy reliance on clustering algorithmsthat require parameters such as number of clusters,and due to lack of suitable clustering criteria to evaluateclustering in different subspaces. In this paper we proposea filter' method that is independent of any clustering algorithm.The proposed method is based on the observationthat data with clusters has very different point-to-point distancehistogram than that of data without clusters. Usingthis we propose an entropy measure that is low if data hasdistinct clusters and high otherwise. The entropy measure issuitable for selecting the most important subset of featuresbecause it is invariant with number of dimensions, and isaffected only by the quality of clustering. Extensive performanceevaluation over synthetic, benchmark, and realdatasets shows its effectiveness."
ICDM	Mining A Set of Coregulated RNA Sequences.	Yuh-Jyh Hu	2002	Post-transcriptional regulation, though less studied, isan important research topic in bioinformatics. In a set ofpost-transcriptionally coregulated RNAs, the basepair interactionscan organize the molecules into domains andprovide a framework for functional interactions. Their consensusmotifs may represent the binding sites of RNA regulatoryproteins. Unlike DNA motifs, RNA motifs are moreconserved in structures than in sequences. Knowing thestructural motifs can help us better understand the regulationactivities. In this paper, we propose a novel data miningapproach to RNA secondary structure prediction. Todemonstrate the performance of our new approach, we firsttested it on the same data sets previously used and publishedin literature. Secondly, to show the flexibility of ournew approach, we also tested it on a data set that containspseudoknot motifs that most current systems cannot identify.
ICDM	Extraction Techniques for Mining Services from Web Sources.	Hasan Davulcu,Saikat Mukherjee,I. V. Ramakrishnan	2002	The Web has established itself as the dominantmedium for doing electronic commerce. Consequentlythe number of service providers, bothlarge and small, advertising their services on theweb continues to proliferate. In this paper we describenew extraction algorithms for mining servicedirectories from web pages. We develop anovel propagation technique for identifying andaccumulating all of the attributes related to a serviceentity in a web page. We provide experimentalresults of the effectiveness of our extractiontechniques by mining a database of veterinarianservice providers from web sources.
ICDM	Neighborgram Clustering Interactive Exploration of Cluster Neighborhoods.	Michael R. Berthold,Bernd Wiswedel,David E. Patterson	2002	"We describe an interactive way to generate a set of clustersfor a given data set. The clustering is done by constructinglocal histograms, which can then be used to visualize,select, and fine-tune potential cluster candidates.The accompanying algorithmcan also generate clusters automatically,allowing for an automatic or semi-automaticclustering process where the user only occasionally interactswith the algorithm. We illustrate the ability to automaticallyidentify and visualize clusters using NCI's AIDSAntiviral Screen data set."
ICDM	A New Algorithm for Learning Parameters of a Bayesian Network from Distributed Data.	R. Chen,Krishnamoorthy Sivakumar	2002	We present a novel approach for learning parametersof a Bayesian network from distributed heterogeneousdataset. In this case, the whole dataset is distributedin several sites and each site contains observations fora different subset of features. The new method usesthe collective learning approach proposed in our earlierwork and substantially reduces the computational andtransmission overhead. Theoretical analysis is givenand experimental results are provided to illustrate theaccuracy and efficiency of our method.
ICDM	webSPADE: A Parallel Sequence Mining Algorithm to Analyze Web Log Data.	Ayhan Demiriz	2002	Enterprise-class web sites receive a large amountof traffic, from both registered and anonymous users.Data warehouses are built to store and help analyze the click streams within this traffic to providecompanies with valuable insights into the behaviorof their customers. This article proposes a parallelsequence mining algorithm, webSPADE, to analyzethe click streams found in site web logs. In this process, raw web logs are first cleaned and inserted intoa data warehouse. The click streams are then minedby webSPADE. An innovative web-based front-endis used to visualize and query the sequence miningresults. The webSPADE algorithm is currently usedby Verizon to analyze the daily traffic of the Verizon.com web site.
ICDM	Iterative Clustering of High Dimensional Text Data Augmented by Local Search.	Inderjit S. Dhillon,Yuqiang Guan,J. Kogan	2002	"The k-means algorithm with cosine similarity, alsoknown as the spherical k-means algorithm, is a popularmethod for clustering document collections. However,spherical k-means can often yield qualitatively poor results,especially when cluster sizes are small, say 25-30 documentsper cluster, where it tends to get stuck at a localmaximum far away from the optimal solution. In this paper,we present a local search procedure, which we call""first-variation"" that refines a given clustering by incrementallymoving data points between clusters, thus achievinga higher objective function value. An enhancement offirst variation allows a chain of such moves in a Kernighan-Linfashion and leads to a better local maximum. Combiningthe enhanced first-variation with spherical k-meansyields a powerful ""ping-pong"" strategy that often qualitativelyimproves k-means clustering and is computationallyefficient. We present several experimental results to high-lightthe improvement achieved by our proposed algorithmin clustering high-dimensional and sparse text data."
ICDM	Cluster merging and splitting in hierarchical clustering algorithms.	Chris H. Q. Ding,Xiaofeng He	2002	Hierarchical clustering constructs a hierarchy of clusterseither repeatedly mer in two smaller clusters into alarger one or splittin a larger cluster into smaller ones. The crucial step is how to best select the next cluster(s)to split or merge. Here we provide a comprehensiveanalysis of selection methods and propose several newmethods. We perform extensive clustering experimentsto test 8 selection methods, and ?nd that the averagesimilarity is the best method in divisive clustering andMinMax linkage is the best in agglomerativeCluster balance is a key factor to achieve goodperformance. We also introduce the concept of objective function saturation and clustering target distanceto effectively assess the quality of clustering.
ICDM	Convex Hull Ensemble Machine.	Yongdai Kim	2002	"We propose a new ensemble algorithm called ""ConvexHull Ensemble Machine (CHEM)."" CHEM in Hilbert spaceis developed first and it is modified to regression and clas-sificationproblems. Empirical studies show that in classi-ficationproblems CHEM has similar prediction accuracyas AdaBoost, but CHEM is much more robust to outputnoise. In regression problems, CHEM works competitivelywith other ensemble methods such as Gradient Boost andBagging."
ICDM	Adaptive dimension reduction for clustering high dimensional data.	Chris H. Q. Ding,Xiaofeng He,Hongyuan Zha,Horst D. Simon	2002	It is well-known that for high dimensional data clustering, standard algorithms such as EM and the K -meansare often trapped in local minimum. Many initializationmethods were proposed to tackle this problem, but withonly limited success. In this paper we propose newapproach to resolve this problem by repeated dimension reductions such that K-means or EM are performedonly in very low dimensions.Cluster membership is utilized as a bridge between the reduced dimensional sub-space and the original space, providing flexibility andease of implementation. Clustering analysis performedon highly overlapped Gaussians, DNA gene expressionprofiles and internet newsgroups demonstrate the effectiveness of the proposed algorithm.
ICDM	High Performance Data Mining Using the Nearest Neighbor Join.	Christian Böhm,Florian Krebs	2002	The similarity join has become an important database primitiveto support similarity search and data mining. A similarity joincombines two sets of complex objects such that the result containsall pairs of similar objects. Well-known are two types of thesimilarity join, the distance range join where the user defines adistance threshold for the join, and the closest point query ork-distance join which retrieves the k most similar pairs. In thispaper, we investigate an important, third similarity join operationcalled k-nearest neighbor join which combines each point ofone point set with its k nearest neighbors in the other set. It hasbeen shown that many standard algorithms of Knowledge Discoveryin Databases (KDD) such as k-means and k-medoid clustering,nearest neighbor classification, data cleansing, postprocessingof sampling-based data mining etc. can be implementedon top of the k-nn join operation to achieve performance improvementswithout affecting the quality of the result of these algorithms.We propose a new algorithm to compute the k-nearestneighbor join using the multipage index (MuX), a specialized indexstructure for the similarity join. To reduce both CPU and I/Ocost, we develop optimal loading and processing strategies.
ICDM	Modal-style operators in qualitative data analysis.	Ivo Düntsch,Günther Gediga	2002	We explore the usage of the modal possibility operator (andits dual necessity operator) in qualitative data analysis, andshow that it - quite literally - complements the derivationoperator of formal concept analysis; we also propose a newgeneralization of the rough set approximation operators. Asan example for the applicability of the concepts we investigatethe Morse data set which has been frequently studiedin multidimensional scaling procedures.
ICDM	Mining Molecular Fragments: Finding Relevant Substructures of Molecules.	Christian Borgelt,Michael R. Berthold	2002	"We present an algorithm to find fragments in a setof molecules that help to discriminate between differentclasses of, for instance, activity in a drug discovery context.Instead of carrying out a brute-force search, our methodgenerates fragments by embedding them in all appropriatemolecules in parallel and prunes the search tree based ona local order of the atoms and bonds, which results in substantiallyfaster search by eliminating the need for frequent,computationally expensive reembeddings and by suppressingredundant search. We prove the usefulness of our algorithmby demonstrating the discovery of activity-relatedgroups of chemical compounds in the well-known NationalCancer Institute's HIV-screening dataset."
ICDM	Telecommunications Strategic Marketing - KDD and Economic Modeling.	Stefano Cazzella,Luigi Dragone,Stefano Trisolini	2002	The Italian deregulation process of telecommunications market in the last years has produced a largeeconomic impact since it has altered equilibriums thatwere established for a long time. In this framework, wenotice a strong need for adequate tools to analyze themarket and its trends and, at the same time, a lack ofspecific solutions within the scientific literature, due tothe new technical challenges issued by the problem.In particular, in the context of building a DecisionSupport System (DSS) for the strategic marketing unit ofTELECOM Italia (TI) we have devised a newmethodology to profitably combine most powerful toolsfrom KDD and Economic Sciences. We have tested ourapproach by analyzing the residential telecommunicationsmarket demand in Italy during the transition from amonopolistic structure to an oligopolistic one.In this paper, we first address the state of the art inDSS design, then we describe the proposed methodologyand its application in the case study.
ICDM	Mining General Temporal Association Rules for Items with Different Exhibition Periods.	Cheng-Yue Chang,Ming-Syan Chen,Chang-Hung Lee	2002	In this paper, we explore a new model of mining generaltemporal association rules from large databases wherethe exhibition periods of the items are allowed to be differentfrom one to another. Note that in this new model,the downward closure property which all prior Apriori-basedalgorithms relied upon to attain good efficiency isno longer valid. As a result, how to efficiently generatecandidate itemsets form large databases has become themajor challenge. To address this issue, we develop an efficientalgorithm, referred to as algorithm SPF (standingfor Segmented Progressive Filter) in this paper. The basicidea behind SPF is to first segment the database into sub-databasesin such a way that items in each sub-databasewill have either the common starting time or the commonending time. Then, for each sub-database, SPF progressivelyfilters candidate 2-itemsets with cumulative filteringthresholds either forward or backward in time. This featureallows SPF of adopting the scan reduction techniqueby generating all candidate k-itemsets (k >2) from candidate2-itemsets directly. The experimental results show thatalgorithm SPF significantly outperforms other schemeswhich are extended from prior methods in terms of the executiontime and scalability.
ICDM	Progressive Modeling.	Wei Fan,Haixun Wang,Philip S. Yu,Shaw-hwa Lo,Salvatore J. Stolfo	2002	Presently, inductive learning is still performed in a frustratingbatch process. The user has little interaction withthe system and no control over the final accuracy and trainingtime. If the accuracy of the produced model is too low,all the computing resources are misspent. In this paper, wepropose a progressive modeling framework. In progressivemodeling, the learning algorithm estimates online both theaccuracy of the final model and remaining training time. Ifthe estimated accuracy is far below expectation, the usercan terminate training prior to completion without wastingfurther resources. If the user chooses to complete the learningprocess, progressive modeling will compute a modelwith expected accuracy in expected time. We describe oneimplementation of progressive modeling using ensemble ofclassifiers.
ICDM	Discriminative Category Matching: Efficient Text Classification for Huge Document Collections.	Gabriel Pui Cheong Fung,Jeffrey Xu Yu,Hongjun Lu	2002	Discriminative Category Matching: Efficient Text Classification for Huge Document Collections.
ICDM	ESRS: A Case Selection Algorithm Using Extended Similarity-based Rough Sets.	Liqiang Geng,Howard J. Hamilton	2002	A case selection algorithm selects representative casesfrom a large data set for future case-based reasoningtasks. This paper proposes the ESRS algorithm, based onextended similarity-based rough set theory, which selectsa reasonable number of the representative cases whilemaintaining satisfactory classification accuracy. It alsocan handle noise and inconsistent data. Experimentalresults on synthetic and real sets of cases showed that itspredictive accuracy is similar to that of well-knownmachine learning systems on standard data sets, while ithas the advantage of being applicable to any data setwhere a similarity function can be defined.
ICDM	Using Text Mining to Infer Semantic Attributes for Retail Data Mining.	Rayid Ghani,Andrew E. Fano	2002	"Current Data Mining techniques usually do not have amechanism to automatically infer semantic features inherentin the data being ""mined"". The semantics are eitherinjected in the initial stages (by feature construction) or byinterpreting the results produced by the algorithms. Bothof these techniques have proved effective but require a lotof human effort. In many domains, semantic informationis implicitly available and can be extracted automaticallyto improve data mining systems. In this paper, we present acase study of a system that is trained to extract semantic featuresfor apparel products and populate a knowledge basewith these products and features. We show that semanticfeatures of these items can be successfully extracted by applyingtext learning techniques to the descriptions obtainedfrom websites of retailers. We also describe several applicationsof such a knowledge base of product semantics that wehave built including recommender systems and competitiveintelligence tools and provide evidence that our approachcan successfully build a knowledge base with accurate factswhich can then be used to create profiles of individual customers,groups of customers, or entire retail stores."
ICDM	On Evaluating Performance of Classifiers for Rare Classes.	Mahesh V. Joshi	2002	Predicting rare classes effectively is an important problem.The definition of effective classifier, embodied in theclassifier evaluation metric, is however very subjective, dependenton the application domain. In this paper, a widevariety of point-metrics are put into a common analyticalcontext defined by the recall and precision of the target rareclass. This enables us to compare various metrics in an objective,domain-independent manner. We judge their suitabilityfor the rare class problems along the dimensions oflearning difficulty and levels of rarity. This yields manyvaluable insights. In order to address the goal of achievingbetter recall and precision, we also propose a way ofcomparing classifiers directly based on the relationships betweenrecall and precision values. It resorts to a compositepoint-metric only when recall-precision based comparisonsyield conflicting results.
ICDM	A Formal Model for User Preference.	Sung Young Jung,Jeong-Hee Hong,Taek-Soo Kim	2002	Personalization and recommendation systems requireformalized model for user preference. This paper presentsthe formal model of preference including positivepreference and negative preference. For rare events, weapply the probability of random occurrence in order toreduce noise effects caused by data sparseness. Paretodistribution is adopted for the random occurrenceprobability. We also present the method for combininginformation of joint feature variables in different sizes bydynamic weighting using random occurrence probability.
ICDM	Learning from Order Examples.	Toshihiro Kamishima,Shotaro Akaho	2002	We advocate a new learning task that deals with ordersof items, and we call this the Learning from Order Examples(LOE) task. The aim of the task is to acquire the rule thatis used for estimating the proper order of a given unordereditem set. The rule is acquired from training examples thatare ordered item sets. We present several solution methodsfor this task, and evaluate the performance and the characteristicsof these methods based on the experimental resultsof tests using both artificial data and realistic data.
ICDM	An Algebraic Approach to Data Mining: Some Examples.	Robert L. Grossman,Richard G. Larson	2002	In this paper, we introduce an algebraic approach tothe foundations of data mining. Our approach is basedupon two algebras of functions defined over a commonstate space X and a pairing between them.One algebra is an algebra of state space observations, and the other is an algebra of labeled sets ofstates.We interpret H as the algebraic encoding of the dataand the pairing as the misclassification rate when theclassifer f is applied to the set of states X.In this paper, we give a realization theorem givingconditions on formal series of data sets built from Dthat imply there is a realization involving a state spaceX, a classifier f \in R and a set of labeled states x \in R_0that yield this series.
ICDM	Phrase-based Document Similarity Based on an Index Graph Model.	Khaled M. Hammouda,Mohamed S. Kamel	2002	Document clustering techniques mostly rely on singleterm analysis of the document data set, such as the VectorSpace Model. To better capture the structure of documents,the underlying data model should be able to represent thephrases in the document as well as single terms. We presenta novel data model, the Document Index Graph, which indexesweb documents based on phrases, rather than singleterms only. The semi-structured web documents helpin identifying potential phrases that when matched withother documents indicate strong similarity between the documents.The Document Index Graph captures this informa-tion,and finding significant matching phrases between documentsbecomes easy and efficient with such model. Thesimilarity between documents is based on both single termweights and matching phrases weights. The combined similaritiesare used with standard document clustering techniquesto test their effect on the clustering quality. Experimentalresults show that our phrase-based similarity, combinedwith single-term similarity measures, enhances webdocument clustering quality significantly.
ICDM	Mining Top-K Frequent Closed Patterns without Minimum Support.	Jiawei Han,Jianyong Wang,Ying Lu,Petre Tzvetkov	2002	In this paper, we propose a new mining task: mining top-kfrequent closed patterns of length no less than min_l, wherek is the desired number of frequent closed patterns to bemined, and min _l is the minimal length of each pattern.An efficient algorithm, called TFP, is developed for mining such patterns without minimum support. Two methods, closed_node_count and descendant_sum are proposedto effiectively raise support threshold and prune FP-tree bothduring and after the construction of FP-tree. During themining process, a novel top-down and bottom-up combinedFP-tree mining strategy is developed to speed-up support-raising and closed frequent pattern discovering. In addition,a fast hash-based closed pattern verification scheme has beenemployed to check efficiently if a potential closed pattern isreally closed.Our performance study shows that in most cases, TFPoutperforms CLOSET and CHARM, two efficient frequentclosed pattern mining algorithms, even when both are running with the best tuned min_support. Furthermore, themethod can be extended to generate association rules andto incorporate user-specified constraints. Thus we concludethat for frequent pattern mining, mining top-k frequent closedpatterns without min support is more preferable than thetraditional min_support-based mining.
ICDM	Recognition of Common Areas in a Web Page Using Visual Information: a possible application in a page classification.	Milos Kovacevic,Michelangelo Diligenti,Marco Gori,Veljko M. Milutinovic	2002	"Extracting and processing information from Webpages is an important task in many areas likeconstructing search engines, information retrieval, anddata mining from the Web. Common approach in theextraction process is to represent a page as a ""bag ofwords"" and then to perform additional processing onsuch a flat representation. In this paper we propose anew, hierarchical representation that includes browserscreen coordinates for every HTML object in a page.Using visual information one is able to define heuristicsfor the recognition of common page areas such asheader, left and right menu, footer and center of a page.We show in initial experiments that using our heuristicsdefined objects are recognized properly in 73% of cases.Finally, we show that a Naive Bayes classifier, takinginto account the proposed representation, clearlyoutperforms the same classifier using only informationabout the content of documents."
ICDM	Mining Similar Temporal Patterns in Long Time-Series Data and Its Application to Medicine.	Shoji Hirano,Shusaku Tsumoto	2002	Data mining in time-series medical databases has beenreceiving considerable attention since it provides a way ofrevealing useful information hidden in the database; forexample relationships between temporal course of examinationresults and onset time of diseases. This paperpresents a new method for finding similar patterns in temporalsequences. The method is a hybridization of phase-constraintmultiscale matching and rough clustering. Multiscalematching enables us cross-scale comparison of thesequences, namely, it enable us to compare temporal patternsby partially changing observation scales. Rough clusteringenable us to construct interpretable clusters of thesequences even if their similarities are given as relativesimilarities. We combine these methods and cluster the sequencesaccording to multiscale similarity of patterns. Experimentalresults on the chronic hepatitis dataset showedthat clusters demonstrating interesting temporal patternswere successfully discovered.
ICDM	Wavelet Based UXO Detection.	Stephen Hodgson,Neil Dunstan,R. Murison	2002	The detection and classification of Unexploded Ordnance(UXO) is considered a multi-dimensional pattern recognitionproblem. Standard techniques in solving multi-dimensionaldetection and classification problems involveusing large sets of templates or libraries. This paper showsthat by using Wavelet Transformation a single library willallow a particular class of ordnance to be classified over arange of depths.
ICDM	Ensemble Modeling Through Multiplicative Adjustment of Class Probability.	Se June Hong,Jonathan R. M. Hosking,Ramesh Natarajan	2002	We develop a new concept for aggregating items of evidencefor class probability estimation. In Naïve Bayes, eachfeature contributes an independent multiplicative factor tothe estimated class probability. We modify this model to includean exponent in each factor in order to introduce fea-tureimportance. These exponents are chosen to maximizethe accuracy of estimated class probabilities on the trainingdata. For Naïve Bayes, this modification accomplishes morethan what feature selection can. More generally, since theindividual features can be the outputs of separate probabilitymodels, this yields a new ensemble modeling approach,which we call APM (Adjusted Probability Model), alongwith a regularized version called APMR.
ICDM	Discovery of Interesting Association Rules from Livelink Web Log Data.	Xiangji Huang,Aijun An,Nick Cercone,Gary Promhouse	2002	We present our experience in mining web usage patternsfrom a large collection of Livelink log data. Livelink is aweb-based product of Open Text, which provides automaticmanagement and retrieval of different types of informationobjects over an intranet or extranet. We report our experiencein preprocessing raw log data and post-processing themining results for finding interesting rules. In particular,we compare and evaluate a number of rule interestingnessmeasures and find that two of the measures that have notbeen used in association rule learning work very well.
ICDM	A Personalized Music Filtering System Based on Melody Style Classification.	Fang-Fei Kuo,Man-Kwan Shan	2002	"With the growth of digital music, the personalized musicfiltering system is helpful for users. Melody style is one ofthe music features to represent user's music preference. Inthis paper, we present a personalized content-based musicfiltering system to support music recommendation based onuser's preference of melody style. We propose the multitypemelody style classification approach to recommend themusic objects. The system learns the user preference bymining the melody patterns from the music access behaviorof the user. A two-way melody preference classifier istherefore constructed for each user. Music recommendationis made through this melody preference classifier.Performance evaluation shows that the filtering effect of theproposed approach meets user's preference."
ICDM	Discovering Frequent Geometric Subgraphs.	Michihiro Kuramochi,George Karypis	2002	As data mining techniques are being increasingly appliedto non-traditional domains, existing approaches forfinding frequent itemsets cannot be used as they cannotmodel the requirement of these domains. An alternate wayof modeling the objects in these data sets, is to use a graphto model the database objects. Within that model, the problemof finding frequent patterns becomes that of discoveringsubgraphs that occur frequently over the entire set ofgraphs. In this paper we present a computationally efficientalgorithm for finding frequent geometric subgraphs ina large collection of geometric graphs. Our algorithm isable to discover geometric subgraphs that can be rotation,scaling and translation invariant, and it can accommodateinherent errors on the coordinates of the vertices. Our experimentalresults show that our algorithms requires relativelylittle time, can accommodate low support values, andscales linearly on the number of transactions.
ICDM	Mining Generalized Association Rules Using Pruning Techniques.	Yin-Fu Huang,Chieh-Ming Wu	2002	The goal of the paper is to mine generalizedassociation rules using pruning techniques. Given a largetransaction database and a hierarchical taxonomy tree ofthe items, we try to find the association rules between theitems at different levels in the taxonomy tree under theassumption that original frequent itemsets and associationrules have already been generated beforehand. In theproposed algorithm GMAR, we use join methods andpruning techniques to generate new generalizedassociation rules. Through several comprehensiveexperiments, we find that the GMAR algorithm is muchbetter than BASIC and Cumulate algorithms.
ICDM	Association Analysis with One Scan of Databases.	Hao Huang,Xindong Wu,Richard Relue	2002	Mining frequent patterns with an FP-tree avoids costlycandidate generation and repeatedly occurrence frequencychecking against the support threshold. It thereforeachieves better performance and efficiency than Apriori-likealgorithms. However, the database still needs tobe scanned twice to get the FP-tree. This can be verytime-consuming when new data are added to an existingdatabase because two scans may be needed for not only thenew data but also the existing data. This paper presentsa new data structure P-tree, Pattern Tree, and a new technique,which can get the P-tree through only one scan of thedatabase and can obtain the corresponding FP-tree with aspecified support threshold. Updating a P-tree with newdata needs one scan of the new data only, and the existingdata do not need to be re-scanned.
ICDM	Adapting classification rule induction to subgroup discovery.	Nada Lavrac,Peter A. Flach,Branko Kavsek,Ljupco Todorovski	2002	Rule learning is typically used for solving classificationand prediction tasks. However, learning of classificationrules can be adapted also to subgroup discovery. This papershows how this can be achieved by modifying the coveringalgorithm and the search heuristic, performing probabilisticclassification of instances, and using an appropriatemeasure for evaluating the results of subgroup discovery.Experimental evaluation of the CN2-SD subgroup discoveryalgorithm on 17 UCI data sets demonstrates substantialreduction of the number of induced rules, increased rulecoverage and rule significance, as well as slight improvementsin terms of the area under the ROC curve.
ICDM	Implementation of a Least Fixpoint Operator for Fast Mining of Relational Databases.	Hasan M. Jamil	2002	Recent research has focused on computing large item sets for association rule mining using SQL3 least fixpoint computation, and by exploiting the monotonic nature of the SQL3 aggregate functions such as sum and create view recursive constructs.Such approaches allow us to view mining as an ad hoc querying exercise and treat the efficiency issue as an optimization problem.In this paper, we present a recursive implementation of a recently proposed least fixpoint operator for computing large item sets from object-relational databases.We present experimental evidence to show that our implementation compares well with several well-regarded and contemporary algorithms for large item set generation.
ICDM	Considering Both Intra-Pattern and Inter-Pattern Anomalies for Intrusion Detection.	Ning Jiang,Kien A. Hua,Simon Sheu	2002	Various approaches have been proposed to discoverpatterns from system call trails of UNIX processes tobetter model application behavior. However, thesetechniques only consider relationship between systemcalls (or system audit events). In this paper, we firstrefine the definition of maximal patterns given in [8] andprovide a pattern extraction algorithm to identify suchmaximal patterns. We then add one additional dimensionto the problem domain by also taking into considerationthe overlap relationship between patterns. We argue thatan execution path of an application is usually not anarbitrary combination of various patterns; but rather,they overlap each other in some specific order. Suchoverlap relationship characterizes the normal behavior ofthe application. Finally, a novel pattern matchingmodule is proposed to detect intrusions based on bothintra-pattern and inter-pattern anomalies. We test thisidea using the data sets obtained from the University ofNew Mexico. The experimental results indicate that ourscheme detect significantly more anomalies than thescheme presented in [8] while maintaining a very lowfalse alarm rate.
ICDM	Linear Causal Model Discovery Using the MML criterion.	Gang Li,Honghua Dai,Yiqing Tu	2002	Determining the causal structure of a domain is a keytask in the area of Data Mining and Knowledge Discovery.The algorithm proposed by Wallace et al. [15] hasdemonstrated its strong ability in discovering Linear CausalModels from given data sets. However, some experimentsshowed that this algorithm experienced difficulty in discoveringlinear relations with small deviation, and it occasion-allygives a negative message length, which should not beallowed. In this paper, a more efficient and precise MML encodingscheme is proposed to describe the model structureand the nodes in a Linear Causal Model. The estimation ofdifferent parameters is also derived. Empirical results showthat the new algorithm outperformed the previous MML-basedalgorithm in terms of both speed and precision.
ICDM	Improving Medical/Biological Data Classification Performance by Wavelet Preprocessing.	Qi Li,Tao Li,Shenghuo Zhu,Chandra Kambhamettu	2002	Many real-world datasets contain noise and noisecould degrade the performances of learning algorithms.Motivated from the success of wavelet denoisingtechniques in image data, we explore a generalsolution to alleviate the effect of noisy databy wavelet preprocessing for medical/biological dataclassification. Our experiments are divided into twocategories: one is of different classification algorithmson a specific database (Ecoli [6]) and the other isof a specific classification algorithm (decision tree)on different databases. The experiment results showthat the wavelet denoising of noisy data is able to improvethe accuracies of those classification methods,if the localities of the attributes are strong enough.
ICDM	Adaptive and Resource-Aware Mining of Frequent Sets.	Salvatore Orlando,Paolo Palmerini,Raffaele Perego,Fabrizio Silvestri	2002	The performance of an algorithm that mines frequent sets from transactional databases may severely depend on the specific features of the data being analyzed. Moreover, some architectural characteristics of the computational platform used - e.g. the available main memory - can dramatically change its runtime behavior. In this paper we present DCI (Direct Count & Intersect), an efficient algorithm for discovering frequent sets from large databases. Due to the multiple heuristics strategies adopted, DCI can adapt its behavior not only to the features of the specific computing platform, but also to the features of the datasetbeing mined, so that it results very effective in mining both short and long patterns from sparse and dense datasets. Finally we also discuss the parallelization strategies adopted in the design of ParDCI, a distributed and multi-threaded implementation of DCI.
ICDM	Solving the Fragmentation Problem of Decision Trees by Discovering Boundary Emerging Patterns.	Jinyan Li,Limsoon Wong	2002	The single coverage constraint discourages a decisiontree to contain many significant rules. The loss of significantrules leads to a loss in accuracy. On the other hand, thefragmentation problem causes a decision tree to contain toomany minor rules. The presence of minor rules decreasesaccuracy. We propose to use emerging patterns to solvethese problems. In our approach, many globally significantrules can be discovered. Extensive experimental results ongene expression datasets show that our approach are moreaccurate than single C4.5 trees, and are also better thanbagged or boosted C4.5 trees.
ICDM	Intersection Based Generalization Rules for the Analysis of Symbolic Septic Shock Patient Data.	Jürgen Paetz	2002	In intensive care units much data is irregularly recorded.Here, we consider the analysis of symbolic septic shock patientdata. We show that it could be worth consideringthe generalization paradigm (individual cases generalizedto more general rules) instead of the association paradigm(combining single attributes) when considering very individualcases (e.g. patients) and when expecting longer rulesthan shorter ones. We present an algorithm for rule generationand classification based on heuristically generatedset-based intersections. We demonstrate the usefulness ofour algorithm by analysing our septic shock patient data.
ICDM	Attribute (Feature) Completion - The Theory of Attributes from Data Mining Prospect.	Tsau Young Lin	2002	"A ""correct"" selection of attributes (features) is vital indata mining. As a first step, this paper constructs all possibleattributes of a given relation. The results are basedon the observations that each relation is isomorphic to aunique abstract relation, called canonical model. The completeset of attributes of the canonical model is, then, constructed.Any attribute of a relation can be interpreted (viaisomorphism) from such a complete set."
ICDM	SLPMiner: An Algorithm for Finding Frequent Sequential Patterns Using Length-Decreasing Support Constraint.	Masakazu Seno,George Karypis	2002	Over the years, a variety of algorithms for finding frequentsequential patterns in very large sequential databaseshave been developed. The key feature in most of these algorithmsis that they use a constant support constraint tocontrol the inherently exponential complexity of the problem.In general, patterns that contain only a few items willtend to be interesting if they have a high support, whereaslong patterns can still be interesting even if their supportis relatively small. Ideally, we desire to have an algorithmthat finds all the frequent patterns whose support decreasesas a function of their length. In this paper we present an algorithmcalled SLPMiner, that finds all sequential patternsthat satisfy a length-decreasing support constraint. Our experimentalevaluation shows that SLPMiner achieves up totwo orders of magnitude of speedup by effectively exploitingthe length-decreasing support constraint, and that itsruntime increases gradually as the average length of the sequences(and the discovered frequent patterns) increases.
ICDM	Mining Optimal Actions for Profitable CRM.	Charles X. Ling,Tielin Chen,Qiang Yang,Jie Cheng	2002	Data mining has been applied to CRM (Customer RelationshipManagement) in many industries witha limitedsuccess.Most data mining tools can only discover customer modelsor profiles (such as customers who are likely attritors andcustomers who are loyal), but not actions that would improvecustomer relationship (such as changing attritors toloyal customers). We describe a novel algorithm that suggestsactions to change customers from an undesired status(such as attritors) to a desired one (such as loyal). Our algorithmtakes into account the cost of actions, and further,it attempts to maximize the expected net profit. To our bestknowledge, no data mining algorithms or tools today can accomplishthis important task in CRM. The algorithm is implemented,with many advanced features, in a specializedand highly effective data mining software called ProactiveSolution.
ICDM	A new implementation technique for fast Spectral based document retrieval systems.	Laurence A. F. Park,Marimuthu Palaniswami,Kotagiri Ramamohanarao	2002	The traditional methods of spectral text retrieval(FDS,CDS) create an index of spatial data and convert thedata to its spectral form at query time. We present a newmethod of implementing and querying an index containingspectral data which will conserve the high precision performanceof the spectral methods, reduce the time needed toresolve the query, and maintain an acceptable size for theindex. This is done by taking advantage of the propertiesof the discrete cosine transform and by applying ideas fromvector space document ranking methods.
ICDM	Efficient Progressive Sampling for Association Rules.	Srinivasan Parthasarathy	2002	In data mining, sampling has often been suggested as aneffective tool to reduce the size of the dataset operated atsome cost to accuracy. However, this loss to accuracy isoften difficult to measure and characterize since the exactnature of the learning curve (accuracy vs. sample size) isparameter and data dependent, i.e., we do not know aprioriwhat sample size is needed to achieve a desired accuracyon a particular dataset for a particular set of parameters.In this article we propose the use of progressive sampling todetermine the required sample size for association rule mining.We first show that a naive application of progressivesampling is not very efficient for association rule mining.We then present a refinement based on equivalence classes,that seems to work extremely well in practice and is able toconverge to the desired sample size very quickly and veryaccurately. An additional novelty of our approach is thedefinition of a support-sensitive, interactive measure of accuracyacross progressive samples.
ICDM	Efficient Discovery of Common Substructures in Macromolecules.	Srinivasan Parthasarathy,Matt Coatney	2002	"Biological macromolecules play a fundamental role indisease; therefore, they are of great interest to fields such aspharmacology and chemical genomics. Yet due to macromolecules'complexity, development of effective techniquesfor elucidating structure-function macromolecular relationshipshas been ill explored. Previous techniques have eitherfocused on sequence analysis, which only approximatesstructure-function relationships, or on small coor-dinatedatasets, which does not scale to large datasets orhandle noise. We present a novel scalable approach toefficiently discover macromolecule substructures based onthree-dimensional coordinate data, without domain-specificknowledge. The approach combines structure-based frequentpattern discovery with search space reduction andcoordinate noise handling. We analyze computational performancecompared to traditional approaches, validate thatour approach can discover meaningful substructures innoisy macromolecule data by automated discovery of primaryand secondary protein structures, and show that ourtechnique is superior to sequence-based approaches at determiningstructural, and thus functional, similarity be-tweenproteins."
ICDM	PERUSE: An Unsupervised Algorithm for Finding Recurrig Patterns in Time Series.	Tim Oates	2002	This paper describes PERUSE, an unsupervised algorithm for finding recurring patterns in time series.It was initially developed and tested with sensor data from a mobile robot, i.e. noisy, re-valued, multivariate time series with variable intervals between observations.The pattern discovery problem is decomposed into two sub-problems: (1) a supervised learning problem in which a teacher provised exemplars of patterns and labels time series according to whether they contain the patterns; (2)an un supervised learning problem in which the time series are used to generate an approximation to the teacher.Experimental results show that PERUSE can discover patterns in audio data corresponding to qualitatively distinct outcomes of taking actions.
ICDM	Mining Motifs in Massive Time Series Databases.	Pranav Patel,Eamonn J. Keogh,Jessica Lin,Stefano Lonardi	2002	"The problem of efficiently locating previously knownpatterns in a time series database (i.e., query by content) hasreceived much attention and may now largely be regardedas a solved problem. However, from a knowledge discoveryviewpoint, a more interesting problem is the enumeration ofpreviously unknown, frequently occurring patterns. We callsuch patterns ""motifs"", because of their close analogy totheir discrete counterparts in computation biology. Anefficient motif discovery algorithm for time series would beuseful as a tool for summarizing and visualizing massivetime series databases. In addition it could be used as asubroutine in various other data mining tasks, including thediscovery of association rules, clustering and classification.In this work we carefully motivate, then introduce, a non-trivialdefinition of time series motifs. We propose anefficient algorithm to discover them, and we demonstrate theutility and efficiency of our approach on several real worlddatasets."
ICDM	Experimentation and Self Learning in Continuous Database Marketing.	James E. Pearce,Geoffrey I. Webb,Robin N. Shaw,Brian Garner	2002	We present a method for continuous database marketingthat identifies target customers for a number of marketingoffers using predictive models. The algorithm thenselects the appropriate offer for the customer. Experimentaldesign principles are encapsulated to capturemore information that will be used to monitor and refinethe predictive models. The updated predictive models arethen used for the next round of marketing offers.
ICDM	Reviewing RELIEF and its Extensions: A new Approach for Estimating Attributes considering high-correlated Features.	Raquel Flórez López	2002	"RELIEF algorithm [4], [5] and its extensions [8], [9]are some of the most known filter methods for estimatingthe quality of attributes in classification problems dealingwith both dependent and independent features. Thesemethods attend to find all meaningful features for eachproblem (both weakly and strongly ones [6]) so they areusually employed like a first stage for detecting irrelevantattributes. Nevertheless, in this paper we checked thatRELIEF-family algorithms present some importantlimitations that could distort the selection of the finalfeatures' subset, specially in the presence of high-correlatedattributes. To overcome these difficulties, anew approach has been developed (WACSA algorithm),which performance and validity are verified on well-knowndata sets."
ICDM	On Computing Condensed Frequent Pattern Bases.	Jian Pei,Guozhu Dong,Wei Zou,Jiawei Han	2002	Frequent pattern mining has been studied extensively.However, the effectiveness and efficiency of this mining isoften limited, since the number of frequent patterns generatedis often too large. In many applications it is sufficientto generate and examine only frequent patterns with supportfrequency in close-enough approximation instead of in fullprecision. Such a compact but close-enough frequent patternbase is called a condensed frequent patterns-base.In this paper, we propose and examine several alternativesat the design, representation, and implementation ofsuch condensed frequent pattern-bases. A few algorithmsfor computing such pattern-bases are proposed. Their effectivenessat pattern compression and their efficient computationmethods are investigated. A systematic performancestudy is conducted on different kinds of databases,which demonstrates the effectiveness and efficiency of ourapproach at handling frequent pattern mining in largedatabases.
ICDM	Automatic Web Page Classification in a Dynamic and Hierarchical Way.	Xiaogang Peng,Ben Choi	2002	Automatic classification of web pages is an effectiveway to deal with the difficulty of retrieving informationfrom the Internet. Although there are many automaticclassification algorithms and systems that have beenproposed, most of them ignore the conflict between thefixed number of categories and the growing number ofweb pages going into the system. They also requiresearching through all existing categories to make anyclassification. We propose a dynamic and hierarchicalclassification system that is capable of adding newcategories as required, organizing the web pages into atree structure, and classifying web pages by searchingthrough only one path of the tree structure. Our testresults show that our proposed single-path searchtechnique reduces the search complexity and increasesthe accuracy by 6% comparing to related algorithms. Ourdynamic-category expansion technique also achievessatisfying results on adding new categories into oursystem as required.
ICDM	Progressive and Interactive Analysis of Event Data Using Event Miner.	Sheng Ma,Joseph L. Hellerstein,Chang-Shing Perng,Genady Grabarnik	2002	Exploring large data sets typically involves activities that iteratebetween data selection and data analysis, in which insights obtainedfrom analysis result in new data selection. Further, data analysis needs touse a combination of analysis techniques: data summarization, mining algorithmsand visualization. This interweaving of functions arises both fromthe semantics of what the analyst hopes to achieve and from scalability requirementsfor dealing with large data volumes. We refer to such a processas a progressive analysis. Herein is described a tool, Event Miner, that integratesdata selection, mining and visualization for progressive analysis oftemporal, categorical data. We discuss a data model and architecture. Weillustrate how our tool can be used for complex mining tasks such as findingpatterns not occurring on Monday. Further, we discuss the novel visualizationemployed, such as visualizing categorical data and the results of datamining. Also, we discuss the extension of the existing mining frameworkneeded to mine temporal events with multiple attributes. Throughout, weillustrate the capabilities of Event Miner by applying it to event data fromlarge computer networks.
ICDM	Visually Mining Web User Clickpaths.	Teresa Mah,Ying Li	2002	As powerful as clickpath mining methods can be, theyoften lead to huge incomprehensible and non-interestingresult sets. Our clickpath mining practice at MSN wasfaced with challenges of keeping analysts closer to thedata exploration process, revealing powerful insight fromclickpath mining that business owners can directly actupon. These challenges stressed the importance of aninteractive and visual representation of clickpath miningresults. Most products today that can perform clickpathvisualization do so by presenting massive cross-weavingweb graphs. We present a new type of clickpathvisualization which focuses only on clickpaths of interest,simplifying the visualization space while still retaining thesame degree of mineable knowledge in the data. We alsodescribe visualization techniques we have used toenhance the detection of interesting clickpath patternsfrom data, and provide a real-life case study that hasbenefited from the use of our implemented clickpathvisualizer PAVE.
ICDM	User-directed Exploration of Mining Space with Multiple Attributes.	Chang-Shing Perng,Haixun Wang,Sheng Ma,Joseph L. Hellerstein	2002	There has been a growing interest in mining frequentitemsets in relational data with multiple attributes. A keystep in this approach is to select a set of attributes thatgroup data into transactions and a separate set of attributesthat labels data into items. Unsupervised and unrestrictedmining, however, is stymied by the combinatorial complexityand the quantity of patterns as the number of attributesgrows. In this paper, we focus on leveraging the semanticsof the underlying data for mining frequent itemsets. Forinstance, there are usually taxonomies in the data schemaand functional dependencies among the attributes. Domainknowledge and user preferences often have the potentialto significantly reduce the exponentially growing miningspace. These observations motivate the design of a user-directeddata mining framework that allows such domainknowledge to guide the mining process and control the miningstrategy. We show examples of tremendous reductionin computation by using domain knowledge in mining relationaldata with multiple attributes.
ICDM	A Theory of Inductive Query Answering.	Luc De Raedt,Manfred Jaeger,Sau Dan Lee,Heikki Mannila	2002	We introduce the boolean inductive query evaluationproblem, which is concerned with answering inductivequeries that are arbitrary boolean expressions over monotonicand anti-monotonic predicates. Secondly, we developa decomposition theory for inductive query evaluation inwhich a boolean query Q is reformulated into k sub-queriesQ_i= Q_A\wedge Q_Mthat are the conjunction of a monotonicand an anti-monotonic predicate. The solution to each sub-querycan be represented using a version space. We investigatehow the number of version spaces k needed to answerthe query can be minimized. Thirdly, for the pattern domainof strings, we show how the version spaces can berepresented using a novel data structure, called the versionspace tree, and can be computed using a variant of the famousApriori algorithm. Finally, we present some experi-mentsthat validate the approach.
ICDM	Mining Significant Associations in Large Scale Text Corpora.	Prabhakar Raghavan,Panayiotis Tsaparas	2002	Mining large-scale text corpora is an essential step in extractingthe key themes in a corpus. We motivate a quanti-tativemeasure for significant associations through the distributionsof pairs and triplets of co-occurring words. Weconsider the algorithmic problem of efficiently enumerat-ingsuch significant associations and present pruning algorithmsfor these problems, with theoretical as well as empiricalanalyses. Our algorithms make use of two novel miningmethods: (1) matrix mining, and (2) shortened documents.We present evidence from a diverse set of documents that ourmeasure does in fact elicit interesting co-occurrences.
ICDM	Toward XML-Based Knowledge Discovery Systems.	Rosa Meo,Giuseppe Psaila	2002	Inductive databases are intended to be general purposedatabases in which both source data and mined patterns canbe represented, retrieved and manipulated; however, theheterogeneity of models for mined patterns makes difficult torealize them. In this paper, we explore the feasibility of usingXML as the unifying framework for inductive databases,introducing a suitable data model called XDM (XML forData Mining). XDM is designed to describe source rawdata, heterogeneous mined patterns and data mining statements,so that they can be stored inside a unique XML-basedinductive database.
ICDM	O-Cluster: Scalable Clustering of Large High Dimensional Data Sets.	Boriana L. Milenova,Marcos M. Campos	2002	"Clustering large data sets of high dimensionality hasalways been a challenge for clustering algorithms. Manyrecently developed clustering algorithms have attemptedto address either handling data sets with a very largenumber of records and/or with a very high number ofdimensions. This paper provides a discussion of theadvantages and limitations of existing algorithms whenthey operate on very large multidimensional data sets. Tosimultaneously overcome both the ""curse ofdimensionality"" and the scalability problems associatedwith large amounts of data, we propose a new clusteringalgorithm called O-Cluster. O-Cluster combines a novelactive sampling technique with an axis-parallelpartitioning strategy to identify continuous areas of highdensity in the input space. The method operates on alimited memory buffer and requires at most a single scanthrough the data. We demonstrate the high quality of theobtained clustering solutions, their robustness to noise,and O-Cluster's excellent scalability."
ICDM	Employing Discrete Bayes Error rate for discretization and feature selection.	Ankush Mittal,Loong Fah Cheong	2002	The tasks of discretization and feature selection are frequentlyused to improve classification accuracy. In this paper,we use discrete approximation of Bayes error rate toperform discretization on the features. The discretizationprocedure targets minimization of Bayes error rate withineach partition. A class-pair discriminatory measure can bedefined on discretized partitions which forms the basis offeature selection algorithm. Small value of this measure fora class-pair indicates that the class-pair in considerationis confusing and the features which distinguish them wellshould be chosen first. A video classification problem ona large database is considered for showing the comparisonof a classifier using our discretization and feature selectiontasks with SVM, Neural network classifier, decision treesand K-Nearest neighbor classifier
ICDM	Using Sequential and Non-Sequential Patterns in Predictive Web Usage Mining Tasks.	Bamshad Mobasher,Honghua Dai,Tao Luo,Miki Nakagawa	2002	We describe an efficient framework for Web personalizationbased on sequential and non-sequential pattern discov-eryfrom usage data. Our experimental results performedon real usage data indicate that more restrictive patterns,such as contiguous sequential patterns (e.g., frequent navigationalpaths) are more suitable for predictive tasks, suchas Web prefetching, which involve predicting which item isaccessed next by a user), while less constrained patterns,such as frequent itemsets or general sequential patterns aremore effective alternatives in the context of Web personalizationand recommender systems.
ICDM	On a Capacity Control Using Boolean Kernels for the Learning of Boolean Functions.	Ken Sadohara	2002	"This paper concerns the classification task discrete attribute spaces, but consider the task in a more fundamental framework: the learning of Boolean functions.The purpose of this paper is to present a new learning algorithm for Boolean functions called Boolean Kernel Classifier (BKC) employing capacity control using Boolean kernels.BKC uses Support Vector Machines (SVMs) as learning engines and Boolean kernels are primarily used for running SVMs in feature spaces spanned by conjunctions of Boolean literals.However, another inportant role of Boolean kernels is to appropriately control the size of its hypothesis space to avoid overfitting.After applying a SVM to learn a classifier f in a feature space H induced by a Boolean kernel f k of f onto a subspace Hk of H spanned by conjunctions with length at most k, BKC can determine the smallest k such that f k is as accurate as f and learn another f' in Hk expected to have lower error for unseen data.By an empirical study on learning of randomly generated Boolean functions, it is shown that the capacity control is effective, and BKC outperforms C4.5 and naive Bayes classifiers."
ICDM	Feature Selection Algorithms: A Survey and Experimental Evaluation.	Luis Carlos Molina,Lluís Belanche,Àngela Nebot	2002	In view of the substantial number of existing feature selection algorithms, the need arises to count on criteria that enables to adequately decide which algorithm to us in certain situations.This work assess the performance of several fundamental algorithms found in the literature in a controlled scenario.A scoring measure ranks the algorithms by taking into account the amount of relevance, irrelevance and redundance on sample data sets.This measure computer the degree of matching between the output given by the algorithm and the know optimal solution.Sample size effects are also studied.
ICDM	Multivariate supervised discretization, a neighborhood graph approach.	Fabrice Muhlenbach,Ricco Rakotomalala	2002	We present a new discretization method in the contextof supervised learning. This method entitled HyperClusterFinder is characterized by its supervised and polytheticbehavior. The method is based on the notion of clustersand processes in two steps. First, a neighborhood graphconstruction from the learning database allows discoveringhomogenous clusters. Second, the minimal and maximalvalues of each cluster are transferred to each dimension inorder to define some boundaries to cut the continuous attributein a set of intervals. The discretization abilities ofthis method are illustrated by some examples, in particular,processing the XOR problem.
ICDM	Exploring Interestingness Through Clustering: A Framework.	Sigal Sahar	2002	Determining interestingness is a notoriously difficultproblem: it is subjective and elusive to capture. It is alsobecoming an increasingly more important problem in KDDas the number of mined patterns increases. In this work weintroduce and investigate a framework for association ruleclustering that enables automating much of the laboriousmanual effort normally involved in the exploration and understandingof interestingness. Clustering is ideally suitedfor this task; it is the unsupervised organization of patternsinto groups, so that patterns in the same group are moresimilar to each other than to patterns in other groups. Wealso define a data-driven inferred labeling of these clusters,the ancestor coverage, which provides an intuitive, conciserepresentation of the clusters.
ICDM	On Incorporating Subjective Interestingness Into the Mining Process.	Sigal Sahar	2002	"Subjective interestingness is at the heart of thesuccessful discovery of association rules. To determine what is subjectively interesting, users' domainknowledge must be applied. [7] introduced an approach that requires very little domain knowledgeand inter action to eliminate the majority of therules that are subjectively not interesting. In thispaper we investigate how this approach can be incorporated into the mining process, the benefits anddisadvantages of doing so, and examine the resultsof its application to real databases."
ICDM	Mining Association Rules from Stars.	Eric Ka Ka Ng,Ada Wai-Chee Fu,Ke Wang	2002	Association rule mining is an important data mining problem.It is found to be useful for conventional relational data.However, previous work had mostly targeted on mining a single table.In real life, a database is typically made up of multiple table and one important case is where some of the tables form a star schema.That tables typically correspond to entity sets and joining the tables in a star schema gives relationship amoung entity sets which can be very interesting information.Hence mining on the join result is an important problem.Based on characteristics of the star schema we propose an efficient algorithm for mining association rules on the joinresult but without actually performing the join opertation.We show that this approach can significantly out-perform the join-then-mine approach even when the latter adopts a fastest known mining algorithm.
ICDM	Exploring the Parameter State Space of Stacking.	Alexander K. Seewald	2002	"Ensemble learning schemes are a new field in data mining.While current research concentrates mainly on improvingthe performance of single learning algorithms, an alternativeis to combine learners with different biases. Stackingis the best-known such scheme which tries to combine learners'predictions or confidences via another learning algorithm.However, the adoption of Stacking into the data mining communityis hampered by its large parameter space, consistingmainly of other learning algorithms: (1) the set of learning algorithmsto combine, (2) the meta-learner responsible for thecombining and (3) the type of meta-data to use: confidencesor predictions. None of these parameters are obvious choices.Furthermore, little is known about the relation between parametersettings and performance of Stacking. By exploring all ofStacking's parameter settings and their interdependencies, weintend make Stacking a suitable choice for mainstream datamining applications."
ICDM	Evaluating the Utility of Statistical Phrases and Latent Semantic Indexing for Text Classification.	Huiwen Wu,Dimitrios Gunopulos	2002	The term-based vector space model is a prominenttechnique to retrieve textual information. In this paper weexamine the usefulness of phrases as terms in vector-baseddocument classification. We focus on statistical techniquesto extract both adjacent and window phrases fromdocuments. We discover that the positive effect of addingphrase terms is very limited, if we have already achievedgood performance using single-word terms, even whenSVD/LSI is used as dimensionality reduction method.
ICDM	Objective-Oriented Utility-Based Association Mining.	Yi-Dong Shen,Zhong Zhang,Qiang Yang	2002	"The necessity to develop methods for discovering associationpatterns to increase business utility of an enterprisehas long been recognized in data mining community.This requires modeling specific association patterns thatare both statistically (based on support and confidence) andsemantically (based on objective utility) relating to a givenobjective that a user wants to achieve or is interested in.However, we notice that no such a general model has beenreported in the literature. Traditional association miningfocuses on deriving correlations among a set of items andtheir association rules like diaper ¿ beer only tell us thata pattern like fdiaperg is statistically related to an itemlike beer. In this paper, we present a new approach, calledObjective-Oriented utility-based Association (OOA)mining,to modeling such association patterns that are explicitlyrelating to a user's objective and its utility. Due to its focuson a user's objective and the use of objective utility as keysemantic information to measure the usefulness of associationpatterns, OOA mining differs significantly from existingapproaches such as the existing constraint-based associationmining. We formally define OOA mining and developan algorithm for mining OOA rules. The algorithm is anenhancement to Apriori with specific mechanisms for handlingobjective utility. We prove that the utility constraint isneither monotone nor anti-monotone nor succinct nor convertibleand present a novel pruning strategy based on theutility constraint to improve the efficiency of OOA mining."
ICDM	A Self-Organizing Map with Expanding Force for Data Clustering and Visualization.	Wing-Ho Shum,Huidong Jin,Kwong-Sak Leung,Man Leung Wong	2002	The Self-Organizing Map (SOM) is a powerful tool in theexploratory phase of data mining. However, due to the dimensional conflict, the neighborhood preservation cannotalways lead to perfect topology preservation. In this paper, we establish an Expanding SOM (ESOM) to detect andpreserve better topology correspondence between the twospaces. Our experiment results demonstrate that the ESOMconstructs better mappings than the classic SOM in terms ofboth the topological and the quantization errors. Furthermore, clustering results generated by the ESOM are moreaccurate than those by the SOM.
ICDM	On the Mining of Substitution Rules for Statistically Dependent Items.	Wei-Guang Teng,Ming-Jyh Hsieh,Ming-Syan Chen	2002	In this paper, a new mining capability, called mining ofsubstitution rules, is explored. A substitution refers to thechoice made by a customer to replace the purchase of someitems with that of others. The process of mining substitutionrules can be decomposed into two procedures. The first procedureis to identify concrete itemsets among a large numberof frequent itemsets, where a concrete itemset is a frequentitemset whose items are statistically dependent. Thesecond procedure is then on the substitution rule generation.Two concrete itemsets X and Y form a substitutionrule, denoted by X \triangleright Y to mean that X is a substitute for Y,if and only if (1) X and Y are negatively correlated and (2)the negative association rule X \to \overline Y exists. In this paper,we derive theoretical properties for the model of substitutionrule mining. Then, in light of these properties, algorithmSRM (standing for substitution rule mining) is designedand implemented to discover the substitution rulesefficiently while attaining good statistical significance. Empiricalstudies are performed to evaluate the performance ofalgorithm SRM proposed. It is shown that algorithm SRMproduces substitution rules of very high quality.
ICDM	TreeFinder: a First Step towards XML Data Mining.	Alexandre Termier,Marie-Christine Rousset,Michèle Sebag	2002	In this paper, we consider the problem of searching fre-quenttrees from a collection of tree-structured data model-ingXML data. The TreeF inder algorithm aims at findingtrees, such that their exact or perturbed copies are frequentin a collection of labelled trees.To cope with complexity issues, TreeF inder is correctbut not complete: it finds a subset of the actually frequenttrees. The default of completeness is experimentally inves-tigatedon artificial medium size datasets; it is shown thatTreeFinderreaches completeness or falls short to it for arange of experimental settings.
ICDM	Mining Associated Implication Networks: Computational Intermarket Analysis.	Philip W. Tse,Jiming Liu	2002	Current attempts to analyze international financialmarkets include the use of financial technical analysis anddata mining techniques. In this paper, we propose a newapproach that incorporates implication networks andassociation rules to form an associated network structure.The proposed approach explicitly addresses the issue oflocal vs. global influences between financial markets.
ICDM	Computing Frequent Graph Patterns from Semistructured Data.	Natalia Vanetik,Ehud Gudes,Solomon Eyal Shimony	2002	Whereas data mining in structured data focuses on frequentdata values, in semi-structured and graph data theemphasis is on frequent labels and common topologies.Here, the structure of the data is just as important as its content.We study the problem of discovering typical patterns ofgraph data. The discovered patterns can be useful for manyapplications, including: compact representation of sourceinformation and a road-map for browsing and querying informationsources. Difficulties arise in the discovery taskfrom the complexity of some of the required sub-tasks, suchas sub-graph isomorphism. This paper proposes a new algorithmfor mining graph data, based on a novel definitionof support. Empirical evidence shows practical, as well astheoretical, advantages of our approach.
ICDM	Estimating the number of segments in time series data using permutation tests.	Kari Vasko,Hannu Toivonen	2002	Segmentation is a popular technique for discoveringstructure in time series data. We address the largely openproblem of estimating the number of segments that can bereliably discovered. We introduce a novel method for theproblem, called Pete. Pete is based on permutation testing.The problem is an instance of model (dimension) selection.The proposed method analyzes the possible overfitof a model to the available data rather than uses a termfor penalizing model complexity. In this respect the approachis more similar to cross-validation than regulariza-tionbased techniques (e.g., AIC, BIC, MDL, MML). Further,the method produces a p value for each increase in thenumber of segments. This gives the user an overview of thestatistical significance of the segmentations. We evaluatethe performance of the proposed method using both syntheticand real time series data. The experiments show thatpermutation testing gives realistic results about the numberof reliably identifiable segments and that it compares favorablywith the Monte Carlo cross-validation (MCCV) andcommonly used BIC criteria.
ICDM	Predicting Rare Events In Temporal Domains.	Ricardo Vilalta,Sheng Ma	2002	Temporal data mining aims at finding patterns in historicaldata. Our work proposes an approach to extract temporalpatterns from data to predict the occurrence of targetevents, such as computer attacks on host networks, or fraudulenttransactions in financial institutions. Our problemformulation exhibits two major challenges: 1) we assumeevents being characterized by categorical features and displayinguneven inter-arrival times; such an assumption fallsoutside the scope of classical time-series analysis, 2) weassume target events are highly infrequent; predictive techniquesmust deal with the class-imbalance problem. We pro-posean efficient algorithm that tackles the challenges aboveby transforming the event prediction problem into a searchfor all frequent eventsets preceding target events. The classimbalance problem is overcome by a search for patterns onthe minority class exclusively; the discrimination power ofpatterns is then validated against other classes. Patternsare then combined into a rule-based model for prediction.Our experimental analysis indicates the types of event sequenceswhere target events can be accurately predicted.
ICDM	Delta B+ Tree: Indexing 3D Point Sets for Pattern Discovery.	Xiong Wang	2002	Three-dimensional point sets can be used to representdata in different domains. Given a database of 3D pointsets, pattern discovery looks for similar subsets that occurin multiple point sets. Geometric hashing proved to be aneffective technique in discovering patterns in 3D point sets.However, there are also known shortcomings. We proposea new indexing technique called \Delta B+Trees. It is an extensionof B+-Trees that stores point triplet information. Itovercomes the shortcomings of the geometric hashing technique.We introduce four different ways of constructing thekey from a triplet. We give analytical comparison betweenthe new index structure and the geometric hashing technique.We also conduct experiments on both synthetic dataand real data to evaluate the performance.
ICDM	Maintenance of Sequential Patterns for Record Modification Using Pre-large Sequences.	Ching-Yao Wang,Tzung-Pei Hong,Shian-Shyong Tseng	2002	In the past, we proposed incremental miningalgorithms for maintenance of sequential patterns basedon the concept of pre-large sequences as records wereinserted or deleted. Although maintenance of sequentialpatterns for record modification can be performed byusage of the deletion procedure and then the insertionprocedure, twice computation time of a single procedureis needed. In this paper, we thus attempt to apply theconcept of pre-large sequences to maintain sequentialpatterns as records are modified. The proposed algorithmdoes not require rescanning original databases until theaccumulative amount of modified customer sequencesexceeds a safety bound derived by pre-large concept. Asdatabases grow larger, the numbers of modified customersequences allowed before database rescanning isrequired also grow.
ICDM	Mining Associations by Pattern Structure in Large Relational Tables.	Haixun Wang,Chang-Shing Perng,Sheng Ma,Philip S. Yu	2002	Association rule mining aims at discovering patternswhose support is beyond a given threshold. Mining patternscomposed of items described by an arbitrary subset ofattributes in a large relational table represents a new challengeand has various practical applications, including theevent management systems that motivated this work. Theattribute combinations that define the items in a pattern providethe structural information of the pattern. Current associationalgorithms do not make full use of the structuralinformation of the patterns: the information is either lostafter it is encoded with attribute values, or is constrainedby a given hierarchy or taxonomy. Pattern structures conveyimportant knowledge about the patterns. In this paper,we present a novel architecture that organizes the miningspace based on pattern structures. By exploiting the inter-relationshipsamong pattern structures, execution times formining can be reduced significantly. This advantage isdemonstrated by our experiments using both synthetic andreal-life datasets.
ICDM	Comparison of Lazy Bayesian Rule and Tree-Augmented Bayesian Learning.	Zhihai Wang,Geoffrey I. Webb	2002	The naive Bayes classifier is widely used in interactiveapplications due to its computational efficiency, direct theoreticalbase, and competitive accuracy. However, its attributeindependence assumption can result in sub-optimalaccuracy. A number of techniques have explored simple relaxationsof the attribute independence assumption in or-derto increase accuracy. Among these, the lazy Bayesianrule () and the tree-augmented naive Bayes ()have demonstrated strong prediction accuracy. However,their relative performance has never been evaluated. Thispaper compares and contrasts these two techniques, findingthat they have comparable accuracy and hence shouldbe selected according to computational profile. LBR is desirablewhen small numbers of objects are to be classifiedwhile TAN is desirable when large numbers of objects areto be classified.
ICDM	Concept Tree Based Clustering Visualization with Shaded Similarity Matrices.	Jun Wang,Bei Yu,Les Gasser	2002	One of the problems with existing clustering methods isthat the interpretation of clusters may be difficult. Two differentapproaches have been used to solve this problem:conceptual clustering in machine learning and clusteringvisualization in statistics and graphics. The purpose of thispaper is to investigate the benefits of combining clusteringvisualization and conceptual clustering to obtain bettercluster interpretations. In our research we have combinedconcept trees for conceptual clustering with shaded similaritymatrices for visualization. Experimentation shows thatthe two interpretation approaches can complement eachother to help us understand data better.
ICDM	An Incremental Approach to Building a Cluster Hierarchy.	Dwi H. Widyantoro,Thomas R. Ioerger,John Yen	2002	In this paper we present a novel Incremental HierarchicalClustering (IHC) algorithm. Our approach aims to constructa hierarchy that satisfies the homogeneity and themonotonicity properties. Working in a bottom-up fashion,a new instance is placed in the hierarchy and a sequence ofhierarchy restructuring process is performed only in regionsthat have been affected by the presence of the new instance.The experimental results on a variety of domains demonstratethat our algorithm is not sensitive to input ordering,can produce a quality cluster hierarchy, and is efficient interms of its computational time.
ICDM	A Comparative Study of RNN for Outlier Detection in Data Mining.	Graham J. Williams,Rohan A. Baxter,Hongxing He,Simon Hawkins,Lifang Gu	2002	We have proposed replicator neural networks (RNNs)for outlier detection [8]. Here we compare RNN for outlierdetection with three other methods using both publiclyavailable statistical datasets (generally small) and datamining datasets (generally much larger and generally realdata). The smaller datasets provide insights into the relativestrengths and weaknesses of RNNs. The larger datasetsparticularly test scalability and practicality of application.
ICDM	Adapting Information Extraction Knowledge For Unseen Web Sites.	Tak-Lam Wong,Wai Lam	2002	We propose a wrapper adaptation framework which aimsat adapting a learned wrapper to an unseen Web site. It significantlyreduces human effort in constructing wrappers.Our framework makes use of extraction rules previously discoveredfrom a particular site to seek potential training ex-amplecandidates for an unseen site. Rule generalizationand text categorization are employed for finding suitable examplecandidates. Another feature of our approach is thatit makes use of the previously discovered lexicon to classifygood training examples automatically for the new site. Weconducted extensive experiments to evaluate the quality ofthe extraction performance and the adaptability of our approach.
ICDM	A Hybrid Approach to Discover Bayesian Networks From Databases Using Evolutionary Programming.	Man Leung Wong,Shing Yan Lee,Kwong-Sak Leung	2002	This paper describes a novel data mining approach thatemploys evolutionary programming to discover knowledgerepresented in Bayesian networks. There are two differentapproaches to the network learning problem. The first oneuses dependency analysis, while the second one searchesgood network structures according to a metric. Unfortu-nately,both approaches have their own drawbacks. Thus,we propose a novel hybrid algorithm of the two approaches,which consists of two phases, namely, the Conditional Inde-pendence(CI) test and the search phases. A new opera-toris introduced to further enhance the search efficiency.We conduct a number of experiments and compare the hy-bridalgorithm with our previous algorithm, MDLEP [18],which uses EP for network learning. The empirical resultsillustrate that the new approach has better performance.We apply the approach to a data sets of direct marketingand compare the performance of the evolved Bayesian net-worksobtained by the new algorithm with the models gen-eratedby other methods. In the comparison, the inducedBayesian networks produced by the new algorithm outper-formthe other models.
ICDM	Using Category-Based Adherence to Cluster Market-Basket Data.	Ching-Huang Yun,Kun-Ta Chuang,Ming-Syan Chen	2002	In this paper, we devise an efficient algorithm for clusteringmarket-basket data. Different from those of the traditionaldata, the features of market-basket data are knownto be of high dimensionality, sparsity, and with massive out-liers.Without explicitly considering the presence of the tax-onomy,most prior efforts on clustering market-basket datacan be viewed as dealing with items in the leaf level of thetaxonomy tree. Clustering transactions across different levelsof the taxonomy is of great importance for marketingstrategies as well as for the result representation of the clusteringtechniques for market-basket data. In view of thefeatures of market-basket data, we devise in this paper anovel measurement, called the category-based adherence,and utilize this measurement to perform the clustering. Thedistance of an item to a given cluster is defined as the numberof links between this item and its nearest large node inthe taxonomy tree where a large node is an item (i.e., leaf)or a category (i.e., internal) node whose occurrence countexceeds a given threshold. The category-based adherenceof a transaction to a cluster is then defined as the averagedistance of the items in this transaction to that cluster.With this category-based adherence measurement, wedevelop an efficient clustering algorithm, called algorithmCBA (standing for Category-Based Adherence), for market-basketdata with the objective to minimize the category-basedadherence. A validation model based on InformationGain (IG) is also devised to assess the quality of clusteringfor market-basket data. As validated by both real and syntheticdatasets, it is shown by our experimental results, withthe taxonomy information, algorithm CBA devised in thispaper significantly outperforms the prior works in both theexecution efficiency and the clustering quality for market-basketdata.
ICDM	Mixtures of ARMA Models for Model-Based Time Series Clustering.	Yimin Xiong,Dit-Yan Yeung	2002	Clustering problems are central to many knowledge discoveryand data mining tasks. However, most existing clusteringmethods can only work with fixed-dimensional representationsof data patterns. In this paper, we study the clusteringof data patterns that are represented as sequencesor time series possibly of different lengths. We propose amodel-based approach to this problem using mixtures of autoregressivemoving average (ARMA) models. We derive anexpectation-maximization (EM) algorithm for learning themixing coefficients as well as the parameters of the componentmodels. Experiments were conducted on simulatedand real datasets. Results show that our method comparesfavorably with another method recently proposed by othersfor similar time series clustering problems.
ICDM	From Path Tree To Frequent Patterns: A Framework for Mining Frequent Patterns.	Yabo Xu,Jeffrey Xu Yu,Guimei Liu,Hongjun Lu	2002	In this paper, we propose a new framework for miningfrequent patterns from large transactional databases. Thecore of the framework is of a novel coded prefix-path treewith two representations, namely, a memory-based prefix-pathtree and a disk-based prefix-path tree. The disk-basedprefix-path tree is simple in its data structure yet rich ininformation contained, and is small in size. The memory-basedprefix-path tree is simple and compact. Upon thememory-based prefix-path tree, a new depth-first frequentpattern discovery algorithm, called P P-Mine, is proposedin this paper that outperforms FP-growth significantly. Thememory-based prefix-path tree can be stored on disk usinga disk-based prefix-path tree with assistance of the new codingscheme. We present efficient loading algorithms to loadthe minimal required disk-based prefix-path tree into mainmemory. Our technique is to push constraints into the loadingprocess, which has not been well studied yet.
ICDM	Clustering Spatial Data when Facing Physical Constraints.	Osmar R. Zaïane,Chi-Hoon Lee	2002	Clustering spatial data is a well-known problem that hasbeen extensively studied to find hidden patterns or meaningfulsub-groups and has many applications such as satelliteimagery, geographic information systems, medical imageanalysis, etc. Although many methods have been proposedin the literature, very few have considered constraintssuch that physical obstacles and bridges linking clustersmay have significant consequences on the effectiveness ofthe clustering. Taking into account these constraints duringthe clustering process is costly, and the effective modeling ofthe constraints is of paramount importance for good performance.In this paper, we define the clustering problem in thepresence of constraints - obstacles and crossings - and investigateits efficiency and effectiveness for large databases.In addition, we introduce a new approach to model theseconstraints to prune the search space and reduce the numberof polygons to test during clustering. The algorithmDBCluC we present detects clusters of arbitrary shape andis insensitive to noise and the input order. Its average runningcomplexity is O(NlogN) where N is the number of dataobjects.
ICDM	gSpan: Graph-Based Substructure Pattern Mining.	Xifeng Yan,Jiawei Han	2002	We investigate new approaches for frequent graph-basedpattern mining in graph datasets and propose a novel algorithmcalled gSpan (graph-based Substructure pattern mining),which discovers frequent substructures without candidategeneration. gSpan builds a new lexicographic orderamong graphs, and maps each graph to a unique minimumDFS code as its canonical label. Based on this lexico-graphicorder, gSpan adopts the depth-first search strategyto mine frequent connected subgraphs efficiently. Our performancestudy shows that gSpan substantially outperformsprevious algorithms, sometimes by an order of magnitude.
ICDM	Mining Case Bases for Action Recommendation.	Qiang Yang,Hong Cheng	2002	"Corporations and institutions are often interested inderiving marketing strategies from corporate data andproviding informed advice for their customers oremployees. For example, a financial institution mayderive marketing strategies for turning their reluctantcustomers into active ones and a telecommunicationscompany may plan actions to stop their valuablecustomers from leaving. In data mining terms, theseadvice and action plans are aimed at convertingindividuals from an undesirable class to a desirable one,or to help devising a direct-marketing plan in order toincrease the profit for the institution. In this paper, wepresent an approach to use role models' for generatingsuch advice and plans. These role models are typicalcases that form a case base and can be used forcustomer advice generation. For each new customerseeking advice, a nearest-neighbor algorithm is used tofind a cost-effective and highly probable plan forswitching a customer to the most desirable role models.In this paper, we explore the tradeoff among time, spaceand quality of computation in this case-based reasoningframework. We demonstrate the effectiveness of themethods through empirical results."
ICDM	Mining Surveillance Video for Independent Motion Detection.	Zhongfei Zhang	2002	This paper addresses the special applications of datamining techniques in homeland defense. The problemtargeted, which is frequently encountered in military/intelligence surveillance, is to mine a massive surveillancevideo database automatically collected to retrieve theshots containing independently moving targets. A novelsolution to this problem is presented in this paper, whichoffers a completely qualitative approach to solving for theautomatic independent motion detection problem directlyfrom the compressed surveillance video in a faster thanrealtime mining performance. This approach is based onthe linear system consistency analysis, and consequentlyis called QLS. SincetheQLS approach only focuses onwhat exactly is necessary to compute a solution, it savesthe computation to a minimum and achieves the efficacy tothe maximum. Evaluations from real data show that QLSdelivers effective mining performance at the achieved efficiency.
ICDM	InfoMiner+: Mining Partial Periodic Patterns with Gap Penalties.	Jiong Yang,Wei Wang,Philip S. Yu	2002	"In this paper, we focus on mining periodic patterns allowing some degreeof imperfection in the form of random replacement from a perfectperiodic pattern. Information gain was proposed to identify patternswith events of vastly different occurrence frequencies and adjust forthe deviation from a pattern. However, it does not take any penaltyif there exists some gap between the pattern occurrences. In manyapplications, e.g., bio-informatics, it is important to identify subsequencesthat a pattern repeats perfectly (or near perfectly). As a solution,we extend the information gain measure to include a penaltyfor gaps between pattern occurrences. We call this measure as generalizedinformation gain. Furthermore, we want to find subsequenceS' such that for a pattern P , the generalized information gain of Pin S' is high. This is particularly useful in locating repeats in DNAsequences. In this paper, we developed an effective mining algorithm,InfoMiner+, to simultaneously mine significant patterns and the as-sociatedsubsequences."
ICDM	A Comparison Study on Algorithms for Incremental Update of Frequent Sequences.	Minghua Zhang,Ben Kao,Chi Lap Yip	2002	The problem of mining frequent sequences is to extractfrequently occurring subsequences in a sequence database.Algorithms on this mining problem include GSP, MFS, andSPADE. The problem of incremental update of frequent sequencesis to keep track of the set of frequent sequences asthe underlying database changes. Previous studies have extendedthe traditional algorithms to efficiently solve the up-dateproblem. These incremental algorithms include ISM,GSP+and MFS+. Each incremental algorithm has its owncharacteristics and they have been studied and evaluatedseparately under different scenarios. This paper presentsa comprehensive study on the relative performance of theincremental algorithms as well as their non-incrementalcounterparts. Our goal is to provide guidelines on thechoice of an algorithm for solving the incremental updateproblem given the various characteristics of a sequencedatabase.
ICDM	FD_Mine: Discovering Functional Dependencies in a Database Using Equivalences.	Hong Yao,Howard J. Hamilton,Cory J. Butz	2002	The discovery of FDs from databases has recentlybecome a significant research problem. In this paper, wepropose a new algorithm, called FD_Mine. FD_Minetakes advantage of the rich theory of FDs to reduce boththe size of the dataset and the number of FDs to bechecked by using discovered equivalences. We show thatthe pruning does not lead to loss of information.Experiments on 15 UCI datasets show that FD_Mine canprune more candidates than previous methods.
ICDM	Mining Genes in DNA Using GeneScout.	Michael M. Yin,Jason Tsong-Li Wang	2002	In this paper, we present a new system, calledGeneScout, for predicting gene structures in vertebrate genomicDNA. The system contains specially designed hiddenMarkov models (HMMs) for detecting functional sites includingprotein-translation start sites, mRNA splicing junctiondonor and acceptor sites, etc. Our main hypothesisis that, given a vertebrate genomic DNA sequence S, it isalways possible to construct a directed acyclic graph Gsuch that the path for the actual coding region of S is inthe set of all paths on G. Thus, the gene detection problemis reduced to that of analyzing the paths in the graphG. A dynamic programming algorithm is used to find theoptimal path in G. The proposed system is trained usingan expectation-maximization (EM) algorithm and its performanceon vertebrate gene prediction is evaluated usingthe 10-way cross-validation method. Experimental resultsshow the good performance of the proposed system and itscomplementarity to a widely used gene detection system.
ICDM	Demand Forecasting by the Neural Network with Discrete Fourier Transform.	Mariko Yohda,Makiko Saito-Arita,Akira Okada,Ryota Suzuki,Yoshitsugu Kakemoto	2002	This paper proposes a new demand forecastingmethod using the Neural Network and Fourier Transform.In this method, time series data of sales resultsconsidered as a combination of frequency aretransformed into several frequency data. They areidentified from objective indexes that consist of productproperties or economic indicators and so forth. Thismethod is efficient for demand forecasting aimed at newproducts that have no historical data.
ICDM	Adaptive Parallel Sentences Mining from Web Bilingual News Collection.	Bing Zhao,Stephan Vogel	2002	In this paper a robust, adaptive approach for miningparallel sentences from a bilingual comparable newscollection is described. Sentence length models andlexicon-based models are combined under a maximumlikelihood criterion. Specific models are proposed to handleinsertions and deletions that are frequent in bilingualdata collected from the web. The proposed approach isadaptive, updating the translation lexicon iteratively usingthe mined parallel data to get better vocabulary coverageand translation probability parameter estimation.Experiments are carried out on 10 years of Xinhuabilingual news collection. Using the mined data, we getsignificant improvement in word-to-word alignment accuracyin machine translation modeling.
ICDM	On Active Learning for Data Acquisition.	Zhiqiang Zheng,Balaji Padmanabhan	2002	"Many applications are characterized by having naturallyincomplete data on customers - where data on only somefixed set of local variables is gathered. However, having amore complete picture can help build better models. Thenaïve solution to this problem - acquiring complete datafor all customers - is often impractical due to the costs ofdoing so. A possible alternative is to acquire completedata for ""some"" customers and to use this to improve themodels built. The data acquisition problem is determininghow many, and which, customers to acquire additionaldata from. In this paper we suggest using active learningbased approaches for the data acquisition problem. Inparticular, we present initial methods for data acquisitionand evaluate these methods experimentally on web usagedata and UCI datasets. Results show that the methodsperform well and indicate that active learning basedmethods for data acquisition can be a promising area fordata mining research."
ICDM	Adaptive Ripple Down Rules Method based on Minimum Description Length Principle.	Tetsuya Yoshida,Hiroshi Motoda,Takashi Washio	2002	Adaptive Ripple Down Rules Method based on Minimum Description Length Principle.
ICDM	Heterogeneous Learner for Web Page Classification.	Hwanjo Yu,Kevin Chen-Chuan Chang,Jiawei Han	2002	"Classification of an interesting class of Web pages (e.g.,personal homepages, resume pages) has been an interestingproblem. Typical machine learning algorithms for thisproblem require two classes of data for training: positiveand negative training examples. However, in applicationto Web page classification, gathering an unbiased sampleof negative examples appears to be difficult. We proposea heterogeneous learning framework for classifying Webpages, which (1) eliminates the need for negative trainingdata, and (2) increases classification accuracy by using twoheterogeneous learners. Our framework uses two heterogeneouslearners - a decision list and a linear separatorwhich complement each other - to eliminate the need fornegative training data in the training phase and to increasethe accuracy in the testing phase. Our results show that ourheterogeneous framework achieves high accuracy withoutrequiring negative training data; it enhances the accuracyof linear separators by reducing the errors on ""low-margindata"". That is, it classifies more accurately while requiringless human efforts in training."
ICDM	SmartMiner: A Depth First Algorithm Guided by Tail Information for Mining Maximal Frequent Itemsets.	Qinghua Zou,Wesley W. Chu,Baojing Lu	2002	Maximal frequent itemsets (MFI) are crucial to manytasks in data mining. Since the MaxMiner algorithm firstintroduced enumeration trees for mining MFI in 1998,several methods have been proposed to use depth firstsearch to improve performance. To further improve theperformance of mining MFI, we proposed a techniquethat takes advantage of the information gathered fromprevious steps to discover new MFI. More specifically,our algorithm called SmartMiner gathers and passes tailinformation and uses a heuristic select function whichuses the tail information to select the next node toexplore. Compared with Mafia and GenMax, SmartMinergenerates a smaller search tree, requires a smallernumber of support counting, and does not requiresuperset checking. Using the datasets Mushroom andConnect, our experimental study reveals that SmartMinergenerates the same MFI as Mafia and GenMax, but yieldsan order of magnitude improvement in speed.
KDD	Relational Markov models and their application to adaptive web navigation.	Corin R. Anderson,Pedro Domingos,Daniel S. Weld	2002	Relational Markov models (RMMs) are a generalization of Markov models where states can be of different types, with each type described by a different set of variables. The domain of each variable can be hierarchically structured, and shrinkage is carried out over the cross product of these hierarchies. RMMs make effective learning possible in domains with very large and heterogeneous state spaces, given only sparse data. We apply them to modeling the behavior of web site users, improving prediction in our PROTEUS architecture for personalizing web sites. We present experiments on an e-commerce and an academic web site showing that RMMs are substantially more accurate than alternative methods, and make good predictions even when applied to previously-unvisited parts of the site.
KDD	Sequential PAttern mining using a bitmap representation.	Jay Ayres,Jason Flannick,Johannes Gehrke,Tomi Yiu	2002	We introduce a new algorithm for mining sequential patterns. Our algorithm is especially efficient when the sequential patterns in the database are very long. We introduce a novel depth-first search strategy that integrates a depth-first traversal of the search space with effective pruning mechanisms.Our implementation of the search strategy combines a vertical bitmap representation of the database with efficient support counting. A salient feature of our algorithm is that it incrementally outputs new frequent itemsets in an online fashion.In a thorough experimental evaluation of our algorithm on standard benchmark data from the literature, our algorithm outperforms previous work up to an order of magnitude.
KDD	Topics in 0--1 data.	Ella Bingham,Heikki Mannila,Jouni K. Seppänen	2002	Large 0--1 datasets arise in various applications, such as market basket analysis and information retrieval. We concentrate on the study of topic models, aiming at results which indicate why certain methods succeed or fail. We describe simple algorithms for finding topic models from 0--1 data. We give theoretical results showing that the algorithms can discover the epsilon-separable topic models of Papadimitriou et al. We present empirical results showing that the algorithms find natural topics in real-world data sets. We also briefly discuss the connections to matrix approaches, including nonnegative matrix factorization and independent component analysis.
KDD	Extracting decision trees from trained neural networks.	Olcay Boz	2002	Neural Networks are successful in acquiring hidden knowledge in datasets. Their biggest weakness is that the knowledge they acquire is represented in a form not understandable to humans. Researchers tried to address this problem by extracting rules from trained Neural Networks. Most of the proposed rule extraction methods required specialized type of Neural Networks; some required binary inputs and some were computationally expensive. Craven proposed extracting MofN type Decision Trees from Neural Networks. We believe MofN type Decision Trees are only good for MofN type problems and trees created for regular high dimensional real world problems may be very complex. In this paper, we introduced a new method for extracting regular C4.5 like Decision Trees from trained Neural Networks. We showed that the new method (DecText) is effective in extracting high fidelity trees from trained networks. We also introduced a new discretization technique to make DecText be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity.
KDD	Shrinkage estimator generalizations of Proximal Support Vector Machines.	Deepak K. Agarwal	2002	We give a statistical interpretation of Proximal Support Vector Machines (PSVM) proposed at KDD2001 as linear approximaters to (nonlinear) Support Vector Machines (SVM). We prove that PSVM using a linear kernel is identical to ridge regression, a biased-regression method known in the statistical community for more than thirty years. Techniques from the statistical literature to estimate the tuning constant that appears in the SVM and PSVM framework are discussed. Better shrinkage strategies that incorporate more than one tuning constant are suggested. For nonlinear kernels, the minimization problem posed in the PSVM framework is equivalent to finding the posterior mode of a Bayesian model defined through a Gaussian process on the predictor space. Apart from providing new insights, these interpretations help us attach an estimate of uncertainty to our predictions and enable us to build richer classes of models. In particular, we propose a new algorithm called PSVMMIX which is a combination of ridge regression and a Gaussian process model. Extension to the case of continuous response is straightforward and illustrated with example datasets.
KDD	On effective classification of strings with wavelets.	Charu C. Aggarwal	2002	In recent years, the technological advances in mapping genes have made it increasingly easy to store and use a wide variety of biological data. Such data are usually in the form of very long strings for which it is difficult to determine the most relevant features for a classification task. For example, a typical DNA string may be millions of characters long, and there may be thousands of such strings in a database. In many cases, the classification behavior of the data may be hidden in the compositional behavior of certain segments of the string which cannot be easily determined apriori. Another problem which complicates the classification task is that in some cases the classification behavior is reflected in global behavior of the string, whereas in others it is reflected in local patterns. Given the enormous variation in the behavior of the strings over different data sets, it is useful to develop an approach which is sensitive to both the global and local behavior of the strings for the purpose of classification. For this purpose, we will exploit the multi-resolution property of wavelet decomposition in order to create a scheme which can mine classification characteristics at different levels of granularity. The resulting scheme turns out to be very effective in practice on a wide range of problems.
KDD	Collaborative crawling: mining user experiences for topical resource discovery.	Charu C. Aggarwal	2002	The rapid growth of the world wide web had made the problem of topic specific resource discovery an important one in recent years. In this problem, it is desired to find web pages which satisfy a predicate specified by the user. Such a predicate could be a keyword query, a topical query, or some arbitrary contraint. Several techniques such as focussed crawling and intelligent crawling have recently been proposed for topic specific resource discovery. All these crawlers are linkage based, since they use the hyperlink behavior in order to perform resource discovery. Recent studies have shown that the topical correlations in hyperlinks are quite noisy and may not always show the consistency necessary for a reliable resource discovery process. In this paper, we will approach the problem of resource discovery from an entirely different perspective; we will mine the significant browsing patterns of world wide web users in order to model the likelihood of web pages belonging to a specified predicate. This user behavior can be mined from the freely available traces of large public domain proxies on the world wide web. We refer to this technique as collaborative crawling because it mines the collective user experiences in order to find topical resources. Such a strategy is extremely effective because the topical consistency in world wide web browsing patterns turns out to very reliable. In addition, the user-centered crawling system can be combined with linkage based systems to create an overall system which works more effectively than a system based purely on either user behavior or hyperlinks.
KDD	Frequent term-based text clustering.	Florian Beil,Martin Ester,Xiaowei Xu	2002	Text clustering methods can be used to structure large sets of text or hypertext documents. The well-known methods of text clustering, however, do not really address the special problems of text clustering: very high dimensionality of the data, very large size of the databases and understandability of the cluster description. In this paper, we introduce a novel approach which uses frequent item (term) sets for text clustering. Such frequent sets can be efficiently discovered using algorithms for association rule mining. To cluster based on frequent term sets, we measure the mutual overlap of frequent sets with respect to the sets of supporting documents. We present two algorithms for frequent term-based text clustering, FTC which creates flat clusterings and HFTC for hierarchical clustering. An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain clusterings of comparable quality significantly more efficiently than state-of-the- art text clustering algorithms. Furthermore, our methods provide an understandable description of the discovered clusters by their frequent term sets.
KDD	A theoretical framework for learning from a pool of disparate data sources.	Shai Ben-David,Johannes Gehrke,Reba Schuller	2002	Many enterprises incorporate information gathered from a variety of data sources into an integrated input for some learning task. For example, aiming towards the design of an automated diagnostic tool for some disease, one may wish to integrate data gathered in many different hospitals. A major obstacle to such endeavors is that different data sources may vary considerably in the way they choose to represent related data. In practice, the problem is usually solved by a manual construction of semantic mappings and translations between the different sources. Recently there have been attempts to introduce automated algorithms based on machine learning tools for the construction of such translations.In this work we propose a theoretical framework for making classification predictions from a collection of different data sources, without creating explicit translations between them. Our framework allows a precise mathematical analysis of the complexity of such tasks, and it provides a tool for the development and comparison of different learning algorithms. Our main objective, at this stage, is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of questions related to this framework.
KDD	Exploiting unlabeled data in ensemble methods.	Kristin P. Bennett,Ayhan Demiriz,Richard Maclin	2002	"An adaptive semi-supervised ensemble method, ASSEMBLE, is proposed that constructs classification ensembles based on both labeled and unlabeled data. ASSEMBLE alternates between assigning ""pseudo-classes"" to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data. Mathematically, this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space as measured on both the labeled and unlabeled of data. Unlike alternative approaches, ASSEMBLE does not require a semi-supervised learning method for the base classifier. ASSEMBLE can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems. ASSEMBLE using decision trees won the NIPS 2001 Unlabeled Data Competition. In addition, strong results on several benchmark datasets using both decision trees and neural networks support the proposed method."
KDD	MARK: a boosting algorithm for heterogeneous kernel models.	Kristin P. Bennett,Michinari Momma,Mark J. Embrechts	2002	"Support Vector Machines and other kernel methods have proven to be very effective for nonlinear inference. Practical issues are how to select the type of kernel including any parameters and how to deal with the computational issues caused by the fact that the kernel matrix grows quadratically with the data. Inspired by ensemble and boosting methods like MART, we propose the Multiple Additive Regression Kernels (MARK) algorithm to address these issues. MARK considers a large (potentially infinite) library of kernel matrices formed by different kernel functions and parameters. Using gradient boosting/column generation, MARK constructs columns of the heterogeneous kernel matrix (the base hypotheses) on the fly and then adds them into the kernel ensemble. Regularization methods such as used in SVM, kernel ridge regression, and MART, are used to prevent overfitting. We investigate how MARK is applied to heterogeneous kernel ridge regression. The resulting algorithm is simple to implement and efficient. Kernel parameter selection is handled within MARK. Sampling and ""weak"" kernels are used to further enhance the computational efficiency of the resulting additive algorithm. The user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernels. MARK compares very favorably with SVM and kernel ridge regression on several benchmark datasets."
KDD	Scalable robust covariance and correlation estimates for data mining.	Fatemah A. Alqallaf,Kjell P. Konis,R. Douglas Martin,Ruben H. Zamar	2002	Covariance and correlation estimates have important applications in data mining. In the presence of outliers, classical estimates of covariance and correlation matrices are not reliable. A small fraction of outliers, in some cases even a single outlier, can distort the classical covariance and correlation estimates making them virtually useless. That is, correlations for the vast majority of the data can be very erroneously reported; principal components transformations can be misleading; and multidimensional outlier detection via Mahalanobis distances can fail to detect outliers. There is plenty of statistical literature on robust covariance and correlation matrix estimates with an emphasis on affine-equivariant estimators that possess high breakdown points and small worst case biases. All such estimators have unacceptable exponential complexity in the number of variables and quadratic complexity in the number of observations. In this paper we focus on several variants of robust covariance and correlation matrix estimates with quadratic complexity in the number of variables and linear complexity in the number of observations. These estimators are based on several forms of pairwise robust covariance and correlation estimates. The estimators studied include two fast estimators based on coordinate-wise robust transformations embedded in an overall procedure recently proposed by [14]. We show that the estimators have attractive robustness properties, and give an example that uses one of the estimators in the new Insightful Miner data mining product.
KDD	On the potential of domain literature for clustering and Bayesian network learning.	Peter Antal,Patrick Glenisson,Geert Fannes	2002	Thanks to its increasing availability, electronic literature can now be a major source of information when developing complex statistical models where data is scarce or contains much noise. This raises the question of how to integrate information from domain literature with statistical data. Because quantifying similarities or dependencies between variables is a basic building block in knowledge discovery, we consider here the following question. Which vector representations of text and which statistical scores of similarity or dependency support best the use of literature in statistical models? For the text source, we assume to have annotations for the domain variables as short free-text descriptions and optionally to have a large literature repository from which we can further expand the annotations. For evaluation, we contrast the variables similarities or dependencies obtained from text using different annotation sources and vector representations with those obtained from measurement data or expert assessments. Specifically, we consider two learning problems: clustering and Bayesian network learning. Firstly, we report performance (against an expert reference) for clustering yeast genes from textual annotations. Secondly, we assess the agreement between text-based and data-based scores of variable dependencies when learning Bayesian network substructures for the task of modeling the joint distribution of clinical measurements of ovarian tumors.
KDD	Learning to match and cluster large high-dimensional data sets for data integration.	William W. Cohen,Jacob Richman	2002	Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities. In integrating databases found on the Web or obtained by using information extraction methods, it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases. In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive, in the sense that they can be trained to obtain better performance in a particular domain. An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems, and is nearly always competitive with the best baseline system.
KDD	DualMiner: a dual-pruning algorithm for itemsets with constraints.	Cristian Bucila,Johannes Gehrke,Daniel Kifer,Walker M. White	2002	"Constraint-based mining of itemsets for questions such as ""find all frequent itemsets where the total price is at least $50"" has received much attention recently. Two classes of constraints, monotone and antimonotone, have been identified as very useful. There are algorithms that efficiently take advantage of either one of these two classes, but no previous algorithms can efficiently handle both types of constraints simultaneously. In this paper, we present the first algorithm (called DualMiner) that uses both monotone and antimonotone constraints to prune its search space. We complement a theoretical analysis and proof of correctness of DualMiner with an experimental study that shows the efficacy of DualMiner compared to previous work."
KDD	Discovery net: towards a grid of knowledge discovery.	Vasa Curcin,Moustafa Ghanem,Yike Guo,Martin Köhler,Anthony Rowe,Jameel Syed,Patrick Wendel	2002	This paper provides a blueprint for constructing collaborative and distributed knowledge discovery systems within Grid-based computing environments. The need for such systems is driven by the quest for sharing knowledge, information and computing resources within the boundaries of single large distributed organisations or within complex Virtual Organisations (VO) created to tackle specific projects. The proposed architecture is built on top of a resource federation management layer and is composed of a set of different resources. We show how this architecture will behave during a typical KDD process design and deployment, how it enables the execution of complex and distributed data mining tasks with high performance and how it provides a community of e-scientists with means to collaborate, retrieve and reuse both KDD algorithms, discovery processes and knowledge in a visual analytical environment.
KDD	Pattern discovery in sequences under a Markov assumption.	Darya Chudova,Padhraic Smyth	2002	"In this paper we investigate the general problem of discovering recurrent patterns that are embedded in categorical sequences. An important real-world problem of this nature is motif discovery in DNA sequences. We investigate the fundamental aspects of this data mining problem that can make discovery ""easy"" or ""hard."" We present a general framework for characterizing learning in this context by deriving the Bayes error rate for this problem under a Markov assumption. The Bayes error framework demonstrates why certain patterns are much harder to discover than others. It also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery. We demonstrate how the Bayes error can be used to calibrate existing discovery algorithms, providing a lower bound on achievable performance. We discuss a number of fundamental issues that characterize sequential pattern discovery in this context, present a variety of empirical results to complement and verify the theoretical analysis, and apply our methodology to real-world motif-discovery problems in computational biology."
KDD	CVS: a Correlation-Verification based Smoothing technique on information retrieval and term clustering.	Christina Yip Chung,Bin Chen	2002	As information volume in enterprise systems and in the Web grows rapidly, how to accurately retrieve information is an important research area. Several corpus based smoothing techniques have been proposed to address the data sparsity and synonym problems faced by information retrieval systems. Such smoothing techniques are often unable to discover and utilize the correlations among terms.We propose CVS, a Correlation-Verification based Smoothing method, that considers co-occurrence information in smoothing. Strongly correlated terms in a document are identified by their co-occurrence frequencies in the document. To avoid missing correlated terms with low co-occurrence frequencies but specific to the theme of the document, the joint distributions of terms in the document are compared with those in the corpus for statistical significance.A common approach to apply corpus based smoothing techniques to information retrieval is by refining the vector representations of documents. This paper investigates the effects of corpus based smoothing on information retrieval by query expansion using term clusters generated from a term clustering process. The results can also be viewed in light of the effects of smoothing on clustering.Empirical studies show that our approach outperforms previous corpus based smoothing techniques. It improves retrieval effectiveness by 14.6%. The results demonstrate that corpus based smoothing can be used for query expansion by term clustering.
KDD	Enhanced word clustering for hierarchical text classification.	Inderjit S. Dhillon,Subramanyam Mallela,Rahul Kumar	2002	"In this paper we propose a new information-theoretic divisive algorithm for word clustering applied to text classification. In previous work, such ""distributional clustering"" of features has been found to achieve improvements over feature selection in terms of classification accuracy, especially at lower number of features [2, 28]. However the existing clustering techniques are agglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost. In order to explicitly capture the optimality of word clusters in an information theoretic framework, we first derive a global criterion for feature clustering. We then present a fast, divisive algorithm that monotonically decreases this objective function value, thus converging to a local minimum. We show that our algorithm minimizes the ""within-cluster Jensen-Shannon divergence"" while simultaneously maximizing the ""between-cluster Jensen-Shannon divergence"". In comparison to the previously proposed agglomerative strategies our divisive algorithm achieves higher classification accuracy especially at lower number of features. We further show that feature clustering is an effective technique for building smaller class models in hierarchical classification. We present detailed experimental results using Naive Bayes and Support Vector Machines on the 20 Newsgroups data set and a 3-level hierarchy of HTML documents collected from Dmoz Open Directory."
KDD	A new two-phase sampling based algorithm for discovering association rules.	Bin Chen,Peter J. Haas,Peter Scheuermann	2002	"This paper introduces FAST, a novel two-phase sampling-based algorithm for discovering association rules in large databases. In Phase I a large initial sample of transactions is collected and used to quickly and accurately estimate the support of each individual item in the database. In Phase II these estimated supports are used to either trim ""outlier"" transactions or select ""representative"" transactions from the initial sample, thereby forming a small final sample that more accurately reflects the statistical characteristics (i.e., itemset supports) of the entire database. The expensive operation of discovering association rules is then performed on the final sample. In an empirical study, FAST was able to achieve 90--95% accuracy using a final sample having a size of only 15--33% of that of a comparable random sample. This efficiency gain resulted in a speedup by roughly a factor of 10 over previous algorithms that require expensive processing of the entire database --- even efficient algorithms that exploit sampling. Our new sampling technique can be used in conjunction with almost any standard association-rule algorithm, and can potentially render scalable other algorithms that mine ""count"" data."
KDD	SECRET: a scalable linear regression tree algorithm.	Alin Dobra,Johannes Gehrke	2002	Developing regression models for large datasets that are both accurate and easy to interpret is a very important data mining problem. Regression trees with linear models in the leaves satisfy both these requirements, but thus far, no truly scalable regression tree algorithm is known. This paper proposes a novel regression tree construction algorithm (SECRET) that produces trees of high quality and scales to very large datasets. At every node, SECRET uses the EM algorithm for Gaussian mixtures to find two clusters in the data and to locally transform the regression problem into a classification problem based on closeness to these clusters. Goodness of split measures, like the gini gain, can then be used to determine the split variable and the split point much like in classification tree construction. Scalability of the algorithm can be achieved by employing scalable versions of the EM and classification tree construction algorithms. An experimental evaluation on real and artificial data shows that SECRET has accuracy comparable to other linear regression tree algorithms but takes orders of magnitude less computation time for large datasets.
KDD	Instability of decision tree classification algorithms.	Ruey-Hsia Li,Geneva G. Belford	2002	The instability problem of decision tree classification algorithms is that small changes in input training samples may cause dramatically large changes in output classification rules. Different rules generated from almost the same training samples are against human intuition and complicate the process of decision making. In this paper, we present fundamental theorems for the instability problem of decision tree classifiers. The first theorem gives the relationship between a data change and the resulting tree structure change (i.e. split change). The second theorem, Instability Theorem, provides the cause of the instability problem. Based on the two theorems, algorithmic improvements can be made to lessen the instability problem. Empirical results illustrate the theorem statements. The trees constructed by the proposed algorithm are more stable, noise-tolerant, informative, expressive, and concise. Our proposed sensitivity measure can be used as a metric to evaluate the stability of splitting predicates. The tree sensitivity is an indicator of the confidence level in rules and the effective lifetime of rules.
KDD	SyMP: an efficient clustering approach to identify clusters of arbitrary shapes in large data sets.	Hichem Frigui	2002	We propose a new clustering algorithm, called SyMP, which is based on synchronization of pulse-coupled oscillators. SyMP represents each data point by an Integrate-and-Fire oscillator and uses the relative similarity between the points to model the interaction between the oscillators. SyMP is robust to noise and outliers, determines the number of clusters in an unsupervised manner, identifies clusters of arbitrary shapes, and can handle very large data sets. The robustness of SyMP is an intrinsic property of the synchronization mechanism. To determine the optimum number of clusters, SyMP uses a dynamic resolution parameter. To identify clusters of various shapes, SyMP models each cluster by multiple Gaussian components. The number of components is automatically determined using a dynamic intra-cluster resolution parameter. Clusters with simple shapes would be modeled by few components while clusters with more complex shapes would require a larger number of components. The scalable version of SyMP uses an efficient incremental approach that requires a simple pass through the data set. The proposed clustering approach is empirically evaluated with several synthetic and real data sets, and its performance is compared with CURE.
KDD	From run-time behavior to usage scenarios: an interaction-pattern mining approach.	Mohammad El-Ramly,Eleni Stroulia,Paul G. Sorenson	2002	"A key challenge facing IT organizations today is their evolution towards adopting e-business practices that gives rise to the need for reengineering their underlying software systems. Any reengineering effort has to be aware of the functional requirements of the subject system, in order not to violate the integrity of its intended uses. However, as software systems get regularly maintained throughout their lifecycle, the documentation of their requirements often become obsolete or get lost. To address this problem of ""software requirements loss"", we have developed an interaction-pattern mining method for the recovery of functional requirements as usage scenarios. Our method analyzes traces of the run-time system-user interaction to discover frequently recurring patterns; these patterns correspond to the functionality currently exercised by the system users, represented as usage scenarios. The discovered scenarios provide the basis for reengineering the software system into web-accessible components, each one supporting one of the discovered scenarios. In this paper, we describe IPM2, our interaction-pattern discovery algorithm, we illustrate it with a case study from a real application and we give an overview of the reengineering process in the context of which it is employed."
KDD	Tina Eliassi-Rad, Terence Critchlow, Ghaleb Abdulla.	Tina Eliassi-Rad,Terence Critchlow,Ghaleb Abdulla	2002	Tina Eliassi-Rad, Terence Critchlow, Ghaleb Abdulla.
KDD	Web site mining: a new way to spot competitors, customers and suppliers in the world wide web.	Martin Ester,Hans-Peter Kriegel,Matthias Schubert	2002	When automatically extracting information from the world wide web, most established methods focus on spotting single HTML-documents. However, the problem of spotting complete web sites is not handled adequately yet, in spite of its importance for various applications. Therefore, this paper discusses the classification of complete web sites. First, we point out the main differences to page classification by discussing a very intuitive approach and its weaknesses. This approach treats a web site as one large HTML-document and applies the well-known methods for page classification. Next, we show how accuracy can be improved by employing a preprocessing step which assigns an occurring web page to its most likely topic. The determined topics now represent the information the web site contains and can be used to classify it more accurately. We accomplish this by following two directions. First, we apply well established classification algorithms to a feature space of occurring topics. The second direction treats a site as a tree of occurring topics and uses a Markov tree model for further classification. To improve the efficiency of this approach, we additionally introduce a powerful pruning method reducing the number of considered web pages. Our experiments show the superiority of the Markov tree approach regarding classification accuracy. In particular, we demonstrate that the use of our pruning method not only reduces the processing time, but also improves the classification accuracy.
KDD	Privacy preserving mining of association rules.	Alexandre V. Evfimievski,Ramakrishnan Srikant,Rakesh Agrawal,Johannes Gehrke	2002	"We present a framework for mining association rules from transactions consisting of categorical items where the data has been randomized to preserve privacy of individual transactions. While it is feasible to recover association rules and preserve privacy using a straightforward ""uniform"" randomization, the discovered rules can unfortunately be exploited to find privacy breaches. We analyze the nature of privacy breaches and propose a class of randomization operators that are much more effective than uniform randomization in limiting the breaches. We derive formulae for an unbiased support estimator and its variance, which allow us to recover itemset supports from randomized datasets, and show how to incorporate these formulae into mining algorithms. Finally, we present experimental results that validate the algorithm by applying it on real datasets."
KDD	Scaling multi-class support vector machines using inter-class confusion.	Shantanu Godbole,Sunita Sarawagi,Soumen Chakrabarti	2002	Support vector machines (SVMs) excel at two-class discriminative learning problems. They often outperform generative classifiers, especially those that use inaccurate generative models, such as the na&iuml;ve Bayes (NB) classifier. On the other hand, generative classifiers have no trouble in handling an arbitrary number of classes efficiently, and NB classifiers train much faster than SVMs owing to their extreme simplicity. In contrast, SVMs handle multi-class problems by learning redundant yes/no (one-vs-others) classifiers for each class, further worsening the performance gap. We propose a new technique for multi-way classification which exploits the accuracy of SVMs and the speed of NB classifiers. We first use a NB classifier to quickly compute a confusion matrix, which is used to reduce the number and complexity of the two-class SVMs that are built in the second stage. During testing, we first get the prediction of a NB classifier and use that to selectively apply only a subset of the two-class SVMs. On standard benchmarks, our algorithm is 3 to 6 times faster than SVMs and yet matches or even exceeds their accuracy.
KDD	Tumor cell identification using features rules.	Bin Fang,Wynne Hsu,Mong-Li Lee	2002	Advances in imaging techniques have led to large repositories of images. There is an increasing demand for automated systems that can analyze complex medical images and extract meaningful information for mining patterns. Here, we describe a real-life image mining application to the problem of tumour cell counting. The quantitative analysis of tumour cells is fundamental to characterizing the activity of tumour cells. Existing approaches are mostly manual, time-consuming and subjective. Efforts to automate the process of cell counting have largely focused on using image processing techniques only. Our studies indicate that image processing alone is unable to give accurate results. In this paper, we examine the use of extracted features rules to aid in the process of tumor cell counting. We propose a robust local adaptive thresholding and dynamic water immersion algorithms to segment regions of interesting from background. Meaningful features are then extracted from the segmented regions. A number of base classifiers are built to generate features rules to help identify the tumor cell. Two voting strategies are implemented to combine the base classifiers into a meta-classifier. Experiment results indicate that this process of using extracted features rules to help identify tumor cell leads to better accuracy than pure image processing techniques alone.
KDD	Integrating feature and instance selection for text classification.	Dimitris Fragoudis,Dimitris Meretakis,Spiros Likothanassis	2002	Instance selection and feature selection are two orthogonal methods for reducing the amount and complexity of data. Feature selection aims at the reduction of redundant features in a dataset whereas instance selection aims at the reduction of the number of instances. So far, these two methods have mostly been considered in isolation. In this paper, we present a new algorithm, which we call FIS (Feature and Instance Selection) that targets both problems simultaneously in the context of text classificationOur experiments on the Reuters and 20-Newsgroups datasets show that FIS considerably reduces both the number of features and the number of instances. The accuracy of a range of classifiers including Na&iuml;ve Bayes, TAN and LB considerably improves when using the FIS preprocessed datasets, matching and exceeding that of Support Vector Machines, which is currently considered to be one of the best text classification methods. In all cases the results are much better compared to Mutual Information based feature selection. The training and classification speed of all classifiers is also greatly improved.
KDD	Visualization support for a user-centered KDD process.	Tu Bao Ho,Trong Dung Nguyen,DucDung Nguyen	2002	Viewing knowledge discovery as a user-centered process that requires an effective collaboration between the user and the discovery system, our work aims to support an active role of the user in that process by developing synergistic visualization tools integrated in our discovery system D2MS. These tools provide an ability of visualizing the entire process of knowledge discovery in order to help the user with data preprocessing, selecting mining algorithms and parameters, evaluating and comparing discovered models, and taking control of the whole discover process. Our case-studies with two medical datasets on meningitis and stomach cancer show that, with visualization tools in D2MS, the user gains better insight in each step of the knowledge discovery process as well the relationship between data and discovered knowledge.
KDD	Mining complex models from arbitrarily large databases in constant time.	Geoff Hulten,Pedro Domingos	2002	"In this paper we propose a scaling-up method that is applicable to essentially any induction algorithm based on discrete search. The result of applying the method to an algorithm is that its running time becomes independent of the size of the database, while the decisions made are essentially identical to those that would be made given infinite data. The method works within pre-specified memory limits and, as long as the data is iid, only requires accessing it sequentially. It gives anytime results, and can be used to produce batch, stream, time-changing and active-learning versions of an algorithm. We apply the method to learning Bayesian networks, developing an algorithm that is faster than previous ones by orders of magnitude, while achieving essentially the same predictive performance. We observe these gains on a series of large databases ""generated from benchmark networks, on the KDD Cup 2000 e-commerce data, and on a Web log containing 100 million requests."
KDD	Transforming data to satisfy privacy constraints.	Vijay S. Iyengar	2002	Data on individuals and entities are being collected widely. These data can contain information that explicitly identifies the individual (e.g., social security number). Data can also contain other kinds of personal information (e.g., date of birth, zip code, gender) that are potentially identifying when linked with other available data sets. Data are often shared for business or legal reasons. This paper addresses the important issue of preserving the anonymity of the individuals or entities during the data dissemination process. We explore preserving the anonymity by the use of generalizations and suppressions on the potentially identifying portions of the data. We extend earlier works in this area along various dimensions. First, satisfying privacy constraints is considered in conjunction with the usage for the data being disseminated. This allows us to optimize the process of preserving privacy for the specified usage. In particular, we investigate the privacy transformation in the context of data mining applications like building classification and regression models. Second, our work improves on previous approaches by allowing more flexible generalizations for the data. Lastly, this is combined with a more thorough exploration of the solution space using the genetic algorithm framework. These extensions allow us to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraints.
KDD	A model for discovering customer value for E-content.	Srinivasan Jagannathan,Jayanth Nayak,Kevin C. Almeroth,Markus Hofmann	2002	There exists a huge demand for multimedia goods and services in the Internet. Currently available bandwidth speeds can support sale of downloadable content like CDs, e-books, etc. as well as services like video-on-demand. In the future, such services will be prevalent in the Internet. Since costs are typically fixed, maximizing revenue can maximize profits. A primary determinant of revenue in such e-content markets is how much value the customers associate with the content. Though marketing surveys are useful, they cannot adapt to the dynamic nature of the Internet market. In this work, we examine how to learn customer valuations in close to real-time. Our contributions in this paper are threefold: (1) we develop a probabilistic model to describe customer behavior, (2) we develop a framework for pricing e-content based on basic economic principles, and (3) we propose a price discovering algorithm that learns customer behavior parameters and suggests prices to an e-content provider. We validate our algorithm using simulations. Our simulations indicate that our algorithm generates revenue close to the maximum expectation. Further, they also indicate that the algorithm is robust to transient customer behavior.
KDD	SimRank: a measure of structural-context similarity.	Glen Jeh,Jennifer Widom	2002	"The problem of measuring ""similarity"" of objects arises in many applications, and many domain-specific measures have been developed, e.g., matching text across documents or computing overlap among item-sets. We propose a complementary approach, applicable in any domain with object-to-object relationships, that measures similarity of the structural context in which objects occur, based on their relationships with other objects. Effectively, we compute a measure that says ""two objects are similar if they are related to similar objects:"" This general similarity measure, called SimRank, is based on a simple and intuitive graph-theoretic model. For a given domain, SimRank can be combined with other domain-specific similarity measures. We suggest techniques for efficient computation of SimRank scores, and provide experimental results on two application domains showing the computational feasibility and effectiveness of our approach."
KDD	Similarity measure based on partial information of time series.	Xiaoming Jin,Yuchang Lu,Chunyi Shi	2002	Similarity measure of time series is an important subroutine in many KDD applications. Previous similarity models mainly focus on the prominent series behaviors by considering the whole information of time series. In this paper, we address the problem: which portion of information is more suitable for similarity measure for the data collected from a certain field. We propose a model for the retrieval and representation of the partial information in time series data, and a methodology for evaluating the similarity measurements based on partial information. The methodology is to retrieve various portions of information from the raw data and represent it in a concise form, then cluster the time series using the partial information and evaluate the similarity measurements through comparing the results with a standard classification. Experiments on data set from stock market give some interesting observations and justify the usefulness of our approach.
KDD	Optimizing search engines using clickthrough data.	Thorsten Joachims	2002	This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.
KDD	Predicting rare classes: can boosting make any weak learner strong?	Mahesh V. Joshi,Ramesh C. Agarwal,Vipin Kumar	2002	Boosting is a strong ensemble-based learning algorithm with the promise of iteratively improving the classification accuracy using any base learner, as long as it satisfies the condition of yielding weighted accuracy > 0.5. In this paper, we analyze boosting with respect to this basic condition on the base learner, to see if boosting ensures prediction of rarely occurring events with high recall and precision. First we show that a base learner can satisfy the required condition even for poor recall or precision levels, especially for very rare classes. Furthermore, we show that the intelligent weight updating mechanism in boosting, even in its strong cost-sensitive form, does not prevent cases where the base learner always achieves high precision but poor recall or high recall but poor precision, when mapped to the original distribution. In either of these cases, we show that the voting mechanism of boosting falls to achieve good overall recall and precision for the ensemble. In effect, our analysis indicates that one cannot be blind to the base learner performance, and just rely on the boosting mechanism to take care of its weakness. We validate our arguments empirically on variety of real and synthetic rare class problems. In particular, using AdaCost as the boosting algorithm, and variations of PNrule and RIPPER as the base learners, we show that if algorithm A achieves better recall-precision balance than algorithm B, then using A as the base learner in AdaCost yields significantly better performance than using B as the base learner.
KDD	Mining intrusion detection alarms for actionable knowledge.	Klaus Julisch,Marc Dacier	2002	In response to attacks against enterprise networks, administrators increasingly deploy intrusion detection systems. These systems monitor hosts, networks, and other resources for signs of security violations. The use of intrusion detection has given rise to another difficult problem, namely the handling of a generally large number of alarms. In this paper, we mine historical alarms to learn how future alarms can be handled more efficiently. First, we investigate episode rules with respect to their suitability in this approach. We report the difficulties encountered and the unexpected insights gained. In addition, we introduce a new conceptual clustering technique, and use it in extensive experiments with real-world data to show that intrusion detection alarms can be handled efficiently by using previously mined knowledge.
KDD	On the need for time series data mining benchmarks: a survey and empirical demonstration.	Eamonn J. Keogh,Shruti Kasetty	2002	In the last decade there has been an explosion of interest in mining time series data. Literally hundreds of papers have introduced new algorithms to index, classify, cluster and segment time series. In this work we make the following claim. Much of this work has very little utility because the contribution made (speed in the case of indexing, accuracy in the case of classification and clustering, model accuracy in the case of segmentation) offer an amount of &ldquo;improvement&rdquo; that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets, or the variance that would have been observed by changing minor (unstated) implementation details.To illustrate our point, we have undertaken the most exhaustive set of time series experiments ever attempted, re-implementing the contribution of more than two dozen papers, and testing them on 50 real world, highly diverse datasets. Our empirical results strongly support our assertion, and suggest the need for a set of time series benchmarks and more careful empirical evaluation in the data mining community.
KDD	Finding surprising patterns in a time series database in linear time and space.	Eamonn J. Keogh,Stefano Lonardi,Bill Yuan-chi Chiu	2002	"The problem of finding a specified pattern in a time series database (i.e. query by content) has received much attention and is now a relatively mature field. In contrast, the important problem of enumerating all surprising or interesting patterns has received far less attention. This problem requires a meaningful definition of ""surprise"", and an efficient search technique. All previous attempts at finding surprising patterns in time series use a very limited notion of surprise, and/or do not scale to massive datasets. To overcome these limitations we introduce a novel technique that defines a pattern surprising if the frequency of its occurrence differs substantially from that expected by chance, given some previously seen data."
KDD	Bursty and hierarchical structure in streams.	Jon M. Kleinberg	2002	A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise&mdash;that the appearance of a topic in a document stream is signaled by a &ldquo;burst of activity,&rdquo; with certain features rising sharply in frequency as the topic emerges.The goal of the present work is to develop a formal approach for modeling such &ldquo;bursts,&rdquo; in such a way that they can be robustly and efficiently identified, and can provide an organizational framework for analyzing the underlying content. The approach is based on modeling the stream using an infinite-state automaton, in which bursts appear naturally as state transitions&semi; it can be viewed as drawing an analogy with models from queueing theory for bursty network traffic. The resulting algorithms are highly efficient, and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream. Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them.
KDD	Efficient handling of high-dimensional feature spaces by randomized classifier ensembles.	Aleksander Kolcz,Xiaomei Sun,Jugal K. Kalita	2002	"Handling massive datasets is a difficult problem not only due to prohibitively large numbers of entries but in some cases also due to the very high dimensionality of the data. Often, severe feature selection is performed to limit the number of attributes to a manageable size, which unfortunately can lead to a loss of useful information. Feature space reduction may well be necessary for many stand-alone classifiers, but recent advances in the area of ensemble classifier techniques indicate that overall accurate classifier aggregates can be learned even if each individual classifier operates on incomplete ""feature view"" training data, i.e., such where certain input attributes are excluded. In fact, by using only small random subsets of features to build individual component classifiers, surprisingly accurate and robust models can be created. In this work we demonstrate how these types of architectures effectively reduce the feature space for submodels and groups of sub-models, which lends itself to efficient sequential and/or parallel implementations. Experiments with a randomized version of Adaboost are used to support our arguments, using the text classification task as an example."
KDD	A parallel learning algorithm for text classification.	Canasai Kruengkrai,Chuleerat Jaruskulchai	2002	Text classification is the process of classifying documents into predefined categories based on their content. Existing supervised learning algorithms to automatically classify text need sufficient labeled documents to learn accurately. Applying the Expectation-Maximization (EM) algorithm to this problem is an alternative approach that utilizes a large pool of unlabeled documents to augment the available labeled documents. Unfortunately, the time needed to learn with these large unlabeled documents is too high. This paper introduces a novel parallel learning algorithm for text classification task. The parallel algorithm is based on the combination of the EM algorithm and the naive Bayes classifier. Our goal is to improve the computational time in learning and classifying process. We studied the performance of our parallel algorithm on a large Linux PC cluster called PIRUN Cluster. We report both timing and accuracy results. These results indicate that the proposed parallel algorithm is capable of handling large document collections.
KDD	Clustering seasonality patterns in the presence of errors.	Mahesh Kumar,Nitin R. Patel,Jonathan Woo	2002	Clustering is a very well studied problem that attempts to group similar data points. Most traditional clustering algorithms assume that the data is provided without measurement error. Often, however, real world data sets have such errors and one can obtain estimates of these errors. We present a clustering method that incorporates information contained in these error estimates. We present a new distance function that is based on the distribution of errors in data. Using a Gaussian model for errors, the distance function follows a Chi-Square distribution and is easy to compute. This distance function is used in hierarchical clustering to discover meaningful clusters. The distance function is scale-invariant so that clustering results are independent of units of measuring data. In the special case when the error distribution is the same for each attribute of data points, the rank order of pair-wise distances is the same for our distance function and the Euclidean distance function. The clustering method is applied to the seasonality estimation problem and experimental results are presented for the retail industry data as well as for simulated data, where it outperforms classical clustering methods.
KDD	Construct robust rule sets for classification.	Jiuyong Li,Rodney W. Topor,Hong Shen	2002	We study the problem of computing classification rule sets from relational databases so that accurate predictions can be made on test data with missing attribute values. Traditional classifiers perform badly when test data are not as complete as the training data because they tailor a training database too much. We introduce the concept of one rule set being more robust than another, that is, able to make more accurate predictions on test data with missing attribute values. We show that the optimal class association rule set is as robust as the complete class association rule set. We then introduce the k-optimal rule set, which provides predictions exactly the same as the optimal class association rule set on test data with up to k missing attribute values. This leads to a hierarchy of k-optimal rule sets in which decreasing size corresponds to decreasing robustness, and they all more robust than a traditional classification rule set. We introduce two methods to find k-optimal rule sets, i.e. an optimal association rule mining approach and a heuristic approximate approach. We show experimentally that a k-optimal rule set generated by the optimal association rule mining approach performs better than that by the heuristic approximate approach and both rule sets perform significantly better than a typical classification rule set (C4.5Rules) on incomplete test data.
KDD	Mining heterogeneous gene expression data with time lagged recurrent neural networks.	Yulan Liang,Arpad Kelemen	2002	Heterogeneous types of gene expressions may provide a better insight into the biological role of gene interaction with the environment, disease development and drug effect at the molecular level. In this paper for both exploring and prediction purposes a Time Lagged Recurrent Neural Network with trajectory learning is proposed for identifying and classifying the gene functional patterns from the heterogeneous nonlinear time series microarray experiments. The proposed procedures identify gene functional patterns from the dynamics of a state-trajectory learned in the heterogeneous time series and the gradient information over time. Also, the trajectory learning with Back-propagation through time algorithm can recognize gene expression patterns vary over time. This may reveal much more information about the regulatory network underlying gene expressions. The analyzed data were extracted from spotted DNA microarrays in the budding yeast expression measurements, produced by Eisen et al. The gene matrix contained 79 experiments over a variety of heterogeneous experiment conditions. The number of recognized gene patterns in our study ranged from two to ten and were divided into three cases. Optimal network architectures with different memory structures were selected based on Akaike and Bayesian information statistical criteria using two-way factorial design. The optimal model performance was compared to other popular gene classification algorithms such as Nearest Neighbor, Support Vector Machine, and Self-Organized Map. The reliability of the performance was verified with multiple iterated runs.
KDD	A robust and efficient clustering algorithm based on cohesion self-merging.	Cheng-Ru Lin,Ming-Syan Chen	2002	Data clustering has attracted a lot of research attention in the field of computational statistics and data mining. In most related studies, the dissimilarity between two clusters is defined as the distance between their centroids, or the distance between two closest (or farthest) data points. However, all of these measurements are vulnerable to outliers, and removing the outliers precisely is yet another difficult task. In view of this, we propose a new similarity measurement referred to as cohesion, to measure the inter-cluster distances. By using this new measurement of cohesion, we design a two-phase clustering algorithm, called cohesion-based self-merging (abbreviated as CSM), which runs in linear time to the size of input data set. Combining the features of partitional and hierarchical clustering methods, algorithm CSM partitions the input data set into several small subclusters in the first phase, and then continuously merges the subclusters based on cohesion in a hierarchical manner in the second phase. As shown by our performance studies, the cohesion-based clustering is very robust and possesses the excellent tolerance to outliers in various workloads. More importantly, algorithm CSM is shown to be able to cluster the data sets of arbitrary shapes very efficiently, and provide better clustering results than those by prior methods.Index Terms: Data mining, data clustering, hierarchical clustering, partitional clustering
KDD	Discovering informative content blocks from Web documents.	Shian-Hua Lin,Jan-Ming Ho	2002	In this paper, we propose a new approach to discover informative contents from a set of tabular documents (or Web pages) of a Web site. Our system, InfoDiscoverer, first partitions a page into several content blocks according to HTML tag <TABLE> in a Web page. Based on the occurrence of the features (terms) in the set of pages, it calculates entropy value of each feature. According to the entropy value of each feature in a content block, the entropy value of the block is defined. By analyzing the information measure, we propose a method to dynamically select the entropy-threshold that partitions blocks into either informative or redundant. Informative content blocks are distinguished parts of the page, whereas redundant content blocks are common parts. Based on the answer set generated from 13 manually tagged news Web sites with a total of 26,518 Web pages, experiments show that both recall and precision rates are greater than 0.956. That is, using the approach, informative blocks (news articles) of these sites can be automatically separated from semantically redundant contents such as advertisements, banners, navigation panels, news categories, etc. By adopting InfoDiscoverer as the preprocessor of information retrieval and extraction applications, the retrieval and extracting precision will be increased, and the indexing size and extracting complexity will also be reduced.
KDD	Distributed data mining in a chain store database of short transactions.	Cheng-Ru Lin,Chang-Hung Lee,Ming-Syan Chen,Philip S. Yu	2002	In this paper, we broaden the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database. Specifically, the causality rule explored in this paper consists of a sequence of triggering events and a set of consequential events, and is designed with the capability of mining non-sequential, inter-transaction information. Hence, the causality rule mining provides a very general framework for rule derivation. Note, however, that the procedure of causality rule mining is very costly particularly in the presence of a huge number of candidate sets and a distributed database, and in our opinion, cannot be dealt with by direct extensions from existing rule mining methods. Consequently, we devise in this paper a series of level matching algorithms, including Level Matching (abbreviatedly as LM), Level Matching with Selective Scan (abbreviatedly as LMS), and Distributed Level Matching (abbreviatedly as Distibuted LM), to minimize the computing cost needed for the distributed data mining of causality rules. In addition, the phenomena of time window constraints are also taken into consideration for the development of our algorithms. As a result of properly employing the technologies of level matching and selective scan, the proposed algorithms present good efficiency and scalability in the mining of local and global causality rules. Scale-up experiments show that the proposed algorithms scale well with the number of sites and the number of customer transactions.Index Terms: knowledge discovery, distributed data mining causality rules, triggering events, consequential events
KDD	Collusion in the U.S. crop insurance program: applied data mining.	Bertis B. Little,Walter L. Johnston,Ashley C. Lovell,Roderick M. Rejesus,Steve A. Steed	2002	"This paper quantitatively analyzes indicators of Agent (policy seller), Adjuster (indemnity claim adjuster), Producer (policy purchaser/holder) indemnity behavior suggestive of collusion in the United States Department of Agriculture (USDA) Risk Management Agency (RMA) national crop insurance program. According to guidance from the federal law and using six indicator variables of indemnity behavior, those entities equal to or exceeding 150% of the county mean (computed using a simple jackknife procedure) on all entity-relevant indicators were flagged as ""anomalous."" Log linear analysis was used to test (I) hierarchical node-node arrangements and (2) a non-recursive model of node information sharing. Chi-square distributed deviance statistic identified the optimal log linear model. The results of the applied data mining technique used here suggest that the non-recursive triplet and Agent-producer doublet collusion probabilistically accounts for the greatest proportion of waste, fraud, and abuse in the federal crop insurance program. Triplet and Agent-producer doublets need detailed investigation for possible collusion. Hence, this data mining technique provided a high level of confidence when 24 million records were quantitatively analyzed for possible fraud, waste, or other abuse of the crop insurance program administered by the USDA RMA, and suspect entities reported to USDA. This data mining technique can be applied where vast amounts of data are available to detect patterns of collusion or conspiracy as may be of interest to the criminal justice or intelligence agencies."
KDD	Incremental context mining for adaptive document classification.	Rey-Long Liu,Yun-Ling Lu	2002	Automatic document classification (DC) is essential for the management of information and knowledge. This paper explores two practical issues in DC: (1) each document has its context of discussion, and (2) both the content and vocabulary of the document database is intrinsically evolving. The issues call for adaptive document classification (ADC) that adapts a DC system to the evolving contextual requirement of each document category, so that input documents may be classified based on their contexts of discussion. We present an incremental context mining technique to tackle the challenges of ADC. Theoretical analyses and empirical results show that, given a text hierarchy, the mining technique is efficient in incrementally maintaining the evolving contextual requirement of each category. Based on the contextual requirements mined by the system, higher-precision DC may be achieved with better efficiency.
KDD	Mining frequent item sets by opportunistic projection.	Junqiang Liu,Yunhe Pan,Ke Wang,Jiawei Han	2002	In this paper, we present a novel algorithm Opportune Project for mining complete set of frequent item sets by projecting databases to grow a frequent item set tree. Our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structures, array-based or tree-based, to represent projected transaction subsets, and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to features of the subsets. More importantly, we propose novel methods to build tree-based pseudo projections and array-based unfiltered projections for projected transaction subsets, which makes our algorithm both CPU time efficient and memory saving. Basically, the algorithm grows the frequent item set tree by depth first search, whereas breadth first search is used to build the upper portion of the tree if necessary. We test our algorithm versus several other algorithms on real world datasets, such as BMS-POS, and on IBM artificial datasets. The empirical results show that our algorithm is not only the most efficient on both sparse and dense databases at all levels of support threshold, but also highly scalable to very large databases.
KDD	Learning nonstationary models of normal network traffic for detecting novel attacks.	Matthew V. Mahoney,Philip K. Chan	2002	Traditional intrusion detection systems (IDS) detect attacks by comparing current behavior to signatures of known attacks. One main drawback is the inability of detecting new attacks which do not have known signatures. In this paper we propose a learning algorithm that constructs models of normal behavior from attack-free network traffic. Behavior that deviates from the learned normal model signals possible novel attacks. Our IDS is unique in two respects. First, it is nonstationary, modeling probabilities based on the time since the last event rather than on average rate. This prevents alarm floods. Second, the IDS learns protocol vocabularies (at the data link through application layers) in order to detect unknown attacks that attempt to exploit implementation errors in poorly tested features of the target software. On the 1999 DARPA IDS evaluation data set [9], we detect 70 of 180 attacks (with 100 false alarms), about evenly divided between user behavioral anomalies (IP addresses and ports, as modeled by most other systems) and protocol anomalies. Because our methods are unconventional there is a significant non-overlap of our IDS with the original DARPA participants, which implies that they could be combined to increase coverage.
KDD	Mining product reputations on the Web.	Satoshi Morinaga,Kenji Yamanishi,Kenji Tateishi,Toshikazu Fukushima	2002	"Knowing the reputations of your own and/or competitors' products is important for marketing and customer relationship management. It is, however, very costly to collect and analyze survey data manually. This paper presents a new framework for mining product reputations on the Internet. It automatically collects people's opinions about target products from Web pages, and it uses text mining techniques to obtain the reputations of those products.On the basis of human-test samples, we generate in advance syntactic and linguistic rules to determine whether any given statement is an opinion or not, as well as whether such any opinion is positive or negative in nature. We first collect statements regarding target products using a general search engine, and then, using the rules, extract opinions from among them and attach three labels to each opinion, labels indicating the positive/negative determination, the product name itself, and an numerical value expressing the degree of system confidence that the statement is, in fact, an opinion. The labeled opinions are then input into an opinion database.The mining of reputations, i.e., the finding of statistically meaningful information included in the database, is then conducted. We specify target categories using label values (such as positive opinions of product A) and perform four types of text mining: extraction of 1) characteristic words, 2) co-occurrence words, 3) typical sentences, for individual target categories, and 4) correspondence analysis among multiple target categories.Actual marketing data is used to demonstrate the validity and effectiveness of the framework, which offers a drastic reduction in the overall cost of reputation analysis over that of conventional survey approaches and supports the discovery of knowledge from the pool of opinions on the web."
KDD	"Evaluating classifiers' performance in a constrained environment."	Anna Olecka	2002	In this paper, we focus on methodology of finding a classifier with a minimal cost in presence of additional performance constraints. ROCCH analysis, where accuracy and cost are intertwined in the solution space, was a revolutionary tool for two-class problems. We propose an alternative formulation, as an optimization problem, commonly used in Operations Research. This approach extends the ROCCH analysis to allow for locating optimal solutions while outside constraints are present. Similarly to the ROCCH analysis, we combine cost and class distribution while defining the objective function. Rather than focusing on slopes of the edges in the convex hull of the solution space, however, we treat cost as an objective function to be minimized over the solution space, by selecting the best performing classifier(s) (one or more vertex in the solution space). The Linear Programming framework provides a theoretical and computational methodology for finding the vertex (classifier) which minimizes the objective function.
KDD	ANF: a fast and scalable tool for data mining in massive graphs.	Christopher R. Palmer,Phillip B. Gibbons,Christos Faloutsos	2002	"Graphs are an increasingly important data source, with such important graphs as the Internet and the Web. Other familiar graphs include CAD circuits, phone records, gene sequences, city streets, social networks and academic citations. Any kind of relationship, such as actors appearing in movies, can be represented as a graph. This work presents a data mining tool, called ANF, that can quickly answer a number of interesting questions on graph-represented data, such as the following. How robust is the Internet to failures? What are the most influential database papers? Are there gender differences in movie appearance patterns? At its core, ANF is based on a fast and memory-efficient approach for approximating the complete ""neighbourhood function"" for a graph. For the Internet graph (268K nodes), ANF's highly-accurate approximation is more than 700 times faster than the exact computation. This reduces the running time from nearly a day to a matter of a minute or two, allowing users to perform ad hoc drill-down tasks and to repeatedly answer questions about changing data sources. To enable this drill-down, ANF employs new techniques for approximating neighbourhood-type functions for graphs with distinguished nodes and/or edges. When compared to the best existing approximation, ANF's approach is both faster and more accurate, given the same resources. Additionally, unlike previous approaches, ANF scales gracefully to handle disk resident graphs. Finally, we present some of our results from mining large graphs using ANF."
KDD	Discovering word senses from text.	Patrick Pantel,Dekang Lin	2002	Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning words to their most similar clusters. After assigning an element to a cluster, we remove their overlapping features from the element. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. Each cluster that a word belongs to represents one of its senses. We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses.
KDD	Sequential cost-sensitive decision making with reinforcement learning.	Edwin P. D. Pednault,Naoki Abe,Bianca Zadrozny	2002	"Recently, there has been increasing interest in the issues of cost-sensitive learning and decision making in a variety of applications of data mining. A number of approaches have been developed that are effective at optimizing cost-sensitive decisions when each decision is considered in isolation. However, the issue of sequential decision making, with the goal of maximizing total benefits accrued over a period of time instead of immediate benefits, has rarely been addressed. In the present paper, we propose a novel approach to sequential decision making based on the reinforcement learning framework. Our approach attempts to learn decision rules that optimize a sequence of cost-sensitive decisions so as to maximize the total benefits accrued over time. We use the domain of targeted' marketing as a testbed for empirical evaluation of the proposed method. We conducted experiments using approximately two years of monthly promotion data derived from the well-known KDD Cup 1998 donation data set. The experimental results show that the proposed method for optimizing total accrued benefits out performs the usual targeted-marketing methodology of optimizing each promotion in isolation. We also analyze the behavior of the targeting rules that were obtained and discuss their appropriateness to the application domain."
KDD	B-EM: a classifier incorporating bootstrap with EM approach for data mining.	Xintao Wu,Jianping Fan,Kalpathi R. Subramanian	2002	This paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy. This is significant for many applications such as image classification where obtaining classification labels is expensive, while large unlabeled examples are easily available. We investigate an Expectation Maximization (EM) algorithm for learning from labeled and unlabeled data. The reason why unlabeled data boosts learning accuracy is because it provides the information about the joint probability distribution. A theoretical argument shows that the more unlabeled examples are combined in learning, the more accurate the result. We then introduce B-EM algorithm, based on the combination of EM with bootstrap method, to exploit the large unlabeled data while avoiding prohibitive I/O cost. Experimental results over both synthetic and real data sets that the proposed approach has a satisfactory performance.
KDD	Making every bit count: fast nonlinear axis scaling.	Leejay Wu,Christos Faloutsos	2002	Existing axis scaling and dimensionality methods focus on preserving structure, usually determined via the Euclidean distance. In other words, they inherently assume that the Euclidean distance is already correct. We instead propose a novel nonlinear approach driven by an information-theoretic viewpoint, which we show is also strongly linked to intrinsic dimensionality, or degrees of freedom; and uniformity. Nonlinear transformations based on common probability distributions, combined with information-driven selection, simultaneously reduce the number of dimensions required and increase the value of those we retain. Experiments on real data confirm that this approach reveals correlations, finds novel attributes, and scales well.
KDD	Bayesian analysis of massive datasets via particle filters.	Greg Ridgeway,David Madigan	2002	"Markov Chain Monte Carlo (MCMC) techniques revolutionized statistical practice in the 1990s by providing an essential toolkit for making the rigor and flexibility of Bayesian analysis computationally practical. At the same time the increasing prevalence of massive datasets and the expansion of the field of data mining has created the need to produce statistically sound methods that scale to these large problems. Except for the most trivial examples, current MCMC methods require a complete scan of the dataset for each iteration eliminating their candidacy as feasible data mining techniques.In this article we present a method for making Bayesian analysis of massive datasets computationally feasible. The algorithm simulates from a posterior distribution that conditions on a smaller, more manageable portion of the dataset. The remainder of the dataset may be incorporated by reweighting the initial draws using importance sampling. Computation of the importance weights requires a single scan of the remaining observations. While importance sampling increases efficiency in data access, it comes at the expense of estimation efficiency. A simple modification, based on the ""rejuvenation"" step used in particle filters for dynamic systems models, sidesteps the loss of efficiency with only a slight increase in the number of data accesses.To show proof-of-concept, we demonstrate the method on a mixture of transition models that has been used to model web traffic and robotics. For this example we show that estimation efficiency is not affected while offering a 95% reduction in data accesses."
KDD	Customer lifetime value modeling and its use for customer retention planning.	Saharon Rosset,Einat Neumann,Uri Eick,Nurit Vatnik,Yizhak Idan	2002	"We present and discuss the important business problem of estimating the effect of retention efforts on the Lifetime Value of a customer in the Telecommunications industry. We discuss the components of this problem, in particular customer value and length of service (or tenure) modeling, and present a novel segment-based approach, motivated by the segment-level view marketing analysts usually employ. We then describe how we build on this approach to estimate the effects of retention on Lifetime Value. Our solution has been successfully implemented in Amdocs' Business Insight (BI) platform, and we illustrate its usefulness in real-world scenarios."
KDD	Interactive deduplication using active learning.	Sunita Sarawagi,Anuradha Bhamidipaty	2002	Deduplication is a key operation in integrating data from multiple sources. The main challenge in this task is designing a function that can resolve when a pair of records refer to the same entity in spite of various data inconsistencies. Most existing systems use hand-coded functions. One way to overcome the tedium of hand-coding is to train a classifier to distinguish between duplicates and non-duplicates. The success of this method critically hinges on being able to provide a covering and challenging set of training pairs that bring out the subtlety of deduplication function. This is non-trivial because it requires manually searching for various data inconsistencies between any two records spread apart in large lists.We present our design of a learning-based deduplication system that uses a novel method of interactively discovering challenging training pairs using active learning. Our experiments on real-life datasets show that active learning significantly reduces the number of instances needed to achieve high accuracy. We investigate various design issues that arise in building a system to provide interactive response, fast convergence, and interpretable output.
KDD	Combining clustering and co-training to enhance text classification using unlabelled data.	Bhavani Raskutti,Herman L. Ferrá,Adam Kowalczyk	2002	In this paper, we present a new co-training strategy that makes use of unlabelled data. It trains two predictors in parallel, with each predictor labelling the unlabelled data for training the other predictor in the next round. Both predictors are support vector machines, one trained using data from the original feature space, the other trained with new features that are derived by clustering both the labelled and unlabelled data. Hence, unlike standard co-training methods, our method does not require a priori the existence of two redundant views either of which can be used for classification, nor is it dependent on the availability of two different supervised learning algorithms that complement each other.We evaluated our method with two classifiers and three text benchmarks: WebKB, Reuters newswire articles and 20 NewsGroups. Our evaluation shows that our co-training technique improves text classification accuracy especially when the number of labelled examples are very few.
KDD	Mining knowledge-sharing sites for viral marketing.	Matthew Richardson,Pedro Domingos	2002	Viral marketing takes advantage of networks of influence among customers to inexpensively achieve large changes in behavior. Our research seeks to put it on a firmer footing by mining these networks from data, building probabilistic models of them, and using these models to choose the best viral marketing plan. Knowledge-sharing sites, where customers review products and advise each other, are a fertile source for this type of data mining. In this paper we extend our previous techniques, achieving a large reduction in computational cost, and apply them to data from a knowledge-sharing site. We optimize the amount of marketing funds spent on each customer, rather than just making a binary decision on whether to market to him. We take into account the fact that knowledge of the network is partial, and that gathering that knowledge can itself have a cost. Our results show the robustness and utility of our approach.
KDD	ADMIT: anomaly-based data mining for intrusions.	Karlton Sequeira,Mohammed Javeed Zaki	2002	Security of computer systems is essential to their acceptance and utility. Computer security analysts use intrusion detection systems to assist them in maintaining computer system security. This paper deals with the problem of differentiating between masqueraders and the true user of a computer terminal. Prior efficient solutions are less suited to real time application, often requiring all training data to be labeled, and do not inherently provide an intuitive idea of what the data model means. Our system, called ADMIT, relaxes these constraints, by creating user profiles using semi-incremental techniques. It is a real-time intrusion detection system with host-based data collection and processing. Our method also suggests ideas for dealing with concept drift and affords a detection rate as high as 80.3% and a false positive rate as low as 15.3%.
KDD	Query, analysis, and visualization of hierarchically structured data using Polaris.	Chris Stolte,Diane Tang,Pat Hanrahan	2002	In the last several years, large OLAP databases have become common in a variety of applications such as corporate data warehouses and scientific computing. To support interactive analysis, many of these databases are augmented with hierarchical structures that provide meaningful levels of abstraction that can be leveraged by both the computer and analyst. This hierarchical structure generates many challenges and opportunities in the design of systems for the query, analysis, and visualization of these databases.In this paper, we present an interactive visual exploration tool that facilitates exploratory analysis of data warehouses with rich hierarchical structure, such as might be stored in data cubes. We base this tool on Polaris, a system for rapidly constructing table-based graphical displays of multidimensional databases. Polaris builds visualizations using an algebraic formalism derived from the interface and interpreted as a set of queries to a database. We extend the user interface, algebraic formalism, and generation of data queries in Polaris to expose and take advantage of hierarchical structure. In the resulting system, analysts can navigate through the hierarchical projections of a database, rapidly and incrementally generating visualizations for each projection.
KDD	Exploiting response models: optimizing cross-sell and up-sell opportunities in banking.	Andrew Storey,Marc-David Cohen	2002	The banking industry regularly mounts campaigns to improve customer value by offering new products to existing customers. In recent years this approach has gained significant momentum because of the increasing availability of customer data and the improved analysis capabilities in data mining. Typically, response models based on historical data are used to estimate the probability of a customer purchasing an additional product and the expected return from that additional purchase. Even with these computational improvements and accurate models of customer behavior, the problem of efficiently using marketing resources to maximize the return on marketing investment is a challenge. This problem is compounded because of the capability to launch multiple campaigns through several distribution channels over multiple time periods. The combination of alternatives creates a complicated array of possible actions. This paper presents a solution that answers the question of what products, if any, to offer to each customer in a way that maximizes the marketing return on investment. The solution is an improvement over the usual approach of picking the customers that have the largest expected value for a particular product because it is a global maximization from the viewpoint of the bank and allows for the effective implementation of business constraints across customers and business units. The approach accounts for limited resources, multiple sequential campaigns, and other business constraints. Furthermore, the solution provides insight into the cost of these constraints, in terms of decreased profits, and thus is an effective tool for both tactical campaign execution and strategic planning.
KDD	Selecting the right interestingness measure for association patterns.	Pang-Ning Tan,Vipin Kumar,Jaideep Srivastava	2002	Many techniques for association rule mining and feature selection require a suitable metric to capture the dependencies among variables in a data set. For example, metrics such as support, confidence, lift, correlation, and collective strength are often used to determine the interestingness of association patterns. However, many such measures provide conflicting information about the interestingness of a pattern, and the best metric to use for a given application domain is rarely known. In this paper, we present an overview of various measures proposed in the statistics, machine learning and data mining literature. We describe several key properties one should examine in order to select the right measure for a given application domain. A comparative study of these properties is made using twenty one of the existing measures. We show that each measure has different properties which make them useful for some application domains, but not for others. We also present two scenarios in which most of the existing measures agree with each other, namely, support-based pruning and table standardization. Finally, we present an algorithm to select a small set of tables such that an expert can select a desirable measure by looking at just this small set of tables.
KDD	Hierarchical model-based clustering of large datasets through fractionation and refractionation.	Jeremy Tantrum,Alejandro Murua,Werner Stuetzle	2002	The goal of clustering is to identify distinct groups in a dataset. Compared to non-parametric clustering methods like complete linkage, hierarchical model-based clustering has the advantage of offering a way to estimate the number of groups present in the data. However, its computational cost is quadratic in the number of items to be clustered, and it is therefore not applicable to large problems. We review an idea called Fractionation, originally conceived by Cutting, Karger, Pedersen and Tukey for non-parametric hierarchical clustering of large datasets, and describe an adaptation of Fractionation to model-based clustering. A further extension, called Refractionation, leads to a procedure that can be successful even in the difficult situation where there are large numbers of small groups.
KDD	Learning domain-independent string transformation weights for high accuracy object identification.	Sheila Tejada,Craig A. Knoblock,Steven Minton	2002	"The task of object identification occurs when integrating information from multiple websites. The same data objects can exist in inconsistent text formats across sites, making it difficult to identify matching objects using exact text match. Previous methods of object identification have required manual construction of domain-specific string transformations or manual setting of general transformation parameter weights for recognizing format inconsistencies. This manual process can be time consuming and error-prone. We have developed an object identification system called Active Atlas [18], which applies a set of domain-independent string transformations to compare the objects' shared attributes in order to identify matching objects. In this paper, we discuss extensions to the Active Atlas system, which allow it to learn to tailor the weights of a set of general transformations to a specific application domain through limited user input. The experimental results demonstrate that this approach achieves higher accuracy and requires less user involvement than previous methods across various application domains."
KDD	Handling very large numbers of association rules in the analysis of microarray data.	Alexander Tuzhilin,Gediminas Adomavicius	2002	The problem of analyzing microarray data became one of important topics in bioinformatics over the past several years, and different data mining techniques have been proposed for the analysis of such data. In this paper, we propose to use association rule discovery methods for determining associations among expression levels of different genes. One of the main problems related to the discovery of these associations is the scalability issue. Microarrays usually contain very large numbers of genes that are sometimes measured in 10,000s. Therefore, analysis of such data can generate a very large number of associations that can often be measured in millions. The paper addresses this problem by presenting a method that enables biologists to evaluate these very large numbers of discovered association rules during the post-analysis stage of the data mining process. This is achieved by providing several rule evaluation operators, including rule grouping, filtering, browsing, and data inspection operators, that allow biologists to validate multiple individual gane regulation patterns at a time. By iteratively applying these operators, biologists can explore a significant part of all the initially generated rules in an acceptable period of time and thus answer biological questions that are of a particular interest to him or her. To validate our method, we tested our system on the microarray data pertaining to the studies of environmental hazards and their influence of gane expression processes. As a result, we managed to answer several questions that were of interest to the biologists that had collected this data.
KDD	Querying multiple sets of discovered rules.	Alexander Tuzhilin,Bing Liu	2002	Rule mining is an important data mining task that has been applied to numerous real-world applications. Often a rule mining system generates a large number of rules and only a small subset of them is really useful in applications. Although there exist some systems allowing the user to query the discovered rules, they are less suitable for complex ad hoc querying of multiple data mining rulebases to retrieve interesting rules. In this paper, we propose a new powerful rule query language Rule-QL for querying multiple rulebases that is modeled after SQL and has rigorous theoretical foundations of a rule-based calculus. In particular, we first propose a rule-based calculus RC based on the first-order logic, and then present the language Rule-QL that is at least as expressive as the safe fragment of RC. We also propose a number of efficient query evaluation techniques for Rule-QL and test them experimentally on some representative queries to demonstrate the feasibility of Rule-QL.
KDD	Single-shot detection of multiple categories of text using parametric mixture models.	Naonori Ueda,Kazumi Saito	2002	"In this paper, we address the problem of detecting multiple topics or categories of text where each text is not assumed to belong to one of a number of mutually exclusive categories. Conventionally, the binary classification approach has been employed, in which whether or not text belongs to a category is judged by the binary classifier for every category. In this paper, we propose a more sophisticated approach to simultaneously detect multiple categories of text using parametric mixture models (PMMs), newly presented in this paper. PMMs are probabilistic generative models for text that has multiple categories. Our PMMs are essentially different from the conventional mixture of multinomial distributions in the sense that in the former several basis multinomial parameters are mixed in the parameter space, while in the latter several multinomial components are mixed. We derive efficient learning algorithms for PMMs within the framework of the maximum a posteriori estimate. We also empirically show that our method can outperform the conventional binary approach when applied to multitopic detection of World Wide Web pages, focusing on those from the ""yahoo.com"" domain."
KDD	"What's the code?: automatic classification of source code archives."	Secil Ugurel,Robert Krovetz,C. Lee Giles	2002	There are various source code archives on the World Wide Web. These archives are usually organized by application categories and programming languages. However, manually organizing source code repositories is not a trivial task since they grow rapidly and are very large (on the order of terabytes). We demonstrate machine learning methods for automatic classification of archived source code into eleven application topics and ten programming languages. For topical classification, we concentrate on C and C++ programs from the Ibiblio and the Sourceforge archives. Support vector machine (SVM) classifiers are trained on examples of a given programming language or programs in a specified category. We show that source code can be accurately and automatically classified into topical categories and can be identified to be in a specific programming language class.
KDD	Privacy preserving association rule mining in vertically partitioned data.	Jaideep Vaidya,Chris Clifton	2002	Privacy considerations often constrain data mining projects. This paper addresses the problem of association rule mining where transactions are distributed across sources. Each site holds some attributes of each transaction, and the sites wish to collaborate to identify globally valid association rules. However, the sites must not reveal individual transaction data. We present a two-party algorithm for efficiently discovering frequent itemsets with minimum support levels, without either site revealing individual transaction values.
KDD	Non-linear dimensionality reduction techniques for classification and visualization.	Michail Vlachos,Carlotta Domeniconi,Dimitrios Gunopulos,George Kollios,Nick Koudas	2002	"In this paper we address the issue of using local embeddings for data visualization in two and three dimensions, and for classification. We advocate their use on the basis that they provide an efficient mapping procedure from the original dimension of the data, to a lower intrinsic dimension. We depict how they can accurately capture the user's perception of similarity in high-dimensional data for visualization purposes. Moreover, we exploit the low-dimensional mapping provided by these embeddings, to develop new classification techniques, and we show experimentally that the classification accuracy is comparable (albeit using fewer dimensions) to a number of other classification procedures."
KDD	On interactive visualization of high-dimensional data using the hyperbolic plane.	Jörg A. Walter,Helge Ritter	2002	"We propose a novel projection based visualization method for high-dimensional datasets by combining concepts from MDS and the geometry of the hyperbolic spaces. Our approach Hyperbolic Multi-Dimensional Scaling (H-MDS) extends earlier work [7] using hyperbolic spaces for visualization of tree structures data ( ""hyperbolic tree browser"" ).By borrowing concepts from multi-dimensional scaling we map proximity data directly into the 2-dimensional hyperbolic space (H2). This removes the restriction to ""quasihierarchical"", graph-based data -- limiting previous work. Since a suitable distance function can convert all kinds of data to proximity (or distance-based) data this type of data can be considered the most general.We used the circular Poincar&eacute; model of the H2 which allows effective human-computer interaction: by moving the ""focus"" via mouse the user can navigate in the data without loosing the ""context"". In H2 the ""fish-eye"" behavior originates not simply by a non-linear view transformation but rather by extraordinary, non-Euclidean properties of the H2. Especially, the exponential growth of length and area of the underlying space makes the H2 a prime target for mapping hierarchical and (now also) high-dimensional data.We present several high-dimensional mapping examples including synthetic and real world data and a successful application for unstructured text. By analyzing and integrating multiple film critiques from news:rec.art.movies.reviews and the internet movie database, each movie becomes placed within the H2. Here the idea is, that related films share more words in their reviews than unrelated. Their semantic proximity leads to a closer arrangement. The result is a kind of high-level content structured display allowing the user to explore the ""space of movies""."
KDD	"Item selection by ""hub-authority"" profit ranking."	Ke Wang,Ming-Yen Thomas Su	2002	"A fundamental problem in business and other applications is ranking items with respect to some notion of profit based on historical transactions. The difficulty is that the profit of one item not only comes from its own sales, but also from its influence on the sales of other items, i.e., the ""cross-selling effect"". In this paper, we draw an analogy between this influence and the mutual reinforcement of hub/authority web pages. Based on this analogy, we present a novel approach to the item ranking problem.We apply this ranking approach to solve two selection problems. In size-constrained selection, the maximum number of items that can be selected is fixed. In cost-constrained selection, there is no maximum number of items to be selected, but there is some cost associated with the selection of each item. In both cases, the question is what items should be selected to maximize the profit. Empirically, we show that this method finds profitable items in the presence of cross-selling effect."
KDD	A system for real-time competitive market intelligence.	Sholom M. Weiss,Naval K. Verma	2002	A method is described for real-time market intelligence and competitive analysis. News stories are collected online for a designated group of companies. The goal is to detect critical differences in the text written about a company versus the text for its competitors. A solution is found by mapping the task into a non-stationary text categorization model. The overall design consists of the following components: (a) a real-time crawler that monitors newswires for stories about the competitors (b) a conditional document retriever that selects only those documents that meet the indicated conditions (c) text analysis techniques that convert the documents to a numerical format (d) rule induction methods for finding patterns in data (e) presentation techniques for displaying results. The method is extended to combine text with numerical measures, such as those based on stock prices and market capitalizations, that allow for more objective evaluations and projections.
KDD	A refinement approach to handling model misfit in text categorization.	Haoran Wu,Tong-Heng Phang,Bing Liu,Xiaoli Li	2002	"Text categorization or classification is the automated assigning of text documents to pre-defined classes based on their contents. This problem has been studied in information retrieval, machine learning and data mining. So far, many effective techniques have been proposed. However, most techniques are based on some underlying models and/or assumptions. When the data fits the model well, the classification accuracy will be high. However, when the data does not fit the model well, the classification accuracy can be very low. In this paper, we propose a refinement approach to dealing with this problem of model misfit. We show that we do not need to change the classification technique itself (or its underlying model) to make it more flexible. Instead, we propose to use successive refinements of classification on the training data to correct the model misfit. We apply the proposed technique to improve the classification performance of two simple and efficient text classifiers, the Rocchio classifier and the na&iuml;ve Bayesian classifier. These techniques are suitable for very large text collections because they allow the data to reside on disk and need only one scan of the data to build a text classifier. Extensive experiments on two benchmark document corpora show that the proposed technique is able to improve text categorization accuracy of the two techniques dramatically. In particular, our refined model is able to improve the na&iuml;ve Bayesian or Rocchio classifier's prediction performance by 45% on average."
KDD	A unifying framework for detecting outliers and change points from non-stationary time series data.	Kenji Yamanishi,Jun-ichi Takeuchi	2002	We are concerned with the issues of outlier detection and change point detection from a data stream. In the area of data mining, there have been increased interest in these issues since the former is related to fraud detection, rare event discovery, etc., while the latter is related to event/trend by change detection, activity monitoring, etc. Specifically, it is important to consider the situation where the data source is non-stationary, since the nature of data source may change over time in real applications. Although in most previous work outlier detection and change point detection have not been related explicitly, this paper presents a unifying framework for dealing with both of them on the basis of the theory of on-line learning of non-stationary time series. In this framework a probabilistic model of the data source is incrementally learned using an on-line discounting learning algorithm, which can track the changing data source adaptively by forgetting the effect of past data gradually. Then the score for any given data is calculated to measure its deviation from the learned model, with a higher score indicating a high possibility of being an outlier. Further change points in a data stream are detected by applying this scoring method into a time series of moving averaged losses for prediction using the learned model. Specifically we develop an efficient algorithms for on-line discounting learning of auto-regression models from time series data, and demonstrate the validity of our framework through simulation and experimental applications to stock market data analysis.
KDD	CLOPE: a fast and effective clustering algorithm for transactional data.	Yiling Yang,Xudong Guan,Jinyuan You	2002	This paper studies the problem of categorical data clustering, especially for transactional data characterized by high dimensionality and large volume. Starting from a heuristic method of increasing the height-to-width ratio of the cluster histogram, we develop a novel algorithm -- CLOPE, which is very fast and scalable, while being quite effective. We demonstrate the performance of our algorithm on two real world datasets, and compare CLOPE with the state-of-art algorithms.
KDD	Topic-conditioned novelty detection.	Yiming Yang,Jian Zhang,Jaime G. Carbonell,Chun Jin	2002	Automated detection of the first document reporting each new event in temporally-sequenced streams of documents is an open challenge. In this paper we propose a new approach which addresses this problem in two stages: 1) using a supervised learning algorithm to classify the on-line document stream into pre-defined broad topic categories, and 2) performing topic-conditioned novelty detection for documents in each topic. We also focus on exploiting named-entities for event-level novelty detection and using feature-based heuristics derived from the topic histories. Evaluating these methods using a set of broadcast news stories, our results show substantial performance gains over the traditional one-level approach to the novelty detection problem.
KDD	PEBL: positive example based learning for Web page classification using SVM.	Hwanjo Yu,Jiawei Han,Kevin Chen-Chuan Chang	2002	"Web page classification is one of the essential techniques for Web mining. Specifically, classifying Web pages of a user-interesting class is the first step of mining interesting information from the Web. However, constructing a classifier for an interesting class requires laborious pre-processing such as collecting positive and negative training examples. For instance, in order to construct a ""homepage"" classifier, one needs to collect a sample of homepages (positive examples) and a sample of non-homepages (negative examples). In particular, collecting negative training examples requires arduous work and special caution to avoid biasing them. We introduce in this paper the Positive Example Based Learning (PEBL) framework for Web page classification which eliminates the need for manually collecting negative training examples in pre-processing. We present an algorithm called Mapping-Convergence (M-C) that achieves classification accuracy (with positive and unlabeled data) as high as that of traditional SVM (with positive and negative data). Our experiments show that when the M-C algorithm uses the same amount of positive examples as that of traditional SVM, the M-C algorithm performs as well as traditional SVM."
KDD	Transforming classifier scores into accurate multiclass probability estimates.	Bianca Zadrozny,Charles Elkan	2002	Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making, such as example-dependent misclassification costs, the outputs of other classifiers, or domain knowledge. Previous calibration methods apply only to two-class problems. Here, we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates. We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples. Using naive Bayes and support vector machine classifiers, we give experimental results from a variety of two-class and multiclass domains, including direct marketing, text categorization and digit recognition.
KDD	Efficiently mining frequent trees in a forest.	Mohammed Javeed Zaki	2002	Mining frequent trees is very useful in domains like bioinformatics, web mining, mining semistructured data, and so on. We formulate the problem of mining (embedded) subtrees in a forest of rooted, labeled, and ordered trees. We present TREEMINER, a novel algorithm to discover all frequent subtrees in a forest, using a new data structure called scope-list. We contrast TREEMINER with a pattern matching tree mining algorithm (PATTERNMATCHER). We conduct detailed experiments to test the performance and scalability of these methods. We find that TREEMINER outperforms the pattern matching approach by a factor of 4 to 20, and has good scaleup properties. We also present an application of tree mining to analyze real web logs for usage patterns.
KDD	Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 23-26, 2002, Edmonton, Alberta, Canada		2002	Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 23-26, 2002, Edmonton, Alberta, Canada
PKDD	Unsupervised Learning: Self-aggregation in Scaled Principal Component Space.	Chris H. Q. Ding,Xiaofeng He,Hongyuan Zha,Horst D. Simon	2002	Unsupervised Learning: Self-aggregation in Scaled Principal Component Space.
PKDD	Optimized Substructure Discovery for Semi-structured Data.	Kenji Abe,Shinji Kawasoe,Tatsuya Asai,Hiroki Arimura,Setsuo Arikawa	2002	Optimized Substructure Discovery for Semi-structured Data.
PKDD	Mining All Non-derivable Frequent Itemsets.	Toon Calders,Bart Goethals	2002	Mining All Non-derivable Frequent Itemsets.
PKDD	Fast Outlier Detection in High Dimensional Spaces.	Fabrizio Angiulli,Clara Pizzuti	2002	Fast Outlier Detection in High Dimensional Spaces.
PKDD	Data Mining in Schizophrenia Research - Preliminary Analysis.	Stefan Arnborg,Ingrid Agartz,Håkan Hall,Erik Jönsson,Anna Sillén,Göran Sedvall	2002	Data Mining in Schizophrenia Research - Preliminary Analysis.
PKDD	Fast Algorithms for Mining Emerging Patterns.	James Bailey,Thomas Manoukian,Kotagiri Ramamohanarao	2002	Fast Algorithms for Mining Emerging Patterns.
PKDD	On the Discovery of Weak Periodicities in Large Time Series.	Christos Berberidis,Ioannis P. Vlahavas,Walid G. Aref,Mikhail J. Atallah,Ahmed K. Elmagarmid	2002	On the Discovery of Weak Periodicities in Large Time Series.
PKDD	Iterative Data Squashing for Boosting Based on a Distribution-Sensitive Distance.	Yuta Choki,Einoshin Suzuki	2002	Iterative Data Squashing for Boosting Based on a Distribution-Sensitive Distance.
PKDD	Finding Association Rules with Some Very Frequent Attributes.	Frans Coenen,Paul H. Leng	2002	Finding Association Rules with Some Very Frequent Attributes.
PKDD	The Need for Low Bias Algorithms in Classification Learning from Large Data Sets.	Damien Brain,Geoffrey I. Webb	2002	The Need for Low Bias Algorithms in Classification Learning from Large Data Sets.
PKDD	A Classification Approach for Prediction of Target Events in Temporal Sequences.	Carlotta Domeniconi,Chang-Shing Perng,Ricardo Vilalta,Sheng Ma	2002	A Classification Approach for Prediction of Target Events in Temporal Sequences.
PKDD	Privacy-Oriented Data Mining by Proof Checking.	Amy P. Felty,Stan Matwin	2002	Privacy-Oriented Data Mining by Proof Checking.
PKDD	Multiscale Comparison of Temporal Patternsin Time-Series Medical Databases.	Shoji Hirano,Shusaku Tsumoto	2002	Multiscale Comparison of Temporal Patternsin Time-Series Medical Databases.
PKDD	Choose Your Words Carefully: An Empirical Study of Feature Selection Metrics for Text Classification.	George Forman	2002	Choose Your Words Carefully: An Empirical Study of Feature Selection Metrics for Text Classification.
PKDD	Association Rules for Expressing Gradual Dependencies.	Eyke Hüllermeier	2002	Association Rules for Expressing Gradual Dependencies.
PKDD	Generating Actionable Knowledge by Expert-Guided Subgroup Discovery.	Dragan Gamberger,Nada Lavrac	2002	Generating Actionable Knowledge by Expert-Guided Subgroup Discovery.
PKDD	Clustering Transactional Data.	Fosca Giannotti,Cristian Gozzi,Giuseppe Manco	2002	Clustering Transactional Data.
PKDD	Support Approximations Using Bonferroni-Type Inequalities.	Szymon Jaroszewicz,Dan A. Simovici	2002	Support Approximations Using Bonferroni-Type Inequalities.
PKDD	Using Condensed Representations for Interactive Association Rule Mining.	Baptiste Jeudy,Jean-François Boulicaut	2002	Using Condensed Representations for Interactive Association Rule Mining.
PKDD	Predicting Rare Classes: Comparing Two-Phase Rule Induction to Cost-Sensitive Boosting.	Mahesh V. Joshi,Ramesh C. Agarwal,Vipin Kumar	2002	Predicting Rare Classes: Comparing Two-Phase Rule Induction to Cost-Sensitive Boosting.
PKDD	Dependency Detection in MobiMine and Random Matrices.	Hillol Kargupta,Krishnamoorthy Sivakumar,Samiran Ghosh	2002	Dependency Detection in MobiMine and Random Matrices.
PKDD	Long-Term Learning for Web Search Engines.	Charles Kemp,Kotagiri Ramamohanarao	2002	Long-Term Learning for Web Search Engines.
PKDD	Geography of Differences between Two Classes of Data.	Jinyan Li,Limsoon Wong	2002	Geography of Differences between Two Classes of Data.
PKDD	Clustering Ontology-Based Metadata in the Semantic Web.	Alexander Maedche,Valentin Zacharias	2002	Clustering Ontology-Based Metadata in the Semantic Web.
PKDD	Spatial Subgroup Mining Integrated in an Object-Relational Spatial Database.	Willi Klösgen,Michael May	2002	Spatial Subgroup Mining Integrated in an Object-Relational Spatial Database.
PKDD	Involving Aggregate Functions in Multi-relational Search.	Arno J. Knobbe,Arno Siebes,Bart Marseille	2002	Involving Aggregate Functions in Multi-relational Search.
PKDD	Iteratively Selecting Feature Subsets for Mining from High-Dimensional Databases.	Hiroshi Mamitsuka	2002	Iteratively Selecting Feature Subsets for Mining from High-Dimensional Databases.
PKDD	Information Extraction in Structured Documents Using Tree Automata Induction.	Raymond Kosala,Jan Van den Bussche,Maurice Bruynooghe,Hendrik Blockeel	2002	Information Extraction in Structured Documents Using Tree Automata Induction.
PKDD	Algebraic Techniques for Analysis of Large Discrete-Valued Datasets.	Mehmet Koyutürk,Ananth Grama,Naren Ramakrishnan	2002	Algebraic Techniques for Analysis of Large Discrete-Valued Datasets.
PKDD	Finding Hidden Factors UsingIndependent Component Analysis.	Erkki Oja	2002	Finding Hidden Factors UsingIndependent Component Analysis.
PKDD	Rule Induction for Classification of Gene Expression Array Data.	Per Lidén,Lars Asker,Henrik Boström	2002	Rule Induction for Classification of Gene Expression Array Data.
PKDD	SVM Classification Using Sequences of Phonemes and Syllables.	Gerhard Paass,Edda Leopold,Martha Larson,Jörg Kindermann,Stefan Eickeler	2002	SVM Classification Using Sequences of Phonemes and Syllables.
PKDD	A Novel Web Text Mining Method Using the Discrete Cosine Transform.	Laurence A. F. Park,Marimuthu Palaniswami,Kotagiri Ramamohanarao	2002	A Novel Web Text Mining Method Using the Discrete Cosine Transform.
PKDD	Reasoning with Classifiers.	Dan Roth	2002	Reasoning with Classifiers.
PKDD	Mining Hierarchical Decision Rules from Clinical Databases Using Rough Sets aaand Medical Diagnostic Model.	Shusaku Tsumoto	2002	Mining Hierarchical Decision Rules from Clinical Databases Using Rough Sets aaand Medical Diagnostic Model.
PKDD	A Scalable Constant-Memory Sampling Algorithm for Pattern Discovery in Large Databases.	Tobias Scheffer,Stefan Wrobel	2002	A Scalable Constant-Memory Sampling Algorithm for Pattern Discovery in Large Databases.
PKDD	A Kernel Approach for Learning from Almost Orthogonal Patterns.	Bernhard Schölkopf,Jason Weston,Eleazar Eskin,Christina S. Leslie,William Stafford Noble	2002	A Kernel Approach for Learning from Almost Orthogonal Patterns.
PKDD	Efficiently Mining Approximate Models of Associations in Evolving Databases.	Adriano Veloso,Bruno Gusmão Rocha,Wagner Meira Jr.,Márcio de Carvalho,Srinivasan Parthasarathy,Mohammed Javeed Zaki	2002	Efficiently Mining Approximate Models of Associations in Evolving Databases.
PKDD	Answering the Most Correlated N Association Rules Efficiently.	Jun Sese,Shinichi Morishita	2002	Answering the Most Correlated N Association Rules Efficiently.
PKDD	Explaining Predictions from a Neural Network Ensemble One at a Time.	Robert Wall,Padraig Cunningham,Paul Walsh	2002	Explaining Predictions from a Neural Network Ensemble One at a Time.
PKDD	Learning with Mixture Models: Concepts and Applications.	Padhraic Smyth	2002	Learning with Mixture Models: Concepts and Applications.
PKDD	Structuring Domain-Specific Text Archives by Deriving a Probabilistic XML DTD.	Karsten Winkler,Myra Spiliopoulou	2002	Structuring Domain-Specific Text Archives by Deriving a Probabilistic XML DTD.
PKDD	Separability Index in Supervised Learning.	Djamel A. Zighed,Stéphane Lallich,Fabrice Muhlenbach	2002	Separability Index in Supervised Learning.
PKDD	Principles of Data Mining and Knowledge Discovery, 6th European Conference, PKDD 2002, Helsinki, Finland, August 19-23, 2002, Proceedings	Tapio Elomaa,Heikki Mannila,Hannu Toivonen	2002	Principles of Data Mining and Knowledge Discovery, 6th European Conference, PKDD 2002, Helsinki, Finland, August 19-23, 2002, Proceedings
SDM	Efficient Local Flexible Nearest Neighbor Classification.	Carlotta Domeniconi,Dimitrios Gunopulos	2002	Efficient Local Flexible Nearest Neighbor Classification.
SDM	Approximate Splitting for Ensembles of Trees using Histograms.	Chandrika Kamath,Erick Cantú-Paz,David Littau	2002	Approximate Splitting for Ensembles of Trees using Histograms.
SDM	MedMeSH Summarizer: Text Mining for Gene Clusters.	Pankaj Kankar,Sudeshna Adak,A. Sarkar,K. Murali,Gaurav Sharma	2002	MedMeSH Summarizer: Text Mining for Gene Clusters.
SDM	Explicit Thermodynamic Properties using Radial Basis Functions Neural Networks.	Olivier Adam,Olivier Léonard	2002	Explicit Thermodynamic Properties using Radial Basis Functions Neural Networks.
SDM	Ensemble-based Adaptive Intrusion Detection.	Wei Fan,Salvatore J. Stolfo	2002	Ensemble-based Adaptive Intrusion Detection.
SDM	A Framework for Scalable Cost-sensitive Learning Based on Combing Probabilities and Benefits.	Wei Fan,Haixun Wang,Philip S. Yu,Salvatore J. Stolfo	2002	A Framework for Scalable Cost-sensitive Learning Based on Combing Probabilities and Benefits.
SDM	Iterative Deepening Dynamic Time Warping for Time Series.	Selina Chu,Eamonn J. Keogh,David Hart,Michael J. Pazzani	2002	Iterative Deepening Dynamic Time Warping for Time Series.
SDM	A Data Parallel Approach for Large-Scale Gaussian Process Modeling.	Arindam Choudhury,Prasanth B. Nair,Andy J. Keane	2002	A Data Parallel Approach for Large-Scale Gaussian Process Modeling.
SDM	Incremental Support Vector Machine Classification.	Glenn Fung,Olvi L. Mangasarian	2002	Incremental Support Vector Machine Classification.
SDM	A Clustering Technique for Mining Data from Text Tables.	Hasan Davulcu,Saikat Mukherjee,I. V. Ramakrishnan	2002	A Clustering Technique for Mining Data from Text Tables.
SDM	Efficient Substructure Discovery from Large Semi-structured Data.	Tatsuya Asai,Kenji Abe,Shinji Kawasoe,Hiroki Arimura,Hiroshi Sakamoto,Setsuo Arikawa	2002	Efficient Substructure Discovery from Large Semi-structured Data.
SDM	On Scaling Up Balanced Clustering Algorithms.	Arindam Banerjee,Joydeep Ghosh	2002	On Scaling Up Balanced Clustering Algorithms.
SDM	Learning Simple Relations: Theory and Applications.	Pavel Berkhin,Jonathan D. Becher	2002	Learning Simple Relations: Theory and Applications.
SDM	Extracting Precursor Rules from Time SeriesA Classical Statistical Viewpoint.	João B. D. Cabrera,Raman K. Mehra	2002	Extracting Precursor Rules from Time SeriesA Classical Statistical Viewpoint.
SDM	The Power of Second-Order Decision Tables.	Rattikorn Hewett,John H. Leuchner	2002	The Power of Second-Order Decision Tables.
SDM	Discovering Frequent Substructures from Hierarchical Semi-structured Data.	Gao Cong,Lan Yi,Bing Liu,Ke Wang	2002	Discovering Frequent Substructures from Hierarchical Semi-structured Data.
SDM	Shared Memory Paraellization of Data Mining Algorithms: Techniques, Programming Interface, and Performance.	Ruoming Jin,Gagan Agrawal	2002	Shared Memory Paraellization of Data Mining Algorithms: Techniques, Programming Interface, and Performance.
SDM	Visualizing Clustering Results.	Ian Davidson	2002	Visualizing Clustering Results.
SDM	Cluster Selection in Divisive Clustering Algorithms.	Sergio M. Savaresi,Daniel Boley,Sergio Bittanti,Giovanna Gazzaniga	2002	Cluster Selection in Divisive Clustering Algorithms.
SDM	Mining Relationship between Triggering and Consequential Events in a Short Transaction Database.	Chang-Hung Lee,Philip S. Yu,Ming-Syan Chen	2002	Mining Relationship between Triggering and Consequential Events in a Short Transaction Database.
SDM	Discovering Fully Dependent Patterns.	Feng Liang,Sheng Ma,Joseph L. Hellerstein	2002	Discovering Fully Dependent Patterns.
SDM	On the Optimal Clustering of Sequential Data.	Cheng-Ru Lin,Ming-Syan Chen	2002	On the Optimal Clustering of Sequential Data.
SDM	Collusion in the U. S. Crop Insurance Program: Applied Data Mining.	Bertis B. Little,Walter L. Johnston,Ashley C. Lovell,Roderick M. Rejesus,Steve A. Steed	2002	"This paper quantitatively analyzes indicators of Agent (policy seller), Adjuster (indemnity claim adjuster), Producer (policy purchaser/holder) indemnity behavior suggestive of collusion in the United States Department of Agriculture (USDA) Risk Management Agency (RMA) national crop insurance program. According to guidance from the federal law and using six indicator variables of indemnity behavior, those entities equal to or exceeding 150% of the county mean (computed using a simple jackknife procedure) on all entity-relevant indicators were flagged as ""anomalous."" Log linear analysis was used to test (I) hierarchical node-node arrangements and (2) a non-recursive model of node information sharing. Chi-square distributed deviance statistic identified the optimal log linear model. The results of the applied data mining technique used here suggest that the non-recursive triplet and Agent-producer doublet collusion probabilistically accounts for the greatest proportion of waste, fraud, and abuse in the federal crop insurance program. Triplet and Agent-producer doublets need detailed investigation for possible collusion. Hence, this data mining technique provided a high level of confidence when 24 million records were quantitatively analyzed for possible fraud, waste, or other abuse of the crop insurance program administered by the USDA RMA, and suspect entities reported to USDA. This data mining technique can be applied where vast amounts of data are available to detect patterns of collusion or conspiracy as may be of interest to the criminal justice or intelligence agencies."
SDM	Mining Frequent Itemsets in Evolving Databases.	Adriano Veloso,Wagner Meira Jr.,Márcio de Carvalho,Bruno Pôssas,Srinivasan Parthasarathy,Mohammed Javeed Zaki	2002	Mining Frequent Itemsets in Evolving Databases.
SDM	Autoregressive Tree Models for Time-Series Analysis.	Christopher Meek,David Maxwell Chickering,David Heckerman	2002	Autoregressive Tree Models for Time-Series Analysis.
SDM	Evaluating the Performance of Association Mining Methods in 3-D Medical Image Databases.	Vasileios Megalooikonomou	2002	Evaluating the Performance of Association Mining Methods in 3-D Medical Image Databases.
SDM	A Pattern Search Method for Model Selection of Support Vector Regression.	Michinari Momma,Kristin P. Bennett	2002	A Pattern Search Method for Model Selection of Support Vector Regression.
SDM	One Step Evolutionary Mining of Context Sensitive Associations and Web Navigation Patterns.	Olfa Nasraoui,Raghu Krishnapuram	2002	One Step Evolutionary Mining of Context Sensitive Associations and Web Navigation Patterns.
SDM	Segmented Regression Estimators for Massive Data Sets.	Ramesh Natarajan,Edwin P. D. Pednault	2002	Segmented Regression Estimators for Massive Data Sets.
SDM	Efficient Filtering of Large DatasetA User-Centric Paradigm.	Yi Xia,Wei Wang,Jiong Yang,Philip S. Yu,Richard R. Muntz	2002	Efficient Filtering of Large DatasetA User-Centric Paradigm.
SDM	Instance Selection Techniques for Memory-based Collaborative Filtering.	Kai Yu,Xiaowei Xu,Jianjua Tao,Martin Ester,Hans-Peter Kriegel	2002	Instance Selection Techniques for Memory-based Collaborative Filtering.
SDM	CHARM: An Efficient Algorithm for Closed Itemset Mining.	Mohammed Javeed Zaki,Ching-Jiu Hsiao	2002	CHARM: An Efficient Algorithm for Closed Itemset Mining.
SDM	VizCluster: An Interactive Visualization Approach to Cluster Analysis and Its Application on Microarray Data.	Li Zhang,Chun Tang,Yong Shi,Yuqing Song,Aidong Zhang,Murali Ramanathan	2002	VizCluster: An Interactive Visualization Approach to Cluster Analysis and Its Application on Microarray Data.
SDM	Why the Information Explosion Can Be Bad for Data Mining, and How Data Fusion Provides a Way Out.	Peter van der Putten,Joost N. Kok,Amar Gupta	2002	Why the Information Explosion Can Be Bad for Data Mining, and How Data Fusion Provides a Way Out.
SDM	Proceedings of the Second SIAM International Conference on Data Mining, Arlington, VA, USA, April 11-13, 2002	Robert L. Grossman,Jiawei Han,Vipin Kumar,Heikki Mannila,Rajeev Motwani	2002	Proceedings of the Second SIAM International Conference on Data Mining, Arlington, VA, USA, April 11-13, 2002
ICDM	"Links Between Kleinberg's Hubs and Authorities, Correspondence Analysis, and Markov Chains."	François Fouss,Marco Saerens,Jean-Michel Renders	2003	"In this work, we show that Kleinberg's hubs and authoritiesmodel is closely related to both correspondence analysis,a well-known multivariate statistical technique, and aparticular Markov chain model of navigation through theweb. The only difference between correspondence analysisand Kleinberg's method is the use of the average value ofthe hubs (authorities) scores for computing the authorities(hubs) scores, instead of the sum for Kleinberg's method.We also show that correspondence analysis and our Markovmodel are related to SALSA, a variant of Kleinberg's model."
ICDM	Mining High Utility Itemsets.	Raymond Chan,Qiang Yang,Yi-Dong Shen	2003	Traditional association rule mining algorithms onlygenerate a large number of highly frequent rules, butthese rules do not provide useful answers for what thehigh utility rules are. In this work, we develop a novelidea of top-K objective-directed data mining, which focuseson mining the top-K high utility closed patterns thatdirectly support a given business objective. To associationmining, we add the concept of utility to capture highly desirablestatistical patterns and present a level-wise item-setmining algorithm. With both positive and negativeutilities, the anti-monotone pruning strategy in Apriorialgorithm no longer holds. In response, we develop a newpruning strategy based on utilities that allow pruning oflow utility itemsets to be done by means of a weaker butanti-monotonic condition. Our experimental results showthat our algorithm does not require a user specifiedminimum utility and hence is effective in practice.
ICDM	Indexing and Mining Free Trees.	Yun Chi,Yirong Yang,Richard R. Muntz	2003	Tree structures are used extensively in domains such ascomputational biology, pattern recognition, computer networks,and so on. In this paper, we present an indexing techniquefor free trees and apply this indexing technique to theproblem of mining frequent subtrees. We first define a novelrepresentation, the canonical form, for rooted trees and extendthe definition to free trees. We also introduce anotherconcept, the canonical string, as a simpler representationfor free trees in their canonical forms. We then apply ourtree indexing technique to the frequent subtree mining problemand present FreeTreeMiner, a computationally efficientalgorithm that discovers all frequently occurring subtreesin a database of free trees. We study the performance andthe scalability of our algorithms through extensive experimentsbased on both synthetic data and datasets from tworeal applications: a dataset of chemical compounds and adataset of Internet multicast trees.
ICDM	Integrating Fuzziness into OLAP for Multidimensional Fuzzy Association Rules Mining.	Reda Alhajj,Mehmet Kaya	2003	This paper contributes to the ongoing research onmultidimensional online association rules mining byproposing a general architecture that utilizes a fuzzy datacube for knowledge discovery. Three different methods areintroduced to mine fuzzy association rules in the constructedfuzzy data cube, namely single dimension, multidimensionaland hybrid association rules mining. Experimental resultsobtained for each of the three methods on the adult data ofthe United States census in 2000 show their effectiveness andapplicability.
ICDM	Efficient Multidimensional Quantitative Hypotheses Generation.	Amihood Amir,Reuven Kashi,Nathan S. Netanyahu	2003	"Finding local interrelations (hypotheses) among attributeswithin very large databases of high dimensionalityis an acute problem for many databases and data miningapplications. These include, dependency modeling, clusteringlarge databases, correlation and link analysis.Traditional statistical methods are concerned with the corroborationof (a set of) hypotheses on a given body ofdata. Testing all of the hypotheses that can be generatedfrom a database with millions of records and dozens offields is clearly infeasible. Generating, on the other hand,a set of the most ""promising"" hypotheses (to be corroborated)requires much intuition and ingenuity.In this paper we present an efficient method for rankingthe multidimensional hypotheses using image processingof data visualization. In the heart of the method lies theuse of visualization techniques and image processing ideasto rank subsets of attributes according to the relation betweenthem in the databases. Some of the scalability issuesare solved by concise generalized histograms and by usingan efficient on-line computation of clustering around amedian with only five additional memory words. In additionto presenting our algorithmic methodology, we demonstrateits efficiency and performance by applying it to realcensus data sets, as well as synthetic data sets."
ICDM	Analyzing High-Dimensional Data by Subspace Validity.	Amihood Amir,Reuven Kashi,Nathan S. Netanyahu,Daniel A. Keim,Markus Wawryniuk	2003	We are proposing a novel method that makes it possibleto analyze high dimensional data with arbitrary shapedprojected clusters and high noise levels. At the core of ourmethod lies the idea of subspace validity. We map the datain a way that allows us to test the quality of subspaces usingstatistical tests. Experimental results, both on synthetic andreal data sets, demonstrate the potential of our method.
ICDM	Objective and Subjective Algorithms for Grouping Association Rules.	Aijun An,Shakil M. Khan,Xiangji Huang	2003	We propose two algorithms for grouping and summarizingassociation rules. The first algorithm recursively groupsrules according to the structure of the rules and generatesa tree of clusters as a result. The second algorithm groupsthe rules according to the semantic distance between therules by making use of an autometically tagged semantictree-structured network of items. We provide a case study inwhich the proposed algorithms are evaluated. The resultsshow that our grouping methods are effective and producegood grouping results.
ICDM	Fast PNN-based Clustering Using K-nearest Neighbor Graph.	Pasi Fränti,Olli Virmajoki,Ville Hautamäki	2003	Search for nearest neighbor is the main source ofcomputation in most clustering algorithms. We proposethe use of nearest neighbor graph for reducing thenumber of candidates. The number of distancecalculations per search can be reduced from O(N) to O(k)where N is the number of clusters, and k is the number ofneighbors in the graph. We apply the proposed schemewithin agglomerative clustering algorithm known as thePNN algorithm.
ICDM	T-Trees, Vertical Partitioning and Distributed Association Rule Mining.	Frans Coenen,Paul H. Leng,Shakil Ahmed	2003	In this paper we consider a technique (DATA-VP) fordistributed (and parallel) Association Rule Mining thatmakes use of a vertical partitioning technique to distributethe input data amongst processors. The proposed verticalpartitioning is facilitated by a novel compressed set enumerationtree data structure (the T-tree), and an associatedmining algorithm (Apriori-T), that allows for computationallyeffective distributed/parallel ARM when compared withexisting approaches.
ICDM	Efficient Subsequence Matching in Time Series Databases Under Time and Amplitude Transformations.	Tassos Argyros,Charis Ermopoulos	2003	Subsequence matching in large time series databases hasattracted a lot of interest and many methods have been proposedthat cope with this problem in an adequate extend.However, locating subsequence matches of arbitrary length,under time and amplitude transformations, has received farless attention and is still an open problem. In this paperwe present an efficient algorithm for variable-length subsequencematching under transformations that guaranteesno false dismissals. Further, this algorithm uses a novelsimilarity criterion for determining similarity under amplitudetransformations in a most efficient way. Finally, ouralgorithm has been tested in various experiments on realdata, resulting in a running time improvement of one orderof magnitude compared to the naive approach.
ICDM	Identifying Markov Blankets with Decision Tree Induction.	Lewis Frey,Douglas H. Fisher,Ioannis Tsamardinos,Constantin F. Aliferis,Alexander R. Statnikov	2003	"The Markov Blanket of a target variable is theminimum conditioning set of variables that makes thetarget independent of all other variables. MarkovBlankets inform feature selection, aid in causal discoveryand serve as a basis for scalable methods of constructingBayesian networks. This paper applies decision treeinduction to the task of Markov Blanket identification.Notably, we compare (a) C5.0, a widely used algorithmfor decision rule induction, (b) C5C, which post-processesC5.0's rule set to retain the most frequentlyreferenced variables and (c) PC, a standard method forBayesian Network induction. C5C performs as well as orbetter than C5.0 and PC across a number of data sets.Our modest variation of an inexpensive, accurate, off-the-shelfinduction engine mitigates the need for specializedprocedures, and establishes baseline performance againstwhich specialized algorithms can be compared."
ICDM	A Fast Algorithm for Computing Hypergraph Transversals and its Application in Mining Emerging Patterns.	James Bailey,Thomas Manoukian,Kotagiri Ramamohanarao	2003	Computing the minimal transversals of a hypergraph isan important problem in computer science that has significantapplications in data mining. In this paper, we present anew algorithm for computing hypergraph transversals andhighlight their close connection to an important class ofpatterns known as emerging patterns. We evaluate our techniqueon a number of large datasets and show that it out-performsprevious approaches by a factor of 9-29 times.
ICDM	Mining Relevant Text from Unlabelled Documents.	Daniel Barbará,Carlotta Domeniconi,Ning Kang	2003	Automatic classification of documents is an importantarea of research with many applications in the fields of documentsearching, forensics and others. Methods to performclassification of text rely on the existence of a sample of documentswhose class labels are known. However, in manysituations, obtaining this sample may not be an easy (oreven possible) task. In this paper we focus on the classificationof unlabelled documents into two classes: relevant andirrelevant, given a topic of interest. By dividing the set ofdocuments into buckets (for instance, answers returned bydifferent search engines), and using association rule miningto find common sets of words among the buckets, we can efficientlyobtain a sample of documents that has a large percentageof relevant ones. This sample can be used to trainmodels to classify the entire set of documents. We prove, viaexperimentation, that our method is capable of filtering relevantdocuments even in adverse conditions where the percentageof irrelevant documents in the buckets is relativelyhigh.
ICDM	Towards Simple, Easy-to-Understand, yet Accurate Classifiers.	Doina Caragea,Dianne Cook,Vasant Honavar	2003	We design a method for weighting linear support vectormachine classifiers or random hyperplanes, to obtain classifierswhose accuracy is comparable to the accuracy of anon-linear support vector machine classifier, and whose resultscan be readily visualized. We conduct a simulationstudy to examine how our weighted linear classifiers behavein the presence of known structure. The results show thatthe weighted linear classifiers might perform well comparedto the non-linear support vector machine classifiers, whilethey are more readily interpretable than the non-linear classifiers.
ICDM	Findings from a Practical Project Concerning Web Usage Mining.	Frank Dellmann,Holger Wulff,Stefan Schmitz	2003	In a practical project a statistical analysis of the Weblog files of the domain www.volkswagen.de was carriedout by using the CRISP-DM procedure. For the preprocessingphase, more profound findings could be gainedthan are usually described in many studies. Since the aimwas to deduce significant statements while measuring theeffect, tests of significance for e-metrics were used inaddition to the commonly described procedure.
ICDM	Frequent Sub-Structure-Based Approaches for Classifying Chemical Compounds.	Mukund Deshpande,Michihiro Kuramochi,George Karypis	2003	In this paper we study the problem of classifying chemical compounddatasets. We present a sub-structure-based classificationalgorithm that decouples the sub-structure discovery processfrom the classification model construction and uses frequentsubgraph discovery algorithms to find all topological and geometricsub-structures present in the dataset. The advantage ofour approach is that during classification model construction, allrelevant sub-structures are available allowing the classifier tointelligently select the most discriminating ones. The computationalscalability is ensured by the use of highly efficient frequentsubgraph discovery algorithms coupled with aggressive featureselection. Our experimental evaluation on eight different classificationproblems shows that our approach is computationallyscalable and on the average, outperforms existing schemes by10% to 35%.
ICDM	Information Theoretic Clustering of Sparse Co-Occurrence Data.	Inderjit S. Dhillon,Yuqiang Guan	2003	"A novel approach to clustering co-occurrence data posesit as an optimization problem in information theory whichminimizes the resulting loss in mutual information. A divisiveclustering algorithm that monotonically reduces thisloss function was recently proposed. In this paper we showthat sparse high-dimensional data presents special challengeswhich can result in the algorithm getting stuck atpoor local minima. We propose two solutions to this problem:(a) a ""prior"" to overcome infinite relative entropy valuesas in the supervised Naive Bayes algorithm, and (b)local search to escape local minima. Finally, we combinethese solutions to get a robust algorithm that is computationallyefficient. We present experimental results to showthat the proposed method is effective in clustering documentcollections and outperforms previous information-theoreticclustering approaches."
ICDM	A User-driven and Quality-oriented Visualization for Mining Association Rules.	Julien Blanchard,Fabrice Guillet,Henri Briand	2003	On account of the enormous amounts of rules that canbe produced by data mining algorithms, knowledgevalidation is one of the most problematic steps in anassociation rule discovery process.In order to findrelevant knowledge for decision-making, the user needs toreally rummage through the rules.Visualization can bevery beneficial to support him/her in this task byimproving the intelligibility of the large rule sets andenabling the user to navigate inside them.In this article,we propose to answer the association rule validationproblem by designing a human-centered visualizationmethod for the rule rummaging task.This new approachbased on a specific rummaging model relies on ruleinterestingness measures and on interactive rule subsetfocusing and mining.We have implemented ourrepresentation by developing a first experimentalprototype called ARVis.
ICDM	ExAMiner: Optimized Level-wise Frequent Pattern Mining with Monotone Constraint.	Francesco Bonchi,Fosca Giannotti,Alessio Mazzanti,Dino Pedreschi	2003	The key point of this paper is that, in frequent patternmining, the most appropriate way of exploiting monotoneconstraints in conjunction with frequency is to use them inorder to reduce the problem input together with the searchspace. Following this intuition, we introduce ExAMiner, alevel-wise algorithm which exploits the real synergy of anti-monotoneand monotone constraints: the total benefit isgreater than the sum of the two individual benefits. ExAMinergeneralizes the basic idea of the preprocessing algorithmExAnte, embedding such ideas at all levels ofan Apriori-like computation. The resulting algorithm is thegeneralization of the Apriori algorithm when a conjunctionof monotone constraints is conjoined to the frequency anti-monotoneconstraint. Experimental results confirm that thisis, so far, the most efficient way of attacking the computationalproblem in analysis.
ICDM	Ensembles of Cascading Trees.	Jinyan Li,Huiqing Liu	2003	We introduce a new method, called CS4, to constructcommittees of decision trees for classification. The methodconsiders different top-ranked features as the root nodes ofmember trees. This idea is particularly suitable for dealingwith high-dimensional bio-medical data as top-ranked featuresin this type of data usually possess similar merits forclassification. To make a decision, the committee combinesthe power of individual trees in a weighted manner. UnlikeBagging or Boosting which uses bootstrapped trainingdata, our method builds all the member trees of a committeeusing exactly the same set of training data. We have testedthese ideas on UCI data sets as well as recent bio-medicaldata sets of gene expression or proteomic profiles that areusually described by more than 10,000 features. All the experimentalresults show that our method is efficient and thatthe classification performance are superior to C4.5 familyalgorithms.
ICDM	Optimized Disjunctive Association Rules via Sampling.	Joseph Elble,Cinda Heeren,Leonard Pitt	2003	The problem of finding optimized support associationrules for a single numerical attribute, where the optimizedregion is a union of k disjoint intervals from the range ofthe attribute, is investigated. The first polynomial timealgorithm for the problem of finding such a region maximizingsupport and meeting a minimum cumulative confidencethreshold is given. Because the algorithm is notpractical, an ostensibly easier, more constrained versionof the problem is considered. Experiments demonstratethat the best extant algorithm for the constrained versionhas significant performance degradation on both a syntheticmodel of patterned data and on real world data sets.Running the algorithm on a small random sample is proposedas a means of obtaining near optimal results withhigh probability. Theoretical bounds on sufficient samplesize to achieve a given performance level are proved, andrapid convergence on synthetic and real-world data is validatedexperimentally.
ICDM	Icon-based Visualization of Large High-Dimensional Datasets.	Ping Chen,Chenyi Hu,Wei Ding,Heloise Lynn,Yves Simon	2003	"High dimensional data visualization is critical todata analysts since it gives a direct view of originaldata. We present a method to visualize large amount ofhigh dimensional data. We divide dimensions of datainto several groups. Then, we use one icon to represent each group, and associate visual properties of eachicon with dimensions in each group. A high dimensional data record will be represented by multiple different types of icons located in the same position. Furthermore, we use summary icons to display local detailsof viewer's interests and the whole data set at meantime. We show its effectiveness and efficiency through a case study on a real large data set."
ICDM	Is random model better? On its accuracy and efficiency.	Wei Fan,Haixun Wang,Philip S. Yu,Sheng Ma	2003	Inductive learning searches an optimal hypothesis thatminimizes a given loss function. It is usually assumed thatthe simplest hypothesis that fits the data is the best approximateto an optimal hypothesis. Since finding the simplesthypothesis is NP-hard for most representations, we generallyemploy various heuristics to search its closest match.Computing these heuristics incurs significant cost, makinglearning inefficient and unscalable for large dataset. In thesame time, it is still questionable if the simplest hypothesisis indeed the closest approximate to the optimal model.Recent success of combining multiple models, such as bagging,boosting and meta-learning, has greatly improved theaccuracy of the simplest hypothesis, providing a strong argumentagainst the optimality of the simplest hypothesis.However, computing these combined hypotheses incurs significantlyhigher cost. In this paper, we first advert that aslong as the error of a hypothesis on each example is withina range dictated by a given loss function, it can still be optimal.Contrary to common beliefs, we propose a completelyrandom decision tree algorithm that achieves much higheraccuracy than the single best hypothesis and is comparableto boosted or bagged multiple best hypotheses. The advantageof multiple random tree is its training efficiency aswell as minimal memory requirement.
ICDM	Validating and Refining Clusters via Visual Rendering.	Keke Chen,Ling Liu	2003	The automatic clustering algorithms are known towork well in dealing with clusters of regular shapes, e.g.compact spherical/elongated shapes, but may incur highererror rates when dealing with arbitrarily shaped clusters.Although some efforts have been devoted to addressingthe problem of skewed datasets, the problem of handlingclusters with irregular shapes is still in its infancy,especially in terms of dimensionality of the datasets andthe precision of the clustering results considered. Notsurprisingly, the statistical indices works ineffective invalidating clusters of irregular shapes, too. In this paper,we address the problem of clustering and validatingarbitrarily shaped clusters with a visual framework(VISTA). The main idea of the VISTA approach is tocapitalize on the power of visualization and interactivefeedbacks to encourage domain experts to participate inthe clustering revision and clustering validation process.
ICDM	Scalable Model-based Clustering by Working on Data Summaries.	Huidong Jin,Man Leung Wong,Kwong-Sak Leung	2003	The scalability problem in data mining involves the developmentof methods for handling large databases withlimited computational resources. In this paper, we presenta two-phase scalable model-based clustering framework:First, a large data set is summed up into sub-clusters; Then,clusters are directly generated from the summary statisticsof sub-clusters by a specifically designed Expectation-Maximization(EM) algorithm. Taking example for Gaussianmixture models, we establish a provably convergentEM algorithm, EMADS, which embodies cardinality, mean,and covariance information of each sub-cluster explicitly.Combining with different data summarization procedures,EMADS is used to construct two clustering systems:gEMADS and bEMADS. The experimental results demonstratethat they run several orders of magnitude faster thanthe classic EM algorithm with little loss of accuracy. Theygenerate significantly better results than other model-basedclustering systems using similar computational resources.
ICDM	The Rough Set Approach to Association Rule Mining.	J. W. Guan,David A. Bell,Dayou Liu	2003	In transaction processing, an association is said to existbetween two sets of items when a transaction containingone set is likely to also contain the other. In informationretrieval, an association between two sets of keywords occurswhen they co-occur in a document. Similarly, in datamining, an association occurs when one attribute set occurstogether with another. As the number of such associationsmay be large, maximal association rules are sought, e.g.,Feldman et al (1997, 1998).Rough set theory is a successful tool for data mining. Byusing this theory, rules similar to maximal associations canbe found. However, we show that the rough set approach todiscovering knowledge is much simpler than the maximalassociation method.
ICDM	SVM Based Models for Predicting Foreign Currency Exchange Rates.	Joarder Kamruzzaman,Ruhul A. Sarker,Iftekhar Ahmad	2003	Support vector machine (SVM) has appeared as a powerfultool for forecasting forex market and demonstrated betterperformance over other methods, e.g., neural network orARIMA based model. SVM-based forecasting modelnecessitates the selection of appropriate kernel function andvalues of free parameters: regularization parameter and \varepsilon-insensitive loss function. In this paper, we investigate the effectof different kernel functions, namely, linear, polynomial, radialbasis and spline on prediction error measured by several widelyused performance metrics. The effect of regularizationparameter is also studied. The prediction of six different foreigncurrency exchange rates against Australian dollar has beenperformed and analyzed. Some interesting results are presented.
ICDM	Predicting distribution of a new forest disease using one-class SVMs.	Qinghua Guo,Maggi Kelly,Catherine Graham	2003	In California, a newly discovered virulent pathogen(Phytophthora ramorum) has killed thousands of nativeoak trees. Mapping the potential distribution of thepathogen is essential for decision makers to assess therisk of the pathogen and aid in preventing its furtherspread. Most methods used to map potential ranges ofspecies (e.g. multivariate or logistic regression) requireboth presence and absence data, the latter of which is notalways feasibly collected. In this study, we present theone-class Support Vector Machine (SVM) to predict thepotential distribution of Sudden Oak Death in California.The model was developed using presence data collectedthroughout the state, and tested for accuracy using a 5-fold cross-validation approach. The model performedwell, and provided 91% predicted accuracy. We believeone-class SVM when coupled with GeographicalInformation Systems (GIS) will become a very usefulmethod to deal with presence-only data in ecologicalanalysis over a range of scales.
ICDM	Understanding Helicoverpa armigera Pest Population Dynamics related to Chickpea Crop Using Neural Networks.	Rajat Gupta,B. V. L. Narayana,P. Krishna Reddy,G. V. Ranga Rao,C. L. L. Gowda,Y. V. R. Reddy,Garimella Rama Murthy	2003	Insect pests are a major cause of crop loss globally. Pestmanagement will be effective and efficient if we canpredict the occurrence of peak activities of a given pest.Research efforts are going on to understand the pestdynamics by applying analytical and other techniques onpest surveillance data sets. In this study we make an effortto understand pest population dynamics using NeuralNetworks by analyzing pest surveillance data set ofHelicoverpa armigera or Pod borer on chickpea (Cicerarietinum L.) crop. The results show that neural networkmethod successfully predicts the pest attack incidences forone week in advance.
ICDM	Reliable Detection of Episodes in Event Sequences.	Robert Gwadera,Mikhail J. Atallah,Wojciech Szpankowski	2003	"Suppose one wants to detect ""bad"" or ""suspicious"" subsequencesin event sequences.Whether an observed patternof activity (in the form of a particular subsequence) is significantand should be a cause for alarm, depends on howlikely it is to occur fortuitously.A long enough sequenceof observed events will almost certainly contain any subsequence,and setting thresholds for alarm is an important issuein a monitoring system that seeks to avoid false alarms.Suppose a long sequence T of observed events contains asuspicious subsequence pattern S within it, where the suspicioussubsequence S consists of m events and spans a windowof size w within T.We address the fundamental problem:is a certain number of occurrences of a particular subsequenceunlikely to be fortuitous (i.e., indicative of suspiciousactivity)?If the probability of fortuitous occurrencesis high and an automated monitoring system flags it as suspiciousanyway, then such a system will suffer from generatingtoo many false alarms.This paper quantifies the probabilityof such an S occuring in T within a window of sizew, the number of distinct windows containing S as a subsequence,the expected number of such occurrences, its variance,and establishes its limiting distribution that allows toset up an alarm threshold so that the probability of falsealarms is very small.We report on experiments confirmingthe theory and showing that we can detect bad subsequenceswith low false alarm rate."
ICDM	On the Privacy Preserving Properties of Random Data Perturbation Techniques.	Hillol Kargupta,Souptik Datta,Qi Wang,Krishnamoorthy Sivakumar	2003	"Privacy is becoming an increasingly important issue inmany data mining applications. This has triggered the developmentof many privacy-preserving data mining techniques.A large fraction of them use randomized data distortiontechniques to mask the data for preserving the privacyof sensitive data. This methodology attempts to hidethe sensitive data by randomly modifying the data values oftenusing additive noise. This paper questions the utility ofthe random value distortion technique in privacy preservation.The paper notes that random objects (particularly randommatrices) have ""predictable"" structures in the spectraldomain and it develops a random matrix-based spectral filteringtechnique to retrieve original data from the datasetdistorted by adding random values. The paper presents thetheoretical foundation of this filtering method and extensiveexperimental results to demonstrate that in many cases randomdata distortion preserve very little data privacy. Thepaper also points out possible avenues for the developmentof new privacy-preserving data mining techniques like exploitingmultiplicative and colored noise for preserving privacyin data mining applications."
ICDM	Comparing Pure Parallel Ensemble Creation Techniques Against Bagging.	Lawrence O. Hall,Kevin W. Bowyer,Robert E. Banfield,Divya Bhadoria,W. Philip Kegelmeyer,Steven Eschrich	2003	We experimentally evaluate randomization-based approachesto creating an ensemble of decision-tree classifiers.Unlike methods related to boosting, all of the eightapproaches considered here create each classifier in an ensembleindependently of the other classifiers. Experimentswere performed on 28 publicly available datasets, usingC4.5 release 8 as the base classifier. While each of the otherseven approaches has some strengths, we find that none ofthem is consistently more accurate than standard baggingwhen tested for statistical significance.
ICDM	Facilitating Fuzzy Association Rules Mining by Using Multi-Objective Genetic Algorithms for Automated Clustering.	Mehmet Kaya,Reda Alhajj	2003	In this paper, we propose an automated clustering methodbased on multi-objective genetic algorithms (GA); the aim ofthis method is to automatically cluster values of a givenquantitative attribute to obtain large number of largeitemsets in low duration (time). We compare the proposedmulti-objective GA-based approach with CURE-basedapproach. In addition to the autonomous specification offuzzy sets, experimental results showed that the proposedautomated clustering exhibits good performance overCURE-based approach in terms of runtime as well as thenumber of large itemsets and interesting association rules.
ICDM	PixelMaps: A New Visual Data Mining Approach for Analyzing Large Spatial Data Sets.	Daniel A. Keim,Christian Panse,Mike Sips,Stephen C. North	2003	PixelMaps are a new pixel-oriented visual data miningtechnique for large spatial datasets. They combine kernel-density-based clustering with pixel-oriented displays to emphasizeclusters while avoiding overlap in locally densepoint sets on maps. Because a full evaluation of densityfunctions is prohibitively expensive, we also propose an efficientapproximation, Fast-PixelMap, based on a synthesisof the quadtree and gridfile data structures.
ICDM	Clustering of Time Series Subsequences is Meaningless: Implications for Previous and Future Research.	Eamonn J. Keogh,Jessica Lin,Wagner Truppel	2003	"Time series data is perhaps the most frequently encountered typeof data examined by the data mining community. Clustering isperhaps the most frequently used data mining algorithm, beinguseful in it's own right as an exploratory technique, and also as asubroutine in more complex data mining algorithms such as rulediscovery, indexing, summarization, anomaly detection, andclassification. Given these two facts, it is hardly surprising thattime series clustering has attracted much attention. The data to beclustered can be in one of two formats: many individual timeseries, or a single time series, from which individual time seriesare extracted with a sliding window. Given the recent explosion ofinterest in streaming data and online algorithms, the latter casehas received much attention.In this work we make an amazing claim. Clustering of streamingtime series is completely meaningless. More concretely, clustersextracted from streaming time series are forced to obey a certainconstraint that is pathologically unlikely to be satisfied by anydataset, and because of this, the clusters extracted by anyclustering algorithm are essentially random. While this constraintcan be intuitively demonstrated with a simple illustration and issimple to prove, it has never appeared in the literature.We can justify calling our claim surprising, since it invalidatesthe contribution of dozens of previously published papers. We willjustify our claim with a theorem, illustrative examples, and acomprehensive set of experiments on reimplementations ofprevious work."
ICDM	Improving Home Automation by Discovering Regularly Occurring Device Usage Patterns.	Edwin O. Heierman III,Diane J. Cook	2003	The data stream captured by recording inhabitant-deviceinteractions in an environment can be mined todiscover significant patterns, which an intelligent agentcould use to automate device interactions. However, thisknowledge discovery problem is complicated by severalchallenges, such as excessive noise in the data, data thatdoes not naturally exist as transactions, a need tooperate in real time, and a domain where frequency maynot be the best discriminator. In this paper, we propose anovel data mining technique that addresses thesechallenges and discovers regularly-occurringinteractions with a smart home. We also discuss a casestudy that shows the data mining technique can improvethe accuracy of two prediction algorithms, thusdemonstrating multiple uses for a home automationsystem. Finally, we present an analysis of the algorithmand results obtained using inhabitant interactions.
ICDM	Dynamic Weighted Majority: A New Ensemble Method for Tracking Concept Drift.	Jeremy Z. Kolter,Marcus A. Maloof	2003	"Algorithms for tracking concept drift are important formany applications. We present a general method basedon the Weighted Majority algorithm for using any on-linelearner for concept drift. Dynamic Weighted Majority(DWM) maintains an ensemble of base learners, predictsusing a weighted-majority vote of these ""experts"",and dynamically creates and deletes experts in response tochanges in performance. We empirically evaluated two experimentalsystems based on the method using incrementalnaive Bayes and Incremental Tree Inducer (ITI) as experts.For the sake of comparison, we also included Blum's implementationof Weighted Majority. On the STAGGER Conceptsand on the SEA Concepts, results suggest that the ensemblemethod learns drifting concepts almost as well as the basealgorithms learn each concept individually. Indeed, we reportthe best overall results for these problems to date."
ICDM	Text Mining for a Clear Picture of Defect Reports: A Praxis Report.	Jutta Kreyß,Steve Selvaggio,Michael White,Zach Zakharian	2003	"We applied the text mining categorization technology,in the publicly available, IBM Enterprise InformationPortal V8.1 to more than 15,000 customer reported,product problem records. We used a proven softwarequality category set to categorize these problem recordsinto different areas of interest. Our intent was to developa clear picture of potential areas for quality improvementin each of the software products reviewed, and to providethis information to development's management.The paper presents the benefits that can be gained fromcategorizing problem records, as well as the limitations."
ICDM	Effectiveness of Information Extraction, Multi-Relational, and Semi-Supervised Learning for Predicting Functional Properties of Genes.	Mark-A. Krogel,Tobias Scheffer	2003	We focus on the problem of predicting functional propertiesof the proteins corresponding to genes in the yeastgenome. Our goal is to study the effectiveness of approachesthat utilize all data sources that are availablein this problem setting, including unlabeled and relationaldata, and abstracts of research papers. We study transductionand co-training for using unlabeled data. We investigatea propositionalization approach which uses relationalgene interaction data. We study the benefit of informationextraction for utilizing a collection of scientific abstracts.The studied tasks are KDD Cup tasks of 2001 and 2002.The solutions which we describe achieved the highest scorefor task 2 in 2001, the fourth rank for task 3 in 2001, thehighest score for one of the two subtasks and the third placefor the overall task 2 in 2002.
ICDM	Ontologies Improve Text Document Clustering.	Andreas Hotho,Steffen Staab,Gerd Stumme	2003	Text document clustering plays an important role in providingintuitive navigation and browsing mechanisms by organizinglarge sets of documents into a small number ofmeaningful clusters. The bag of words representation usedfor these clustering methods is often unsatisfactory as it ignoresrelationships between important terms that do not co-occurliterally. In order to deal with the problem, we integratecore ontologies as background knowledge into theprocess of clustering text documents. Our experimentalevaluations compare clustering techniques based on pre-categorizationsof texts from Reuters newsfeeds and on asmaller domain of an eLearning course about Java. In theexperiments, improvements of results by background knowledgecompared to a baseline without background knowledgecan be shown in many interesting combinations.
ICDM	Probabilistic Noise Identification and Data Cleaning.	Jeremy Kubica,Andrew W. Moore	2003	Real world data is never as perfect as we would like itto be and can often suffer from corruptions that may impactinterpretations of the data, models created from thedata, and decisions made based on the data.One approachto this problem is to identify and remove records that containcorruptions.Unfortunately, if only certain fields in arecord have been corrupted then usable, uncorrupted datawill be lost.In this paper we present LENS, an approach foridentifying corrupted fields and using the remaining non-corruptedfields for subsequent modeling and analysis.Ourapproach uses the data to learn a probabilistic model containingthree components: a generative model of the cleanrecords, a generative model of the noise values, and a probabilisticmodel of the corruption process.We provide an algorithmfor the unsupervised discovery of such models andempirically evaluate both its performance at detecting corruptedfields and, as one example application, the resultingimprovement this gives to a classifier.
ICDM	The Hybrid Poisson Aspect Model for Personalized Shopping Recommendation.	Chun-Nan Hsu,Hao-Hsiang Chung,Han-Shen Huang	2003	"Predicting an individual customer's likelihood of purchasinga specific item forms the basis of many marketingactivities, such as personalized shopping recommendation.Collaborative filtering and association rule miningcan be applied to this problem, but in retail supermarkets,the problem becomes particularly challenging because ofthe sparsity and skewness of transaction data. This paperpresents HyPAM(Hybrid Poisson Aspect Model), a newprobabilistic graphical model that combines a Poisson mixturewith a latent aspect class model to model customers'shopping behavior. We empirically compare HyPAM withtwo well-known recommenders, GroupLens (a correlation-basedmethod), and IBM SmartPad (association rules andcosine similarity). Experimental results show that HyPAMoutperforms the other recommenders by a large margin fortwo real-world retail supermarkets, ranking most of actualpurchases in the top ten percent of the most likely purchaseditems. We also present a new visualization method, rankplot, to evaluate the quality of recommendations."
ICDM	Tractable Group Detection on Large Link Data Sets.	Jeremy Kubica,Andrew W. Moore,Jeff G. Schneider	2003	"Discovering underlying structure from co-occurrencedata is an important task in a variety of fields, including:insurance, intelligence, criminal investigation, epidemiology,human resources, and marketing.Previously Kubicaet. al. presented the group detection algorithm (GDA) - analgorithm for finding underlying groupings of entities fromco-occurrence data.This algorithm is based on a probabilisticgenerative model and produces coherent groups thatare consistent with prior knowledge.Unfortunately, the optimizationused in GDA is slow, potentially making it infeasiblefor many large data sets.To this end, we present k-groups - an algorithm that uses an approach similar tothat of k-means to significantly acclerate the discovery ofgroups while retaining GDA's probabilistic model.We comparethe performance of GDA and k-groups on a variety ofdata, showing that k-groups' sacrifice in solution quality issignificantly offset by its increase in speed."
ICDM	Efficient Mining of Frequent Subgraphs in the Presence of Isomorphism.	Jun Huan,Wei Wang,Jan Prins	2003	Frequent subgraph mining is an active research topic inthe data mining community. A graph is a general modelto represent data and has been used in many domains likecheminformatics and bioinformatics. Mining patterns fromgraph databases is challenging since graph related operations,such as subgraph testing, generally have higher timecomplexity than the corresponding operations on itemsets,sequences, and trees, which have been studied extensively.In this paper, we propose a novel frequent subgraph miningalgorithm: FFSM, which employs a vertical search schemewithin an algebraic graph framework we have developedto reduce the number of redundant candidates proposed.Our empirical study on synthetic and real datasets demonstratesthat FFSM achieves a substantial performance gainover the current start-of-the-art subgraph mining algorithmgSpan.
ICDM	Comparing Naive Bayes, Decision Trees, and SVM with AUC and Accuracy.	Jin Huang,Jingjing Lu,Charles X. Ling	2003	Predictive accuracy has often been used as the mainand often only evaluation criterion for the predictive performanceof classification or data mining algorithms. Inrecent years, the area under the ROC (Receiver OperatingCharacteristics) curve, or simply AUC, has been proposedas an alternative single-number measure for evaluating performanceof learning algorithms. In our previous work, weproved that AUC is, in general, a better measure (definedprecisely) than accuracy. Many popular data mining algorithmsshould then be re-evaluated in terms of AUC. Forexample, it is well accepted that Naive Bayes and decisiontrees are very similar in accuracy. How do they compare inAUC? Also, how does the recently developed SVM (SupportVector Machine) compare to traditional learning algorithmsin accuracy and AUC? We will answer these questions inthis paper. Our conclusions will provide important guide-linesin data mining applications on real-world datasets.
ICDM	A Dynamic Adaptive Self-Organising Hybrid Model for Text Clustering.	Chihli Hung,Stefan Wermter	2003	Clustering by document concepts is a powerful way ofretrieving information from a large number of documents.This task in general does not make any assumption on thedata distribution. In this paper, for this task we propose anew competitive Self-Organising (SOM) model, namelythe Dynamic Adaptive Self-Organising Hybrid model(DASH). The features of DASH are a dynamic structure,hierarchical clustering, non-stationary data learning andparameter self-adjustment. All features are data-oriented:DASH adjusts its behaviour not only by modifying itsparameters but also by an adaptive structure. Thehierarchical growing architecture is a useful facility forsuch a competitive neural model which is designed fortext clustering. In this paper, we have presented a newtype of self-organising dynamic growing neural networkwhich can deal with the non-uniform data distributionand the non-stationary data sets and represent the innerdata structure by a hierarchical view.
ICDM	Tree-structured Partitioning Based on Splitting Histograms of Distances.	Longin Jan Latecki,Rajagopal Venugopal,Marc Sobel,Steve Horvat	2003	"We propose a novel clustering algorithm that is similar in spiritto classification trees. The data is recursively split using a criterionthat applies a discrete curve evolution method to the histogramof distances. The algorithm can be depicted throughtree diagrams with triple splits. Leaf nodes represent eitherclusters or sets of observations that can not yet be clearly assignedto a cluster. After constructing the tree, unclassified datapoints are mapped to their closest clusters. The algorithm hasseveral advantages. First, it deals effectively with observationsthat can not be unambiguously assigned to a cluster by allowinga ""margin of error"". Second, it automatically determinesthe number of clusters; apart from the margin of error the useronly needs to specify the minimal cluster size but not the numberof clusters. Third, it is linear with respect to the number ofdata points and thus suitable for very large data sets. Experimentsinvolving both simulated and real data from differentdomains show that the proposed method is effective and efficient."
ICDM	Mining Significant Pairs of Patterns from Graph Structures with Class Labels.	Akihiro Inokuchi,Hisashi Kashima	2003	In recent years, the problem of mining association rulesover frequent itemsets in transactional data has been frequentlystudied and yielded several algorithms that can findassociation rules within a limited amount of time. Alsomore complex patterns have been considered such as orderedtrees, unordered trees, or labeled graphs. Althoughsome approaches can efficiently derive all frequent subgraphsfrom a massive dataset of graphs, a subgraph orsubtree that is mathematically defined is not necessarily abetter knowledge representation. In this paper, we proposean efficient approach to discover significant rules to classifypositive and negative graph examples by estimating atight upper bound on the statistical metric. This approachabandons unimportant rules earlier in the computations,and thereby accelerates the overall performance. The performancehas been evaluated using real world datasets, andthe efficiency and effect of our approach has been confirmedwith respect to the amount of data and the computation time.
ICDM	Localized Prediction of Continuous Target Variables Using Hierarchical Clustering.	Aleksandar Lazarevic,Ramdev Kanapady,Chandrika Kamath,Vipin Kumar,Kumar K. Tamma	2003	In this paper, we propose a novel technique for the efficientprediction of multiple continuous target variablesfrom high-dimensional and heterogeneous data sets usinga hierarchical clustering approach. The proposed approachconsists of three phases applied recursively:partitioning, localization and prediction. In thepartitioning step, similar target variables are groupedtogether by a clustering algorithm. In the localizationstep, a classification model is used to predict which groupof target variables is of particular interest. If theidentified group of target variables still contains a largenumber of target variables, the partitioning andlocalization steps are repeated recursively and theidentified group is further split into subgroups with moresimilar target variables. When the number of targetvariables per identified subgroup is sufficiently small, thethird step predicts target variables using localized predictionmodels built from only those data records thatcorrespond to the particular subgroup. Experimentsperformed on the problem of damage prediction incomplex mechanical structures indicate that ourproposed hierarchical approach is computationally moreefficient and more accurate than straightforward methodsof predicting each target variable individually orsimultaneously using global prediction models.
ICDM	CoMine: Efficient Mining of Correlated Patterns.	Young-Koo Lee,Won-Young Kim,Y. Dora Cai,Jiawei Han	2003	Association rule mining often generates a huge numberof rules, but a majority of them either are redundantor don not reflect the tue correlation relationship amongdata objects.In this paper, we re-examine this problemand show that two interesting measures, all_confidence(denoted as \alpha) and coherence (denoted as \gamma), both disclosegenuine correlation relationships and can be computedefficiently.Moreover, we propose two interestingalgorithms, CoMine(\alpha) and CoMine(\gamma), based onextensions of a pattern-growth methodology.Our performancestudy shows that the CoMine algorithms havehigh performance in comparison with their Apriori-basedcounterpart algorithms.
ICDM	An Algebra for Inductive Query Evaluation.	Sau Dan Lee,Luc De Raedt	2003	Inductive queries are queries that generate pattern sets.This paper studies properties of boolean inductive queries,i.e. queries that are boolean expressions over monotonicand anti-monotonic constraints. More specifically, we introduceand study algebraic operations on the answer setsof such queries and show how these can be used for constructingand optimizing query plans. Special attention isdevoted to the dimension of the queries, i.e. the minimumnumber of version spaces needed to represent the answersets. The framework has been implemented for the patterndomain of strings and experimentally validated.
ICDM	Mining Production Data with Neural Network & CART.	Mingkun Li,Shuo Feng,Ishwar K. Sethi,Jason Luciow,Keith Wagner	2003	This paper presents the preliminary results of a datamining study of a production line involving hundreds ofvariables related to mechanical, chemical, electrical andmagnetic processes involved in manufacturing coatedglass. The study was performed using two nonlinear,nonparametric approaches, namely neural network andCART, to model the relationship between the qualities ofthe coating and machine readings. Furthermore, neuralnetwork sensitivity analysis and CART variable rankingswere used to gain insight into the coating process. Ourinitial results show the promise of data mining techniquesto improve the production.
ICDM	Spatial Interest Pixels (SIPs): Useful Low-Level Features of Visual Media Data.	Qi Li,Jieping Ye,Chandra Kambhamettu	2003	Visual media data such as an image is the raw data representationfor many important applications. The biggestchallenge in using visual media data comes from the extremelyhigh dimensionality. We present a comparativestudy on spatial interest pixels (SIPs), including eight-way(a novel SIP miner), Harris, and Lucas-Kanade, whose extractionis considered as an important step in reducing thedimensionality of visual media data. With extensive casestudies, we have shown the usefulness of SIPs as the low-levelfeatures of visual media data. A class-preserving dimensionreduction algorithm (using GSVD) is applied tofurther reduce the dimension of feature vectors based onSIPs. The experiments showed its superiority over PCA.
ICDM	Direct Interesting Rule Generation.	Jiuyong Li,Yanchun Zhang	2003	An association rule generation algorithm usually generatestoo many rules including a lot of uninteresting ones.Many interestingness criteria are proposed to prune thoseuninteresting rules. However, they work in post-pruningprocess and hence do not improve the rule generation ef£ciency. In this paper, we discuss properties of informativerule set and conclude that the informative rule set includesall interesting rules measured by many commonly used interestingnesscriteria, and that rules excluded by the informativerule set are forwardly prunable, i.e. they can be removedin the rule generation process instead of post pruning.Based on these properties, we propose a Direct Interestingrule Generation algorithm, DIG, to directly generateinteresting rules de£ned by any of 12 interestingness criteriadiscussed in this paper. We further show experimentallythat DIG is faster and uses less memory than Apriori.
ICDM	Mining Frequent Itemsets in Distributed and Dynamic Databases.	Matthew Eric Otey,Chao Wang,Srinivasan Parthasarathy,Adriano Veloso,Wagner Meira Jr.	2003	Traditional methods for frequent itemset mining typicallyassume that data is centralized and static. Such methods imposeexcessive communication overhead when data is distributed,and they waste computational resources when datais dynamic. In this paper we present what we believe to bethe first unified approach that overcomes these assumptions.Our approach makes use of parallel and incremental techniquesto generate frequent itemsets in the presence of dataupdates without examining the entire database, and imposesminimal communication overhead when mining distributeddatabases. Further, our approach is able to generate bothlocal and global frequent itemsets. This ability permits ourapproach to identify high-contrast frequent itemsets, whichallows one to examine how the data is skewed over differentsites.
ICDM	Interpretations of Association Rules by Granular Computing.	Yuefeng Li,Ning Zhong	2003	"This paper presents interpretations for associationrules. It first introduces Pawlak's method, and thecorresponding algorithm of finding decision rules (a kindof association rules). It then uses extended random sets topresent a new algorithm of finding interesting rules. Itproves that the new algorithm is faster than Pawlak'salgorithm. The extended random sets are easily to includemore than one criterion for determining interesting rules.They also provide two measures for dealing withuncertainties in association rules."
ICDM	Using Discriminant Analysis for Multi-class Classification.	Tao Li,Shenghuo Zhu,Mitsunori Ogihara	2003	Discriminant analysis is known to learn discriminativefeature transformations. This paper studies its use in multi-classclassification problems. The performance is tested ona large collection of benchmark datasets.
ICDM	Unsupervised Link Discovery in Multi-relational Data via Rarity Analysis.	Shou-de Lin,Hans Chalupsky	2003	"A significant portion of knowledge discovery and datamining research focuses on finding patterns of interest indata. Once a pattern is found, it can be used to recognizesatisfying instances. The new area of link discoveryrequires a complementary approach, since patterns ofinterest might not yet be known or might have too fewexamples to be learnable. This paper presents anunsupervised link discovery method aimed at discoveringunusual, interestingly linked entities in multi-relationaldatasets. Various notions of rarity are introduced tomeasure the ""interestingness"" of sets of paths andentities. These measurements have been implemented andapplied to a real-world bibliographic dataset where theygive very promising results."
ICDM	Inference of Protein-Protein Interactions by Unlikely Profile Pair.	Byung-Hoon Park,George Ostrouchov,Gong-Xin Yu,Al Geist,Andrey Gorin,Nagiza F. Samatova	2003	"We note that a set of statistically ""unusual"" protein-profilepairs in experimentally determined database ofprotein-protein interactions can typify protein-proteininteractions, and propose a novel method calledPICUPP that sifts such protein-profile pairs using astatistical simulation. It is demonstrated that unusualPfam and InterPro profile pairs can be extracted fromthe DIP database using a bootstrapping approach. Weparticularly illustrate that such protein-profile pairs canbe used for predicting putative pairs of interactingproteins. Their prediction accuracies are around 86%and 90% when InterPro and Pfam profiles are used,respectively at 75% confidence level."
ICDM	Building Text Classifiers Using Positive and Unlabeled Examples.	Bing Liu,Yang Dai,Xiaoli Li,Wee Sun Lee,Philip S. Yu	2003	This paper studies the problem of building text classifiersusing positive and unlabeled examples. The key feature ofthis problem is that there is no negative example forlearning. Recently, a few techniques for solving thisproblem were proposed in the literature. These techniquesare based on the same idea, which builds a classifier intwo steps. Each existing technique uses a different methodfor each step. In this paper, we first introduce some newmethods for the two steps, and perform a comprehensiveevaluation of all possible combinations of methods of thetwo steps. We then propose a more principled approachto solving the problem based on a biased formulation ofSVM, and show experimentally that it is more accuratethan the existing techniques.
ICDM	Efficient Nonlinear Dimension Reduction for Clustered Data Using Kernel Functions.	Cheong Hee Park,Haesun Park	2003	In this paper, we propose a nonlinear feature extractionmethod which is based on centroids and kernel functions.The dimension reducing nonlinear transformation isobtained by implicitly mapping the input data into a featurespace using a kernel function, and then finding a linearmapping based on an orthonormal basis of centroids in thefeature space that maximally separates the between-classrelationship. The proposed method utilizes an efficient algorithmto compute an orthonormal basis of centroids in thefeature space transformed by a kernel function and achievesdramatic computational savings. The experimental resultsdemonstrate that our method is capable of extracting non-linearfeatures effectively so that competitive performanceof classification can be obtained in the reduced dimensionalspace.
ICDM	OP-Cluster: Clustering by Tendency in High Dimensional Space.	Jinze Liu,Wei Wang	2003	Clustering is the process of grouping a set of objects intoclasses of similar objects. Because of unknownness of thehidden patterns in the data sets, the definition of similarityis very subtle. Until recently, similarity measures are typicallybased on distances, e.g Euclidean distance and cosinedistance. In this paper, we propose a flexible yet powerfulclustering model, namely OP-Cluster (Order PreservingCluster). Under this new model, two objects are similaron a subset of dimensions if the values of these twoobjects induce the same relative order of those dimensions.Such a cluster might arise when the expression levels of (co-regulated)genes can rise or fall synchronously in responseto a sequence of environment stimuli. Hence, discovery ofOP-Cluster is essential in revealing significant gene regulatorynetworks. A deterministic algorithm is designed andimplemented to discover all the significant OP-Clusters. Aset of extensive experiments has been done on several realbiological data sets to demonstrate its effectiveness and efficiencyin detecting co-regulated patterns.
ICDM	Protecting Sensitive Knowledge By Data Sanitization.	Stanley R. M. Oliveira,Osmar R. Zaïane	2003	In this paper, we address the problem of protecting somesensitive knowledge in transactional databases. The challengeis on protecting actionable knowledge for strategicdecisions, but at the same time not losing the great benefitof association rule mining. To accomplish that, we introducea new, efficient one-scan algorithm that meets privacyprotection and accuracy in association rule mining, withoutputting at risk the effectiveness of the data mining per se.
ICDM	Parsing Without a Grammar: Making Sense of Unknown File Formats.	Levon Lloyd,Steven Skiena	2003	The thousands of specialized structured file formats inuse today present a substantial barrier to freely exchanginginformation between applications programs. We considerthe problem of deducing such basic features as thewhitespace characters, bracketing delimiter symbols, andself-delimiter characters of a given file format from one ormore example files. We demonstrate that for sufficientlylarge example files, we can typically identify the basic featuresof interest.
ICDM	Sequence Modeling with Mixtures of Conditional Maximum Entropy Distributions.	Dmitry Pavlov	2003	"We present a novel approach to modeling sequences usingmixtures of conditional maximum entropy (maxent) distributions.Our method generalizes the mixture of first-orderMarkov models by including the ""long-term"" dependenciesin model components.The ""long-term"" dependenciesare represented by the frequently used in the naturallanguage processing (NLP) domain probabilistic triggersor rules (suc as ""A occured k positions back"" \Longrightarrow""the current symbol is B"" with probability P).The maxentframework is then used to create a coherent global probabilisticmodel from all selected triggers.In this paper, weenhance this formalism by using probabilistic mixtures withmaxent models as components, thus representing hidden orunobserved effects in the data.We demonstrate how ourmixture of conditional maxent models can be learned fromdata using the generalized EM algorithm that scales linearlyin the dimensions of the data and the number of mixturecomponents.We present empirical results on the simulatedand real-world data sets and demonstrate that theproposed approach enables us to create better quality modelsthan the mixtures of first-order Markov models and resistoverfitting and curse of dimensionality that would inevitablypresent themselves for the higher order Markov models."
ICDM	Algorithms for Spatial Outlier Detection.	Chang-Tien Lu,Dechang Chen,Yufeng Kou	2003	A spatial outlier is a spatially referenced object whosenon-spatial attribute values are significantly different fromthe values of its neighborhood. Identification of spatial outlierscan lead to the discovery of unexpected, interesting,and useful spatial patterns for further analysis. One drawbackof existing methods is that normal objects tend to befalsely detected as spatial outliers when their neighborhoodcontains true spatial outliers. In this paper, we proposea suite of spatial outlier detection algorithms to overcomethis disadvantage. We formulate the spatial outlier detectionproblem in a general way and design algorithms whichcan accurately detect spatial outliers. In addition, usinga real-world census data set, we demonstrate that our approachescan not only avoid detecting false spatial outliersbut also find true spatial outliers ignored by existing methods.
ICDM	MaPle: A Fast Algorithm for Maximal Pattern-based Clustering.	Jian Pei,Xiaoling Zhang,Moonjung Cho,Haixun Wang,Philip S. Yu	2003	Pattern-based clustering is important in many applications,such as DNA micro-array data analysis, automaticrecommendation systems and target marketing systems.However, pattern-based clustering in large databasesis challenging. On the one hand, there can be a huge numberof clusters and many of them can be redundant and thusmake the pattern-based clustering ineffective. On the otherhand, the previous proposed methods may not be efficient orscalable in mining large databases.In this paper, we study the problem of maximal pattern-basedclustering. Redundant clusters are avoided completelyby mining only the maximal pattern-based clusters.MaPle, an efficient and scalable mining algorithm is developed.It conducts a depth-first, divide-and-conquer searchand prunes unnecessary branches smartly. Our extensiveperformance study on both synthetic data sets and real datasets shows that maximal pattern-based clustering is effective.It reduces the number of clusters substantially. Moreover,MaPle is more efficient and scalable than the previouslyproposed pattern-based clustering methods in mininglarge databases.
ICDM	Structure Search and Stability Enhancement of Bayesian Networks.	Hanchuan Peng,Chris H. Q. Ding	2003	Learning Bayesian network structure from large-scale datasets, without any expert-specified ordering of variables, remainsa difficult problem. We propose systematic improvements toautomatically learn Bayesian network structure from data. (1)We propose a linear parent search method to generate candidategraph. (2) We propose a comprehensive approach to eliminatecycles using minimal likelihood loss, a short cycle first heuristic,and a cut-edge repairing. (3) We propose structure perturbationto assess the stability of the network and a stability-improvementmethod to refine the network structure. The algorithms are easyto implement and efficient for large networks. Experimental resultson two data sets show that our new approach outperformsexisting methods.
ICDM	Exploiting Unlabeled Data for Improving Accuracy of Predictive Data Mining.	Kang Peng,Slobodan Vucetic,Bo Han,Hongbo Xie,Zoran Obradovic	2003	Predictive data mining typically relies on labeled datawithout exploiting a much larger amount of availableunlabeled data. The goal of this paper is to show thatusing unlabeled data can be beneficial in a range ofimportant prediction problems and therefore should be anintegral part of the learning process. Given an unlabeleddataset representative of the underlying distribution and aK-class labeled sample that might be biased, ourapproach is to learn K contrast classifiers each trained todiscriminate a certain class of labeled data from theunlabeled population. We illustrate that contrastclassifiers can be useful in one-class classification, outlierdetection, density estimation, and learning from biaseddata. The advantages of the proposed approach aredemonstrated by an extensive evaluation on synthetic datafollowed by real-life bioinformatics applications for (1)ranking PubMed articles by their relevance to proteindisorder and (2) cost-effective enlargement of adisordered protein database.
ICDM	Learning Rules for Anomaly Detection of Hostile Network Traffic.	Matthew V. Mahoney,Philip K. Chan	2003	We introduce an algorithm called LERAD that learnsrules for finding rare events in nominal time-series datawith long range dependencies. We use LERAD to findanomalies in network packets and TCP sessions to detectnovel intrusions. We evaluated LERAD on the 1999DARPA/Lincoln Laboratory intrusion detection evaluationdata set and on traffic collected in a universitydepartmental server environment.
ICDM	An Algorithm for the Exact Computation of the Centroid of Higher Dimensional Polyhedra and its Application to Kernel Machines.	Frédéric Maire	2003	The Support Vector Machine (SVM) solution correspondsto the centre of the largest sphere inscribed in versionspace. Alternative approaches like Bayesian PointMachines (BPM) and Analytic Centre Machines have suggestedthat the generalization performance can be furtherenhanced by considering other possible centres of versionspace like the centroid (centre of mass) or the analytic centre.We present an algorithm to compute exactly the centroidof higher dimensional polyhedra, then derive approximationalgorithms to build a new learning machine whoseperformance is comparable to BPM. We also show that forregular kernel matrices (Gaussian kernels for example), theSVM solution can be obtained by solving a linear system ofequalities.
ICDM	Probabilistic User Behavior Models.	Eren Manavoglu,Dmitry Pavlov,C. Lee Giles	2003	We present a mixture model based approach for learningindividualized behavior models for the Web users. Weinvestigate the use of maximum entropy and Markov mixturemodels for generating probabilistic behavior models.We first build a global behavior model for the entire populationand then personalize this global model for the existingusers by assigning each user individual componentweights for the mixture model. We then use these individualweights to group the users into behavior model clusters.We show that the clusters generated in this manner areinterpretable and able to represent dominant behavior patterns.We conduct offline experiments on around two monthsworth of data from CiteSeer, an online digital library forcomputer science research papers currently storing morethan 470,000 documents. We show that both maximum entropyand Markov based personal user behavior modelsare strong predictive models. We also show that maximumentropy based mixture model outperforms Markov mixturemodels in recognizing complex user behavior patterns.
ICDM	Regulatory Element Discovery Using Tree-structured Models.	Tu Minh Phuong,Doheon Lee,Kwang Hyung Lee	2003	Computational discovery of transcriptional regulatoryregions in DNA sequences provides an efficient way tobroaden our understanding of how cellular processes arecontrolled. In this paper, we formulate the regulatoryelement discovery problem in the regression frameworkwith regulatory regions treated as predictor variables andgene expression levels as responses. We use regressiontree models to identify structural relationships betweenpredictors and responses. The regression treemethodology is extended to handle multiple responsesfrom different experiments by modifying the split function.We apply this method to two data sets of the yeastSaccharomyces cerevisiae. The method successfullyidentifies most of regulatory motifs that are known tocontrol gene transcription under the given experimentalconditions. Our method also suggests several putativemotifs that can present novel regulatory motifs.
ICDM	Privacy-Preserving Collaborative Filtering Using Randomized Perturbation Techniques.	Huseyin Polat,Wenliang Du	2003	"Collaborative Filtering (CF) techniques are becomingincreasingly popular with the evolution of the Internet. Toconduct collaborative filtering, data from customers areneeded. However, collecting high quality data from customersis not an easy task because many customers areso concerned about their privacy that they might decide togive false information. We propose a randomized perturbation(RP) technique to protect users' privacy while stillproducing accurate recommendations."
ICDM	Statistical Relational Learning for Document Mining.	Alexandrin Popescul,Lyle H. Ungar,Steve Lawrence,David M. Pennock	2003	"A major obstacle to fully integrated deployment of manydata mining algorithms is the assumption that data sitsin a single table, even though most real-world databaseshave complex relational structures. We propose an integratedapproach to statistical modeling from relationaldatabases. We structure the search space based on ""refinementgraphs"", which are widely used in inductive logic programmingfor learning logic descriptions. The use of statisticsallows us to extend the search space to include richerset of features, including many which are not boolean.Search and model selection are integrated into a single process,allowing information criteria native to the statisticalmodel, for example logistic regression, to make feature selectiondecisions in a step-wise manner. We present experimentalresults for the task of predicting where scientific paperswill be published based on relational data taken fromCiteSeer. Our approach results in classification accuraciessuperior to those achieved when using classical ""flat"" features.The resulting classifier can be used to recommendwhere to publish articles."
ICDM	Zigzag: a new algorithm for mining large inclusion dependencies in database.	Fabien De Marchi,Jean-Marc Petit	2003	In the relational model, inclusion dependencies (INDs)convey many information on data semantics. They generalizeforeign keys, which are very popular constraints inpractice. However, one seldom knows the set of satisfiedINDs in a database. The IND discovery problem in existingdatabases can be formulated as a data-mining problem.We underline in this article that the exploration of IND expressionsfrom most general (smallest) INDs to most specific(largest) INDs does not succeed whenever large INDshave to be discovered. To cope with this problem, we introducea new algorithm, called Zigzag , which combinesthe strength of levelwise algorithms (to find out some smallestINDs) with an optimistic criteria to jump more or lessto largest INDs. Preliminary tests, on synthetic databases,are presented and commented on. It is worth noting that themain result of this paper is general enough to be appliedto other data-mining problems, such as maximal frequentitemsets mining.
ICDM	Semantic Role Parsing: Adding Semantic Structure to Unstructured Text.	Sameer Pradhan,Kadri Hacioglu,Wayne Ward,James H. Martin,Daniel Jurafsky	2003	There is a ever-growing need to add structure in the formof semantic markup to the huge amounts of unstructured textdata now available. We present the technique of shallow semanticparsing, the process of assigning a simple WHO didWHAT to WHOM, etc., structure to sentences in text, as auseful tool in achieving this goal. We formulate the semanticparsing problem as a classification problem using SupportVector Machines. Using a hand-labeled training setand a set of features drawn from earlier work together withsome feature enhancements, we demonstrate a system thatperforms better than all other published results on shallowsemantic parsing.
ICDM	Mining Semantic Networks for Knowledge Discovery.	Kanagasabai Rajaraman,Ah-Hwee Tan	2003	"This paper addresses the problem of mining a class ofsemantic networks, called Concept Frame Graphs (CFG's),for knowledge discovery from text. This new representationis motivated by the need to capture richer text content sothat non-trivial mining tasks can be performed. We firstdefine the CFG representation and then describe a rule-basedalgorithm for constructing a CFG from text documents.Treating the CFG as a networked knowledge base,we propose new methods for text mining. On a specific taskof discovering the top companies in an area, we observe thatour approach leads to simpler content mining algorithms,once the CFG has been constructed. Moreover, exploitingthe network structure of CFG results in significant improvementsin precision and recall."
ICDM	Privacy-preserving Distributed Clustering using Generative Models.	Srujana Merugu,Joydeep Ghosh	2003	"We present a framework for clustering distributed datain unsupervised and semi-supervised scenarios, taking intoaccount privacy requirements and communication costs.Rather than sharing parts of the original or perturbed data,we instead transmit the parameters of suitable generativemodels built at each local data site to a central location.We mathematically show that the best representative of allthe data is a certain ""mean"" model, and empirically showthat this model can be approximated quite well by generatingartificial samples from the underlying distributions usingMarkov Chain Monte Carlo techniques, and then fittinga combined global model with a chosen parametric form tothese samples. We also propose a new measure that quantifiesprivacy based on information theoretic concepts, andshow that decreasing privacy leads to a higher quality of thecombined model and vice versa. We provide empirical resultson different data types to highlight the generality of ourframework. The results show that high quality distributedclustering can be achieved with little privacy loss and lowcommunication cost."
ICDM	Change Profiles.	Taneli Mielikäinen	2003	In this paper we introduce a generalization of associationrules: change profiles. We analyze their properties, describetheir relationship to other structures in pattern discoveryand sketch their possible applications. We studyhow the frequent patterns can be clustered based on theirchange profiles and propose methods for approximating thefrequencies of the patterns from the approximate changeprofiles and bounding the intervals where the frequencies ofthe patterns are guaranteed to be. We evaluate empiricallythe methods for estimating the frequencies and the stabilityof their frequency estimates under different kinds of noise.
ICDM	Integrating Customer Value Considerations into Predictive Modeling.	Saharon Rosset,Einat Neumann	2003	"The success of prediction models for business purposesshould not be measured by their accuracy only. Theirevaluation should also take into account the higherimportance of precise prediction for ""valuable""customers. We illustrate this idea through the example ofchurn modeling in telecommunications, where it isobviously much more important to identify potentialchurn among valuable customers. We discuss, boththeoretically and empirically, the optimal use of""customer value"" data in the model training, modelevaluation and scoring stages. Our main conclusion isthat a non-trivial approach of using ""decayed"" value-weightsfor training is usually preferable to the twoobvious approaches of either using non-decayed customervalues as weights or ignoring them."
ICDM	Complex Spatial Relationships.	Robert Munro,Sanjay Chawla,Pei Sun	2003	"This paper describes the need for mining complex relationshipsin spatial data. Complex relationships are definedas those involving two or more of: multi-feature colocation,self-colocation, one-to-many relationships, self-exclusionand multi-feature exclusion. We demonstrate that even inthe mining of simple relationships, knowledge of complexrelationships is necessary to accurately calculate the significanceof results. We implement a representation of spatialdata such that it contains known 'weak-monotonic' properties,which are exploited for the efficient mining of complexrelationships, and discuss the strengths and limitations ofthis representation."
ICDM	TECNO-STREAMS: Tracking Evolving Clusters in Noisy Data Streams with a Scalable Immune System Learning Model.	Olfa Nasraoui,Cesar Cardona Uribe,Carlos Rojas Coronel,Fabio A. González	2003	Artificial Immune System (AIS) models hold many promises inthe field of unsupervised learning. However, existing models arenot scalable, which makes them of limited use in data mining. Wepropose a new AIS based clustering approach (TECNO-STREAMS)that addresses the weaknesses of current AIS models. Comparedto existing AIS based techniques, our approach exhibits superiorlearning abilities, while at the same time, requiring low memoryand computational costs. Like the natural immune system, thestrongest advantage of immune based learning compared to otherapproaches is expected to be its ease of adaptation to the dynamicenvironment that characterizes several applications, particularlyin mining data streams. We illustrate the ability of the proposedapproach in detecting clusters in noisy data sets, and in miningevolving user profiles from Web clickstream data in a single pass.TECNO-STREAMS adheres to all the requirements of clusteringdata streams: compactness of representation, fast incremental processingof new data points, and clear and fast identification of outliers.
ICDM	Simple Estimators for Relational Bayesian Classifiers.	Jennifer Neville,David Jensen,Brian Gallagher	2003	"In this paper we present the Relational BayesianClassifier (RBC), a modification of the Simple BayesianClassifier (SBC) for relational data. There exist severalBayesian classifiers that learn predictive models ofrelational data, but each uses a different estimationtechnique for modeling heterogeneous sets of attributevalues. The effects of data characteristics on estimationhave not been explored. We consider four simpleestimation techniques and evaluate them on three real-worlddata sets. The estimator that assumes each multisetvalue is independently drawn from the same distribution(INDEPVAL) achieves the best empirical results. Weexamine bias and variance tradeoffs over a range of datasets and show that INDEPVAL's ability to model moremultiset information results in lower bias estimates andcontributes to its superior performance."
ICDM	A High-Performance Distributed Algorithm for Mining Association Rules.	Assaf Schuster,Ran Wolff,Dan Trock	2003	We present a new distributed association rule mining(D-ARM) algorithm that demonstrates superlinear speedupwith the number of computing nodes. The algorithm isthe first D-ARM algorithm to perform a single scan overthe database. As such, its performance is unmatched byany previous algorithm. Scale-up experiments over standard synthetic benchmarks demonstrate stable run time regardless of the number of computers. Theoretical analysisreveals a tighter bound on error probability than the oneshown in the corresponding sequential algorithm.
ICDM	Impact Studies and Sensitivity Analysis in Medical Data Mining with ROC-based Genetic Learning.	Michèle Sebag,Jérôme Azé,Noël Lucas	2003	"ROC curves have been used for a fair comparison of machinelearning algorithms since the late 90's. Accordingly,the area under the ROC curve (AUC) is nowadays considereda relevant learning criterion, accommodating imbalanceddata, misclassification costs and noisy data.This paper shows how a genetic algorithm-based optimizationof the AUC criterion can be exploited for impactstudies and sensitivity analysis.The approach is illustrated on the Atherosclerosis Identificationproblem, PKDD 2002 Challenge."
ICDM	Semantic Log Analysis Based on a User Query Behavior Model.	Kawamae Noriaki,Takeya Mukaigaito,Hanaki Miyoshi	2003	Semantic Log Analysis Based on a User Query Behavior Model.
ICDM	Efficient Data Mining for Maximal Frequent Subtrees.	Yongqiao Xiao,Jenq-Foung Yao,Zhigang Li,Margaret H. Dunham	2003	A new type of tree mining is defined in this paper,which uncovers maximal frequent induced subtrees from adatabase of unordered labeled trees. A novel algorithm,PathJoin, is proposed. The algorithm uses a compact datastructure, FST-Forest, which compresses the trees and stillkeeps the original tree structure. PathJoin generates candidatesubtrees by joining the frequent paths in FST-Forest.Such candidate subtree generation is localized and thussubstantially reduces the number of candidate subtrees. Experimentswith synthetic data sets show that the algorithmis effective and efficient.
ICDM	K-D Decision Tree: An Accelerated and Memory Efficient Nearest Neighbor Classifier.	Tomoyuki Shibata,Takekazu Kato,Toshikazu Wada	2003	Most nearest neighbor (NN) classifiers employ NN searchalgorithms for the acceleration. However, NNclassification does not always require the NN search.Based on this idea, we propose a novel algorithm namedk-d decision tree (KDDT). Since KDDT uses Voronoicondensed prototypes, it is less memory consuming thannaive NN classifiers. We have confirmed that KDDT ismuch faster than NN search based classifiers through thecomparative experiment (from 9 to 369 times faster).
ICDM	Evolutionary Gabor Filter Optimization with Application to Vehicle Detection.	Zehang Sun,George Bebis,Ronald Miller	2003	Despite the considerable amount of research work on the applicationof Gabor filters in pattern classification, their design and selectionhave been mostly done on a trial and error basis. Existing techniques areeither only suitable for a small number of filters or less problem-oriented.A systematic and general evolutionary Gabor filter optimization (EGFO)approach that yields a more optimal, problem-specific, set of filters is proposedin this study. The EGFO approach unifies filter design with filter selectionby integrating Genetic Algorithms (GAs) with an incremental clusteringapproach. Specifically, filter design is performed using GAs, a globaloptimization approach that encodes the parameters of the Gabor filters ina chromosome and uses genetic operators to optimize them. Filter selectionis performed by grouping together filters having similar characteristics(i.e., similar parameters) using incremental clustering in the parameterspace. Each group of filters is represented by a single filter whose parameterscorrespond to the average parameters of the filters in the group. Thisstep eliminates redundant filters, leading to a compact, optimized set of filters.The average filters are evaluated using an application-oriented fitnesscriterion based on Support Vector Machines (SVMs). To demonstrate theeffectiveness of the proposed framework, we have considered the challengingproblem of vehicle detection from gray-scale images. Our experimentalresults illustrate that the set of Gabor filters, specifically optimized for theproblem of vehicle detection, yield better performance than using traditionalfilter banks.
ICDM	Introducing Uncertainty into Pattern Discovery in Temporal Event Sequences.	Xingzhi Sun,Maria E. Orlowska,Xue Li	2003	Pattern discovery in temporal event sequences is of greatimportance in many application domains, such as telecommunicationnetwork fault analysis. In reality, not every typeof event has an accurate timestamp. Some of them, definedas inaccurate events in this paper, may only have an intervalas possible time of occurrence. The existence of inaccurateevents may cause uncertainty in event ordering. Thetraditional support model cannot deal with this uncertainty,which would cause some interesting patterns to be missing.In this paper, a new concept, precise support, is introducedto evaluate the probability of a pattern contained in a sequence.Based on this new metric, we define the uncertaintymodel and present an algorithm to discover interesting patternsin the sequence database that has one type of inaccurateevent. In our model, the number of types of inaccurateevents can be extended to k readily, however, at a cost ofincreasing computational complexity.
ICDM	Detecting Interesting Exceptions from Medical Test Data with Visual Summarization.	Einoshin Suzuki,Takeshi Watanabe,Hideto Yokoi,Katsuhiko Takabayashi	2003	In this paper, we propose a method which visualizes irregularmulti-dimensional time-series data as a sequence ofprobabilistic prototypes for detecting exceptions from medicaltest data. Conventional visualization methods often requireiterative analysis and considerable skill thus are nottotally supported by a wide range of medical experts. OurPrototypeLines displays summarized information based ona probabilistic mixture model by using hue only thus is consideredto exhibit novelty. The effectiveness of the summarizationis pursued mainly through use of a novel informationcriterion. We report our endeavor with chronic hepatitisdata, especially discoveries of interesting exceptions bya non-expert and an untrained expert.
ICDM	Mining the Web to Discover the Meanings of an Ambiguous Word.	Raz Tamir,Reinhard Rapp	2003	In information retrieval and text mining, informationon word senses is usually taken from dictionaries or lexicaldatabases that have been prepared by lexicographers.In this paper we propose an automatic method for wordsense induction, i.e. for the discovery of a set of sensedescriptors to a given ambiguous word. The approach isbased on the statistics of word co-occurrence as derivedfrom web pages. The underlying assumption is that thesenses of an ambiguous word are best described by termsthat, although bearing a strong association to this word,are mutually exclusive, i.e. whose association strengthwithin the retrieved web pages is as weak as possible.Measuring association strength is based upon a novelConfidence Gain approach that relates the observed co-occurrencefrequency for two sense descriptor candidatesto an average co-occurrence frequency for pairs of arbitrarywords. The proposed approach is fully unsupervisedand takes into account the contemporary meanings ofwords, as reflected in texts from the internet. Our resultsare evaluated using a list of ambiguous words commonlyreferred to in the literature.
ICDM	Applying Noise Handling Techniques to Genomic Data: A Case Study.	Choh-Man Teng	2003	Osteogenesis Imperfecta (OI) is a genetic collagenousdisease associated with mutations in one or both of thegenes COLIA1 and COLIA2. There are at least four knownphenotypes of OI, of which type II is the severest and oftenlethal. We identified three approaches to noise handling,namely, robust algorithms, filtering, and polishing,and evaluated their effectiveness when applied to the problemof classifying the disease OI based on a data set ofamino acid sequences and associated information of pointmutations of COLIA1. Preliminary results suggest that eachnoise handling mechanism can be useful under different circumstances.Filtering is stable across all cases. Pruningwith robust c4.5 increased the classification accuracy insome cases, and polishing gave rise to some additional improvementin classifying the lethal OI phenotype.
ICDM	A Hybrid Data-Mining Approach in Genomics and Text Structures.	Horia-Nicolai L. Teodorescu,Lucian Iulian Fira	2003	We introduce a genetic sequence identifier based on ahierarchical system using fuzzy and classic (crisp) neuralnetworks. The system is based on a set of predictors andon a decision network. The prediction of the structure ofthe genes is addressed using a new method and tools,involving the sequence of distances between bases andneuro-fuzzy predictors. The method and system have beensuccessful in predicting genomic sequences and textstructures.
ICDM	Learning Bayesian Networks from Incomplete Data Based on EMI Method.	Fengzhan Tian,Hongwei Zhang,Yuchang Lu	2003	Currently, there are few efficient methods in practice forlearning Bayesian networks from incomplete data, whichaffects their use in real world data mining applications.This paper presents a general-duty method that estimatesthe (Conditional) Mutual Information directly from incompletedatasets, EMI. EMI starts by computing the intervalestimates of a joint probability of a variable set, which areobtained from the possible completions of the incompletedataset. And then computes a point estimate via a convexcombination of the extreme points, with weights dependingon the assumed pattern of missing data. Finally, based onthese point estimates, EMI gets the estimated (conditional)Mutual Information. This paper also applies EMI to the dependencyanalysis based learning algorithm by J. Cheng soas to efficiently learn BNs with incomplete data. The experimentalresults on Asia and Alarm networks show that EMIbased algorithm is much more efficient than two search&scoring based algorithms, SEM and EM-EA algorithms. Interms of accuracy, EMI based algorithm is more accuratethan SEM algorithm, and comparable with EM-EA algorithm.
ICDM	Model Stability: A key factor in determining whether an algorithm produces an optimal model from a matching distribution.	Kai Ming Ting,Regina Jing Ying Quek	2003	This paper investigates the factors leading to producingsuboptimal models when training and test class distributions(or misclassification costs) are matched. Our resultshows that model stability plays a key role in determiningwhether the algorithm produces an optimal modelfrom a matching distribution (cost). The performance differencebetween a model trained from the matching distribution(cost) and the optimal model generally increases asthe degree of model stability decreases. The practical implicationof our result is that one should only follow theconventional wisdom of using a training class distribution(cost) that matches the test class distribution (cost) to traina classifier if the learning algorithm is known to be stable.
ICDM	Combining Multiple Weak Clusterings.	Alexander P. Topchy,Anil K. Jain,William F. Punch	2003	A data set can be clustered in many ways dependingon the clustering algorithm employed, parameter settingsused and other factors. Can multiple clusterings becombined so that the final partitioning of data providesbetter clustering? The answer depends on the quality ofclusterings to be combined as well as the properties of thefusion method. First, we introduce a unifiedrepresentation for multiple clusterings and formulate thecorresponding categorical clustering problem. As aresult, we show that the consensus function is related tothe classical intra-class variance criterion using thegeneralized mutual information definition. Second, weshow the efficacy of combining partitions generated byweak clustering algorithms that use data projections andrandom data splits. A simple explanatory model is offeredfor the behavior of combinations of such weak clusteringcomponents. We analyze the combination accuracy as afunction of parameters controlling the power andresolution of component partitions as well as the learningdynamics vs. the number of clusterings involved. Finally,some empirical studies compare the effectiveness ofseveral consensus functions.
ICDM	Enhancing Techniques for Efficient Topic Hierarchy Integration.	Jyh-Jong Tsay,Hsuan-Yu Chen,Chi-Feng Chang,Ching-Han Lin	2003	In this paper, we study the problem of integrating documentsfrom different sources into a comprehensive topic hierarchy.Our objective is to develop efficient techniques thatimprove the accuracy of traditional categorization methodsby incorporating categorization information providedby data sources into categorization process. Notice thatin the World-Wide Web, categorization information is oftenavailable from information sources. We present severalenhancing techniques that use categorization informationto enhance traditional methods such as naive Bayes andsupport vector machines. Experiment on collections fromOpenfind and Yam, and Google and Yahoo!, well-knownpopular web sites in Taiwan and USA, respectively, showsthat our techniques significantly improve the classificationaccuracy from, for example, 55% to 66% for Naive Bayes,and from 57% to 67% for SVM for the data set collectedfrom Yam and Openfind.
ICDM	"Visualization of Rule's Similarity using Multidimensional Scaling."	Shusaku Tsumoto,Shoji Hirano	2003	One of the most important problems with rule inductionmethods is that it is very difficult for domain experts to checkmillions of rules generated from large datasets. The discoveryfrom these rules requires deep interpretation from domainknowledge. Although several solutions have been proposedin the studies on data mining and knowledge discovery,these studies are not focused on similarities betweenrules obtained. When one rule r1 has reasonable featuresand the other rule r2 with high similarity to r1 includes unexpectedfactors, the relations between these rules will becomea trigger to the discovery of knowledge. In this paper,we propose a visualization approach to show the similarrelations between rules based on multidimensional scaling,which assign a two-dimensional cartesian coordinateto each data point from the information about similiariesbetween this data and others data. We evaluated this methodon two medical data sets, whose experimental results showthat knowledge useful for domain experts could be found.
ICDM	Pattern Discovery based on Rule Induction and Taxonomy Generation.	Shusaku Tsumoto,Shoji Hirano	2003	"One of the most important problems with rule inductionmethods is that they cannot extract rules, which plausiblyrepresent experts' decision processes. In this paper,the characteristics of experts' rules are closely examinedand a new approach to extract plausible rules is introduced,which consists of the following three procedures. First, thecharacterization of decision attributes (given classes) is extractedfrom databases and the concept hierarchy for givenclasses is calculated. Second, based on the hierarchy, rulesfor each hierarchical level are induced from data. Then, foreach given class, rules for all the hierarchical levels are integratedinto one rule."
ICDM	TSP: Mining Top-K Closed Sequential Patterns.	Petre Tzvetkov,Xifeng Yan,Jiawei Han	2003	Sequential pattern mining has been studied extensivelyin data mining community.Most previous studies requirethe specification of a minimum support threshold to performthe mining.However, it is difficult for users to providean appropriate threshold in practice.To overcomethis difficulty, we propose an alternative task: mining top-kfrequent closed sequential patterns of length no less thanmin_l, where k is the desired number of closed sequentialpatterns to be mined, and min_l is the minimum length ofeach pattern.We mine closed patterns since they are compactrepresentations of frequent patterns.We developed an efficient algorithm, called TSP, whichmakes use of the length constraint and the properties of top-kclosed sequential patterns to perform dynamic support-raisingand projected database-pruning.Our extensive performancestudy shows that TSP outperforms the closed sequentialpattern mining algorithm even when the latter isrunning with the best tuned minimum support threshold.
ICDM	Active Sampling for Feature Selection.	Sriharsha Veeramachaneni,Paolo Avesani	2003	In knowledge discovery applications, where new featuresare to be added, an acquisition policy can help select thefeatures to be acquired based on their relevance and thecost of extraction. This can be posed as a feature selectionproblem where the feature values are not known in advance.We propose a technique to actively sample the featurevalues with the ultimate goal of choosing between alternativecandidate features with minimum sampling cost.Our heuristic algorithm is based on extracting candidatefeatures in a region of the instance space where the featurevalue is likely to alter our knowledge the most. An experimentalevaluation on a standard database shows that it ispossible outperform a random subsampling policy in termsof the accuracy in feature selection.
ICDM	Combining the web content and usage mining to understand the visitor behavior in a web site.	Juan D. Velásquez,Hiroshi Yasuda,Terumasa Aoki	2003	"A web site is a semi structured collection of differentkinds of data, whose motivation is show relevant informationto visitor and by this way capture her/his attention.Understand the specifics preferences that define the visitorbehavior in a web site, is a complex task. An approximationis suppose that it depend the content, navigationsequence and time spent in each page visited. These variablescan be extracted from the web log files and the website itself, using web usage and content mining respectively.Combining the describe variables, a similarity measureamong visitor sessions is introduced and used in a clusteringalgorithm, which identifies groups of similar sessions,allowing the analysis of visitors behavior.In order to prove the methodology's effectiveness, it wasapplied in a certain web site, showing the benefits of thedescribed approach."
ICDM	Class Decomposition via Clustering: A New Framework for Low-Variance Classifiers.	Ricardo Vilalta,Murali-Krishna Achari,Christoph F. Eick	2003	We propose a pre-processing step to classification thatapplies a clustering algorithm to the training set to discoverlocal patterns in the attribute or input space. Wedemonstrate how this knowledge can be exploited to enhancethe predictive accuracy of simple classifiers. Our focusis mainly on classifiers characterized by high bias butlow variance (e.g., linear classifiers); these classifiers experiencedifficulty in delineating class boundaries over theinput space when a class distributes in complex ways. Decomposingclasses into clusters makes the new class distributioneasier to approximate and provides a viable way toreduce bias while limiting the growth in variance. Experimentalresults on real-world domains show an advantagein predictive accuracy when clustering is used as a pre-processingstep to classification.
ICDM	Bootstrapping Rule Induction.	Lemuel R. Waitman,Douglas H. Fisher,Paul H. King	2003	Most rule learning systems posit hard decision boundariesfor continuous attributes and point estimates of ruleaccuracy, with no measures of variance, which may seemarbitrary to a domain expert. These hard boundaries/pointschange with small perturbations to the training data. Moreover,rule induction typically produces a large number ofrules that must be filtered and interpreted by an analyst.This paper describes a method of combining rules over multiplebootstrap replications of rule induction so as to reducethe total number of rules presented to an analyst and to providemeasures of variance to continuous attribute decisionboundaries and accuracy-point estimates. The method isillustrated with perioperative data.
ICDM	Interactive Visualization and Navigation in Large Data Collections using the Hyperbolic Space.	Jörg A. Walter,Jörg Ontrup,Daniel Wessling,Helge Ritter	2003	"We propose the combination of two recently introducedmethods for the interactive visual data mining of largecollections of data. Both, Hyperbolic Multi-DimensionalScaling (HMDS) and Hyperbolic Self-Organizing Maps(HSOM) employ the extraordinary advantages of the hyperbolicplane (H2): (i) the underlying space grows exponentiallywith its radius around each point - ideal for embeddinghigh-dimensional (or hierarchical) data; (ii) thePoincaré model of the IH2 exhibits a fish-eye perspectivewith a focus area and a context preserving surrounding; (iii)the mouse binding of focus-transfer allows intuitive interactivenavigation.The HMDS approach extends multi-dimensional scalingand generates a spatial embedding of the data representingtheir dissimilarity structure as faithfully as possible. Itis very suitable for interactive browsing of data object collections,but calls for batch precomputation for larger collectionsizes.The HSOM is an extension of Kohonen's Self-OrganizingMap and generates a partitioning of the data collection assignedto an IH2 tessellating grid. While the algorithm'scomplexity is linear in the collection size, the data browsingis rigidly bound to the underlying grid.By integrating the two approaches we gain the synergetic effectof adding advantages of both. And the hybrid architectureuses consistently the IH2 visualization and navigationconcept. We present the successfully application to a textmining example involving the Reuters-21578 text corpus."
ICDM	Center-Based Indexing for Nearest Neighbors Search.	Arkadiusz Wojna	2003	The paper addresses the problem of indexing data forthe k nearest neighbors (k-nn) search. It presents a tree-basedtop-down indexing method that uses an iterative k-meansalgorithm for tree node splitting and combines threedifferent search pruning criteria from BST, GHT and GNATinto one. The experiments show that the presented indexingtree accelerates the k-nn searching up to several thousandstimes in case of large data sets.
ICDM	Association Rule Mining in Peer-to-Peer Systems.	Ran Wolff,Assaf Schuster	2003	We extend the problem of association rule mining -a key data mining problem - to systems in which thedatabase is partitioned among a very large number ofcomputers that are dispersed over a wide area. Such computing systems include GRID computing platforms, federated database systems, and peer-to-peer computing environments. The scale of these systems poses several difficulties, such as the impracticality of global communications and global synchronization, dynamic topology changes ofthe network, on-the-fly data updates, the need to share resources with other applications, and the frequent failureand recovery of resources.We present an algorithm by which every node in thesystem can reach the exact solution, as if it were giventhe combined database. The algorithm is entirely asynchronous, imposes very little communication overhead,transparently tolerates network topology changes andnode failures, and quickly adjusts to changes in the dataas they occur. Simulation of up to 10,000 nodes show thatthe algorithm is local: all rules, except for those whoseconfidence is about equal to the confidence threshold, arediscovered using information gathered from a very smallvicinity, whose size is independent of the size of the system.
ICDM	MPIS: Maximal-Profit Item Selection with Cross-Selling Considerations.	Raymond Chi-Wing Wong,Ada Wai-Chee Fu,Ke Wang	2003	In the literature of data mining, many different algorithmsfor association rule mining have been proposed. However,there is relatively little study on how association rules can aidin more specific targets. In this paper, one of the applicationsfor association rules - maximal-profit item selection with cross-selling effect (MPIS) problem - is investigated. The problemis about selecting a subset of items which can give the maximalprofit with the consideration of cross-selling. We provethat a simple version of this problem is NP-hard. We proposea new approach to the problem with the consideration of theloss rule - a kind of association rule to model the cross-sellingeffect. We show that the problem can be transformed to aquadratic programming problem. In case quadratic programmingis not applicable, we also propose a heuristic approach.Experiments are conducted to show that both of the proposedmethods are highly effective and efficient.
ICDM	Mining Strong Affinity Association Patterns in Data Sets with Skewed Support Distribution.	Hui Xiong,Pang-Ning Tan,Vipin Kumar	2003	Existing association-rule mining algorithms often relyon the support-based pruning strategy to prune its combinatorialsearch space. This strategy is not quite effectivefor data sets with skewed support distributions because theytend to generate many spurious patterns involving itemsfrom different support levels or miss potentially interestinglow-support patterns. To overcome these problems, we proposethe concept of hyperclique pattern, which uses an objectivemeasure called h-confidence to identify strong affinitypatterns. We also introduce the novel concept of cross-supportproperty for eliminating patterns involving itemswith substantially different support levels. Our experimentalresults demonstrate the effectiveness of this method forfinding patterns in dense data sets even at very low supportthresholds, where most of the existing algorithms wouldbreak down. Finally, hyperclique patterns also show greatpromise for clustering items in high dimensional space.
ICDM	Clustering Item Data Sets with Association-Taxonomy Similarity.	Ching-Huang Yun,Kun-Ta Chuang,Ming-Syan Chen	2003	We explore in this paper the efficient clustering of itemdata. Different from those of the traditional data, the featuresof item data are known to be of high dimensionalityand sparsity. In view of the features of item data, we devisein this paper a novel measurement, called the association-taxonomysimilarity, and utilize this measurement to performthe clustering. With this association-taxonomy similaritymeasurement, we develop an efficient clustering algorithm,called algorithm AT (standing for Association-Taxonomy),for item data. Two validation indexes basedon association and taxonomy properties are also devised toassess the quality of clustering for item data. As validatedby the real dataset, it is shown by our experimental resultsthat algorithm AT devised in this paper significantly outperformsthe prior works in the clustering quality as measuredby the validation indexes, indicating the usefulness ofassociation-taxonomy similarity in item data clustering.
ICDM	Cost-Sensitive Learning by Cost-Proportionate Example Weighting.	Bianca Zadrozny,John Langford,Naoki Abe	2003	We propose and evaluate a family of methods for convertingclassifier learning algorithms and classification theoryinto cost-sensitive algorithms and theory. The proposedconversion is based on cost-proportionate weighting of thetraining examples, which can be realized either by feedingthe weights to the classification algorithm (as often done inboosting), or by careful subsampling. We give some theoreticalperformance guarantees on the proposed methods,as well as empirical evidence that they are practical alternativesto existing approaches. In particular, we proposecosting, a method based on cost-proportionate rejectionsampling and ensemble aggregation, which achievesexcellent predictive performance on two publicly availabledatasets, while drastically reducing the computation requiredby other methods.
ICDM	CBC: Clustering Based Text Classification Requiring Minimal Labeled Data.	Hua-Jun Zeng,Xuanhui Wang,Zheng Chen,Hongjun Lu,Wei-Ying Ma	2003	Semi-supervised learning methods construct classifiersusing both labeled and unlabeled training data samples.While unlabeled data samples can help to improve theaccuracy of trained models to certain extent, existingmethods still face difficulties when labeled data is notsufficient and biased against the underlying datadistribution. In this paper, we present a clustering basedclassification (CBC) approach. Using this approach,training data, including both the labeled and unlabeleddata, is first clustered with the guidance of the labeleddata. Some of unlabeled data samples are then labeledbased on the clusters obtained. Discriminative classifierscan subsequently be trained with the expanded labeleddataset. The effectiveness of the proposed method isjustified analytically. Our experimental resultsdemonstrated that CBC outperforms existing algorithmswhen the size of labeled dataset is very small.
ICDM	Mining Plans for Customer-Class Transformation.	Qiang Yang,Hong Cheng	2003	We consider the problem of mining high-utility plansfrom historical plan databases that can be used to transformcustomers from one class to other, more desirable classes.Traditional data mining algorithms are focused on findingfrequent sequences. But high frequency may not imply lowcosts and high benefits. Traditional Markov Decision Process(MDP) algorithms are designed to address this issueby bringing in the concept of utility, but these algorithmsare also known to be expensive to execute. In this paper,we present a novel algorithm AUPlan which automaticallygenerates sequential plans with high utility by combiningdata mining and AI planning. These high-utility plans couldbe used to convert groups of customers from less desirablestates to more desirable ones. Our algorithm adapts theApriori algorithm by considering the concepts of plans andutilities. We show through empirical studies that planningusing our integrated algorithm produces high-utility plansefficiently.
ICDM	Regression Clustering.	Bin Zhang	2003	Complex distribution in real-world data is oftenmodeled by a mixture of simpler distributions. Clusteringis one of the tools to reveal the structure of this mixture.The same is true to the datasets with chosen responsevariables that people run regression on. Withoutseparating the clusters with very different responseproperties, the residue error of the regression is large.Input variable selection could also be misguided to ahigher complexity by the mixture. In RegressionClustering (RC), K (>1) regression functions are appliedto the dataset simultaneously which guide the clusteringof the dataset into K subsets each with a simplerdistribution matching its guiding function. Each functionis regressed on its own subset of data with a muchsmaller residue error. Both the regressions and theclustering optimize a common objective function. Wepresent a RC algorithm based on K-Harmonic Meansclustering algorithm and compare it with other existingRC algorithms based on K-Means and EM.
ICDM	On Precision and Recall of Multi-Attribute Data Extraction from Semistructured Sources.	Guizhen Yang,Saikat Mukherjee,I. V. Ramakrishnan	2003	Machine learning techniques for data extraction fromsemistructured sources exhibit different precision and recallcharacteristics. However to date the formal relationship betweenlearning algorithms and their impact on these twometrics remains unexplored. This paper proposes a formalizationof precision and recall of extraction and investigatesthe complexity-theoretic aspects of learning algorithms formulti-attribute data extraction based on this formalism. Weshow that there is a tradeoff between precision/recall of extractionand computational efficiency and present experimentalresults to demonstrate the practical utility of theseconcepts in designing scalable data extraction algorithmsfor improving recall without compromising on precision.
ICDM	Segmenting Customer Transactions Using a Pattern-Based Clustering Approach.	Yinghui Yang,Balaji Padmanabhan	2003	"Grouping customer transactions into categories helpsunderstand customers better. The marketing literaturehas concentrated on identifying important segmentationvariables (e.g. customer loyalty) and on using clusteringand mixture models for segmentation. The data miningliterature has provided various clustering algorithms forsegmentation. In this paper we investigate using""pattern-based"" clustering approaches to groupingcustomer transactions. We argue that there are clustersin transaction data based on natural behavioral patterns,and present a new technique, YACA, that groupstransactions such that itemsets generated from eachcluster, while similar to each other, are different fromones generated from others. We present experimentalresults from user-centric Web usage data thatdemonstrates that YACA generates a highly effectiveclustering of transactions."
ICDM	Postprocessing Decision Trees to Extract Actionable Knowledge.	Qiang Yang,Jie Yin,Charles X. Ling,Tielin Chen	2003	Most data mining algorithms and tools stop at discoveredcustomer models, producing distribution informationon customer profiles. Such techniques, when applied to industrialproblems such as customer relationship management(CRM), are useful in pointing out customers who arelikely attritors and customers who are loyal, but they requirehuman experts to postprocess the mined information manually.Most of the postprocessing techniques have been limitedto producing visualization results and interestingnessranking, but they do not directly suggest actions that wouldlead to an increase the objective function such as profit. Inthis paper, we present a novel algorithm that suggest actionsto change customers from an undesired status (suchas attritors) to a desired one (such as loyal) while maximizingobjective function: the expected net profit. We developthese algorithms under resource constraints that areabound in reality. The contribution of the work is in takingthe output from an existing mature technique (decisiontrees, for example), and producing novel, actionable knowledgethrough automatic postprocessing.
ICDM	Dimensionality Reduction Using Kernel Pooled Local Discriminant Information.	Peng Zhang,Jing Peng,Carlotta Domeniconi	2003	We study the use of kernel subspace methods for learninglow-dimensional representations for classification. We proposea kernel pooled local discriminant subspace methodand compare it against several competing techniques: generalizedFisher discriminant analysis (GDA) and kernelprincipal components analysis (KPCA) in classificationproblems. We evaluate the classification performance ofthe nearest-neighbor rule with each subspace representation.The experimental results demonstrate the efficacy ofthe kernel pooled local subspace method and the potentialfor substantial improvements over competing methods suchas KPCA in some classification problems.
ICDM	A new optimization criterion for generalized discriminant analysis on undersampled problems.	Jieping Ye,Ravi Janardan,Cheong Hee Park,Haesun Park	2003	A new optimization criterion for discriminant analysis ispresented. The new criterion extends the optimization criteriaof the classical linear discriminant analysis (LDA) byintroducing the pseudo-inverse when the scatter matricesare singular. It is applicable regardless of the relative sizesof the data dimension and sample size, overcoming a limitationof the classical LDA. Recently, a new algorithm calledLDA/GSVD for structure-preserving dimension reductionhas been introduced, which extends the classical LDA tovery high-dimensional undersampled problems by using thegeneralized singular value decomposition (GSVD). The solutionfrom the LDA/GSVD algorithm is a special case of thesolution for our generalized criterion in this paper, which isalso based on GSVD.We also present an approximate solution for our GSVD-basedsolution, which reduces computational complexity byfinding sub-clusters of each cluster, and using their centroidsto capture the structure of each cluster. This reducedproblem yields much smaller matrices of which the GSVDcan be applied efficiently. Experiments on text data, withup to 7000 dimensions, show that the approximation algorithmproduces results that are close to those produced bythe exact algorithm.
ICDM	Sentiment Analyzer: Extracting Sentiments about a Given Topic using Natural Language Processing Techniques.	Jeonghee Yi,Tetsuya Nasukawa,Razvan C. Bunescu,Wayne Niblack	2003	"We present Sentiment Analyzer (SA) that extracts sentiment(or opinion) about a subject from online text documents.Instead of classifying the sentiment of an entire documentabout a subject, SA detects all references to the givensubject, and determines sentiment in each of the referencesusing natural language processing (NLP) techniques. Oursentiment analysis consists of 1) a topic specific featureterm extraction, 2) sentiment extraction, and 3) (subject,sentiment) association by relationship analysis. SA utilizestwo linguistic resources for the analysis: the sentiment lexiconand the sentiment pattern database. The performanceof the algorithms was verified on online product review articles(""digital camera"" and ""music"" reviews), and moregeneral documents including general webpages and newsarticles."
ICDM	Detecting Patterns of Change Using Enhanced Parallel Coordinates Visualization.	Kaidi Zhao,Bing Liu,Thomas M. Tirpak,Andreas Schaller	2003	Analyzing data to find trends, correlations, and stablepatterns is an important problem for many industrialapplications. In this paper, we propose a new techniquebased on parallel coordinates visualization. Previous workon parallel coordinates methods has shown that they areeffective only when variables that are correlated and/orshow similar patterns are displayed adjacently. Althoughcurrent parallel coordinates tools allow the user tomanually rearrange the order of variables, this process isvery time-consuming when the number of variables islarge. Automated assistance is needed. This paperproposes an edit-distance based technique to rearrangevariables so that interesting patterns can be easilydetected. Our system, V-Miner, includes both automatedmethods for visualizing common patterns and a query toolthat enables the user to describe specific target patterns tobe mined/displayed by the system. Following an overviewof the system, a case study is presented to explain howMotorola engineers have used V-Miner to identifysignificant patterns in their product test and design data.
ICDM	Frequent-Pattern based Iterative Projected Clustering.	Man Lung Yiu,Nikos Mamoulis	2003	Irrelevant attributes add noise to high dimensional clustersand make traditional clustering techniques inappropriate.Projected clustering algorithms have been proposed to findthe clusters in hidden subspaces. We realize the analogy betweenmining frequent itemsets and discovering the relevantsubspace for a given cluster. We propose a methodology forfinding projected clusters by mining frequent itemsets andpresent heuristics that improve its quality. Our techniquesare evaluated with synthetic and real data; they are scalableand discover projected clusters accurately.
ICDM	General MC: Estimating Boundary of Positive Class from Small Positive Data.	Hwanjo Yu	2003	Single-Class Classification (SCC) seeks to distinguishone class of data from the universal set of multiple classes.We propose a SCC method called General MC that estimatesan accurate classification boundary of positive classfrom small positive data using the distribution of unlabeleddata. Our theoretical and empirical analyses show that,as long as the distribution of unlabeled data is not highlyskewed in the feature space, General MC significantly outperformsother recent SCC methods when the positive dataset is highly under-sampled.
ICDM	A Feature Selection Framework for Text Filtering.	Zhaohui Zheng,Rohini K. Srihari,Sargur N. Srihari	2003	This paper presents a new framework for local featureselection in text filtering. In this framework, a feature setis constructed per category by first selecting a set of termshighly indicative of membership (positive set) and anotherset of terms highly indicative of non-membership (negativeset), and then combining these two sets. This feature selectionframework not only unifies several standard featureselection methods, but also facilitates the proposal of a newmethod that optimally combines the positive and negativesets. The experimental comparison between the proposedmethod and standard methods was conducted on six featureselection metrics: chi-square, correlation coefficient, oddsratio, GSS coefficient and two proposed variants of odds ratioand GSS coefficient: OR-square and GSS-square respectively.The results show that the proposed feature selectionmethod improves text filtering performance.
ICDM	A K-NN Associated Fuzzy Evidential Reasoning Classifier with Adaptive Neighbor Selection.	Hongwei Zhu,Otman A. Basir	2003	The paper presents a fuzzy evidential reasoning algorithmin light of the Dempster-Shafer evidence theory andthe K-nearest neighbor algorithm for pattern classification.Given an input pattern to be classified, each of its K nearestneighbors is viewed as an evidence source, in terms ofa fuzzy evidence structure. The distance between the inputpattern and each of its K nearest neighbors is usedfor mass determination while the contextual information ofthe nearest neighbor in the training sample space is formulatedby a fuzzy set in determining a fuzzy focal element.Therefore, pooling evidence provided by neighbors is realizedby a fuzzy evidential reasoning, where feature selectionis further considered through ranking and adaptive combinationof neighbors. A fast implementation scheme of thefuzzy evidential reasoning is also developed. Experimentalresults of classifying multi-channel remote sensing imageshave shown that the proposed approach outperforms the K-nearestneighbor (K-NN) algorithm [1], the fuzzy K-nearestneighbor (F-KNN) algorithm [2], the evidence-theoretic K-nearestneighbor (E-KNN) algorithm [3], and the fuzzy ex-tendedversion of E-KNN (FE-KNN) [4], in terms of theclassification accuracy and insensitivity to the number Kof nearest neighbors.
ICDM	Proceedings of the 3rd IEEE International Conference on Data Mining (ICDM 2003), 19-22 December 2003, Melbourne, Florida, USA		2003	Proceedings of the 3rd IEEE International Conference on Data Mining (ICDM 2003), 19-22 December 2003, Melbourne, Florida, USA
KDD	Adaptive duplicate detection using learnable string similarity measures.	Mikhail Bilenko,Raymond J. Mooney	2003	"The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes. Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates. In this paper, we present a framework for improving duplicate detection using trainable measures of textual similarity. We propose to employ learnable text distance functions for each database field, and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field's domain. We present two learnable text similarity measures suitable for this task: an extended variant of learnable string edit distance, and a novel vector-space based measure that employs a Support Vector Machine (SVM) for training. Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques."
KDD	An iterative hypothesis-testing strategy for pattern discovery.	Richard J. Bolton,Niall M. Adams	2003	"Pattern discovery has emerged as a direct result of increased data storage and analytic capabilities available to the data analyst. Without a massive amount of data, we do not have the evidence to support the discovery of the local deterministic structures that we call patterns. As such, pattern discovery is one of the few areas of data mining that cannot be considered simply as a 'scaling-up' of current statistical methodology to analyze large data sets. However, the philosophies of hypothesis testing and modeling in traditional statistics do lend themselves to forming a framework for pattern discovery, and we can also draw from ideas relating to outlier discovery and residual analysis to discover patterns. We illustrate an iterative strategy in a statistical framework by way of its application to one simulated and two real data sets."
KDD	Generative model-based clustering of directional data.	Arindam Banerjee,Inderjit S. Dhillon,Joydeep Ghosh,Suvrit Sra	2003	High dimensional directional data is becoming increasingly important in contemporary applications such as analysis of text and gene-expression data. A natural model for multi-variate directional data is provided by the von Mises-Fisher (vMF) distribution on the unit hypersphere that is analogous to the multi-variate Gaussian distribution in Rd. In this paper, we propose modeling complex directional data as a mixture of vMF distributions. We derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the parameters of this mixture. We also propose two clustering algorithms corresponding to these variants. An interesting aspect of our methodology is that the spherical kmeans algorithm (kmeans with cosine similarity) can be shown to be a special case of both our algorithms. Thus, modeling text data by vMF distributions lends theoretical validity to the use of cosine similarity which has been widely used by the information retrieval community. As part of experimental validation, we present results on modeling high-dimensional text and gene-expression data as a mixture of vMF distributions. The results indicate that our approach yields superior clusterings especially for difficult clustering tasks in high-dimensional spaces.
KDD	Architecting a knowledge discovery engine for military commanders utilizing massive runs of simulations.	Philip S. Barry,Jianping Zhang,Mary McDonald	2003	"The Marine Corps' Project Albert seeks to model complex phenomenon by observing the behavior of relatively simple simulations over thousands of runs. A rich data base is developed by running the simulations thousands of times, varying the agent and scenario input parameters as well as the random seeds. Exploring this result space may provide significant insight into nonlinear, surprising, and emergent behaviors. Capturing these results can provide a path for making the results usable for decision support to a military commander. This paper presents two data mining approaches, rule discovery and Bayesian networks, for analyzing the Albert simulation data. The first approach generates rules from the data and then uses them to create descriptive model. The second generates Bayesian Networks which provide a quantitative belief model for decision support. Both of these approaches as well as the Project Albert simulations are framed in the context of a system architecture for decision support."
KDD	Using randomized response techniques for privacy-preserving data mining.	Wenliang Du,Zhijun Zhan	2003	Privacy is an important issue in data mining and knowledge discovery. In this paper, we propose to use the randomized response techniques to conduct the data mining computation. Specially, we present a method to build decision tree classifiers from the disguised data. We conduct experiments to compare the accuracy of our decision tree with the one built from the original undisguised data. Our results show that although the data are disguised, our method can still achieve fairly high accuracy. We also show how the parameter used in the randomized response techniques affects the accuracy of the results.
KDD	Mining distance-based outliers in near linear time with randomization and a simple pruning rule.	Stephen D. Bay,Mark Schwabacher	2003	Defining outliers by their distance to neighboring examples is a popular approach to finding unusual examples in a data set. Recently, much work has been conducted with the goal of finding fast algorithms for this task. We show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used. We test our algorithm on real high-dimensional data sets with millions of examples and show that the near linear scaling holds over several orders of magnitude. Our average case analysis suggests that much of the efficiency is because the time to process non-outliers, which are the majority of examples, does not depend on the size of the data set.
KDD	Towards systematic design of distance functions for data mining applications.	Charu C. Aggarwal	2003	Distance function computation is a key subtask in many data mining algorithms and applications. The most effective form of the distance function can only be expressed in the context of a particular data domain. It is also often a challenging and non-trivial task to find the most effective form of the distance function. For example, in the text domain, distance function design has been considered such an important and complex issue that it has been the focus of intensive research over three decades. The final design of distance functions in this domain has been reached only by detailed empirical testing and consensus over the quality of results provided by the different variations. With the increasing ability to collect data in an automated way, the number of new kinds of data continues to increase rapidly. This makes it increasingly difficult to undertake such efforts for each and every new data type. The most important aspect of distance function design is that since a human is the end-user for any application, the design must satisfy the user requirements with regard to effectiveness. This creates the need for a systematic framework to design distance functions which are sensitive to the particular characteristics of the data domain. In this paper, we discuss such a framework. The goal is to create distance functions in an automated waywhile minimizing the work required from the user. We will show that this framework creates distance functions which are significantly more effective than popularly used functions such as the Euclidean metric.
KDD	Golden Path Analyzer: using divide-and-conquer to cluster Web clickstreams.	Kamal Ali,Steven P. Ketchpel	2003	"This paper describes a novel algorithm and deployed system Golden Path Analyzer (GPA) that analyzes clickstreams of people trying to complete the same task on a website. It finds the shortest, successful paths taken by users - 'golden paths' - and uses these as seeds for clickstream clusters. Other users are assigned to a cluster if their clickstream is a supersequence of the golden path. The advantages of this approach are that the resulting clusters are easily comprehended, they are few in number, correspond to semantically different strategies used by the users, and jointly partition all the clickstreams. GPA's key contribution over prior work in process funnels is that by not excluding users that make diversions from the golden path, GPA is able to assign more users to fewer clusters. Another key contribution is to use actual full clickstreams as cluster seeds to which supersequences of other users are added. Golden paths correspond to complete clickstreams that are based on actual user page transitions. GPA is particularly useful for site designers to improve processes such as shopping, returns and registration. Its analyses identify which web pages cause many users to deviate from a golden path, which links distract users and the percentage of users taking each golden path. GPA has demonstrated value on more than twenty client projects in diverse industries."
KDD	An adaptive nearest neighbor search for a parts acquisition ePortal.	Rafael Alonso,Jeffrey A. Bloom,Hua Li,Chumki Basu	2003	"One of the major hurdles in maintaining long-lived electronic systems is that electronic parts become obsolete, no longer available from the original suppliers. When this occurs, an engineer is tasked with resolving the problem by finding a replacement that is ""as similar as possible"" to the original part. The current approach involves a laborious manual search through several electronic portals and data books. The search is difficult because potential replacements may differ from the original and from each other by one or more parameters. Worse still, the cumbersome nature of this process may cause the engineers to miss appropriate solutions amid the many thousands of parts listed in industry catalogs.In this paper, we address this problem by introducing the notion of a parametric ""distance"" between electronic components. We use this distance to search a large parts data set and recommend likely replacements. Recommendations are based on an adaptive nearest-neighbor search through the parametric data set. For each user, we learn how to scale the axes of the feature space in which the nearest neighbors are sought. This allows the system to learn each user's judgment of the phrase ""as similar as possible."""
KDD	Mining high dimensional data for classifier knowledge.	Raj Bhatnagar,Goutham Kurra,Wen Niu	2003	We present in this paper the problem of discovering sets of attribute-value pairs in high dimensional data sets that are of interest not because of co-occurrence alone, but due to their value in serving as cores for potential classifiers of clusters. We present our algorithm in the context of a gene-expression dataset. Gene expression data, in most situations, is insufficient for clustering algorithms and any statistical inference because for 6000+ genes, typically only 10s and at most 100s of data points become available. It is difficult to use statistical techniques to design a classifier for such immensely under-specified data. The observed data, though statistically, insufficient contains some information about the domain. Our goal is to discover as much information about all potential classifiers as possible from the data and then summarize this knowledge. This summarization provides insights into the composition of potential classifiers. We present here algorithms and methods for mining a high dimensional data set, exemplified by a gene expression data set, for mining such information.
KDD	Style mining of electronic messages for multiple authorship discrimination: first results.	Shlomo Argamon,Marin Saric,Sterling Stuart Stein	2003	"This paper considers the use of computational stylistics for performing authorship attribution of electronic messages, addressing categorization problems with as many as 20 different classes (authors). Effective stylistic characterization of text is potentially useful for a variety of tasks, as language style contains cues regarding the authorship, purpose, and mood of the text, all of which would be useful adjuncts to information retrieval or knowledge-management tasks. We focus here on the problem of determining the author of an anonymous message, based only on the message text. Several multiclass variants of the Winnow algorithm were applied to a vector representation of the message texts to learn models for discriminating different authors. We present results comparing the classification accuracy of the different approaches. The results show that stylistic models can be accurately learned to determine an author's identity."
KDD	Understanding captions in biomedical publications.	William W. Cohen,Richard C. Wang,Robert F. Murphy	2003	"From the standpoint of the automated extraction of scientific knowledge, an important but little-studied part of scientific publications are the figures and accompanying captions. Captions are dense in information, but also contain many extra-grammatical constructs, making them awkward to process with standard information extraction methods. We propose a scheme for ""understanding"" captions in biomedical publications by extracting and classifying ""image pointers"" (references to the accompanying image). We evaluate a number of automated methods for this task, including hand-coded methods, methods based on existing learning techniques, and methods based on novel learning techniques. The best of these methods leads to a usefully accurate tool for caption-understanding, with both recall and precision in excess of 94% on the most important single class in a combined extraction/classification task."
KDD	Efficient data reduction with EASE.	Hervé Brönnimann,Bin Chen,Manoranjan Dash,Peter J. Haas,Peter Scheuermann	2003	"A variety of mining and analysis problems --- ranging from association-rule discovery to contingency table analysis to materialization of certain approximate datacubes --- involve the extraction of knowledge from a set of categorical count data. Such data can be viewed as a collection of ""transactions,"" where a transaction is a fixed-length vector of counts. Classical algorithms for solving count-data problems require one or more computationally intensive passes over the entire database and can be prohibitively slow. One effective method for dealing with this ever-worsening scalability problem is to run the algorithms on a small sample of the data. We present a new data-reduction algorithm, called EASE, for producing such a sample. Like the FAST algorithm introduced by Chen et al., EASE is especially designed for count data applications. Both EASE and FAST take a relatively large initial random sample and then deterministically produce a subsample whose ""distance"" --- appropriately defined --- from the complete database is minimal. Unlike FAST, which obtains the final subsample by quasi-greedy descent, EASE uses epsilon-approximation methods to obtain the final subsample by a process of repeated halving. Experiments both in the context of association rule mining and classical &chi;2 contingency-table analysis show that EASE outperforms both FAST and simple random sampling, sometimes dramatically."
KDD	Probabilistic discovery of time series motifs.	Bill Yuan-chi Chiu,Eamonn J. Keogh,Stefano Lonardi	2003	"Several important time series data mining problems reduce to the core task of finding approximately repeated subsequences in a longer time series. In an earlier work, we formalized the idea of approximately repeated subsequences by introducing the notion of time series motifs. Two limitations of this work were the poor scalability of the motif discovery algorithm, and the inability to discover motifs in the presence of noise.Here we address these limitations by introducing a novel algorithm inspired by recent advances in the problem of pattern discovery in biosequences. Our algorithm is probabilistic in nature, but as we show empirically and theoretically, it can find time series motifs with very high probability even in the presence of noise or ""don't care"" symbols. Not only is the algorithm fast, but it is an anytime algorithm, producing likely candidate motifs almost immediately, and gradually improving the quality of results over time."
KDD	Data quality through knowledge engineering.	Tamraparni Dasu,Gregg T. Vesonder,Jon R. Wright	2003	Traditionally, data quality programs have acted as a preprocessing stage to make data suitable for a data mining or analysis operation. Recently, data quality concepts have been applied to databases that support business operations such as provisioning and billing. Incorporating business rules that drive operations and their associated data processes is critically important to the success of such projects. However, there are many practical complications. For example, documentation on business rules is often meager. Rules change frequently. Domain knowledge is often fragmented across experts, and those experts do not always agree. Typically, rules have to be gathered from subject matter experts iteratively, and are discovered out of logical or procedural sequence, like a jigsaw puzzle. Our approach is to impement business rules as constraints on data in a classical expert system formalism sometimes called production rules. Our system works by allowing good data to pass through a system of constraints unchecked. Bad data violate constraints and are flagged, and then fed back after correction. Constraints are added incrementally as better understanding of the business rules is gained. We include a real-life case study.
KDD	Extracting semantics from data cubes using cube transversals and closures.	Alain Casali,Rosine Cicchetti,Lotfi Lakhal	2003	In this paper we propose a lattice-based approach intended for extracting semantics from datacubes: borders of version spaces for supervised classification, closed cube lattice to summarize the semantics of datacubes w.r.t. COUNT, SUM, and covering graph of the quotient cube as a visualization tool of minimal multidimensional associations. With this intention, we introduce two novel concepts: the cube transversals and the cube closures over the cube lattice of a categorical database relation. We propose a levelwise merging algorithm for mining minimal cube transversals with a single database scan. We introduce the cube connection, show that it is a Galois connection and derive a closure operator over the cube lattice. Using cube transversals and closures, we define a new characterization of boundary sets which provide a condensed representation of version spaces used to enhance supervised classification. The algorithm designed for computing such borders improves the complexity of previous proposals. We also introduce the concept of closed cube lattice and show that it is isomorph to on one hand the Galois lattice and on the other hand the quotient cube w.r.t. COUNT, SUM. Proposed in [16], the quotient cube is a succinct summary of a datacube preserving the Rollup/Drilldown semantics. We show that the quotient cube w.r.t. COUNT, SUM and the closed cube lattice have a similar expression power but the latter has the smallest possible size. Finally we focus on the multidimensional association issue and introduce the covering graph of the quotient cube which provides the user with a visualization tool of minimal multidimensional associations.
KDD	Translation-invariant mixture models for curve clustering.	Darya Chudova,Scott Gaffney,Eric Mjolsness,Padhraic Smyth	2003	In this paper we present a family of algorithms that can simultaneously align and cluster sets of multidimensional curves defined on a discrete time grid. Our approach uses the Expectation-Maximization (EM) algorithm to recover both the mean curve shapes for each cluster, and the most likely shifts, offsets, and cluster memberships for each curve. We demonstrate how Bayesian estimation methods can improve the results for small sample sizes by enforcing smoothness in the cluster mean curves. We evaluate the methodology on two real-world data sets, time-course gene expression data and storm trajectory data. Experimental results show that models that incorporate curve alignment systematically provide improvements in predictive power and within-cluster variance on test data sets. The proposed approach provides a non-parametric, computationally efficient, and robust methodology for clustering broad classes of curve data.
KDD	Finding recent frequent itemsets adaptively over online data streams.	Joong Hyuk Chang,Won Suk Lee	2003	A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate. Consequently, the knowledge embedded in a data stream is more likely to be changed as time goes by. Identifying the recent change of a data stream, specially for an online data stream, can provide valuable information for the analysis of the data stream. In addition, monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge. However, most of mining algorithms over a data stream do not differentiate the information of recently generated transactions from the obsolete information of old transactions which may be no longer useful or possibly invalid at present. This paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream. The effect of old transactions on the mining result of the data steam is diminished by decaying the old occurrences of each itemset as time goes by. Furthermore, several optimization techniques are devised to minimize processing time as well as main memory usage. Finally, the proposed method is analyzed by a series of experiments.
KDD	Information-theoretic co-clustering.	Inderjit S. Dhillon,Subramanyam Mallela,Dharmendra S. Modha	2003	Two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text, web-log and market-basket data analysis. A basic problem in contingency table analysis is co-clustering: simultaneous clustering of the rows and columns. A novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in information theory---the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters. We present an innovative co-clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages. Using the practical example of simultaneous word-document clustering, we demonstrate that our algorithm works well in practice, especially in the presence of sparsity and high-dimensionality.
KDD	Applications of sampling and fractional factorial designs to model-free data squashing.	William DuMouchel,Deepak K. Agarwal	2003	"The concept of ""data squashing"" was introduced by DuMouchel et al [4] as a method of summarizing massive data sets that preserves statistical relationships among variables. The idea is to create a smaller data set that allows statistical modeling to take place using in-memory algorithms, and to preserve the modeling results more accurately than would a same-size random sample from the massive data set. This research attempts to avoid several limitations of previous approaches to data squashing. Our method avoids the curse of dimensionality by a double use of principal components transformations that makes computing time linear in the number of cases and quadratic in the number of variables. Categorical and continuous variables are smoothly integrated. Because the binning is based on principal components, which are uncorrelated, we can use fractional factorial designs that sample less than one point per bin. We also investigate various weighting schemes for the squashed sample to see whether matching moments or matching subregion data counts is more effective. Finally, previous work required the specification of a statistical model, either to perform the squashing algorithm or to compare the worth of different squashing methods. Our approach to evaluation is model free and does not even require the specification of variables as responses or predictors. Instead, we develop a chi-squared like measure of accuracy to compare the closeness of various discrete densities (the squashed data sets) to the discrete massive data set."
KDD	SEWeP: using site semantics and a taxonomy to enhance the Web personalization process.	Magdalini Eirinaki,Michalis Vazirgiannis,Iraklis Varlamis	2003	"Web personalization is the process of customizing a Web site to the needs of each specific user or set of users, taking advantage of the knowledge acquired through the analysis of the user's navigational behavior. Integrating usage data with content, structure or user profile data enhances the results of the personalization process. In this paper, we present SEWeP, a system that makes use of both the usage logs and the semantics of a Web site's content in order to personalize it. Web content is semantically annotated using a conceptual hierarchy (taxonomy). We introduce C-logs, an extended form of Web usage logs that encapsulates knowledge derived from the link semantics. C-logs are used as input to the Web usage mining process, resulting in a broader yet semantically focused set of recommendations."
KDD	Inverted matrix: efficient discovery of frequent items in large datasets in the context of interactive mining.	Mohammad El-Hajj,Osmar R. Zaïane	2003	Existing association rule mining algorithms suffer from many problems when mining massive transactional datasets. One major problem is the high memory dependency: either the gigantic data structure built is assumed to fit in main memory, or the recursive mining process is too voracious in memory resources. Another major impediment is the repetitive and interactive nature of any knowledge discovery process. To tune parameters, many runs of the same algorithms are necessary leading to the building of these huge data structures time and again. This paper proposes a new disk-based association rule mining algorithm called Inverted Matrix, which achieves its efficiency by applying three new ideas. First, transactional data is converted into a new database layout called Inverted Matrix that prevents multiple scanning of the database during the mining phase, in which finding frequent patterns could be achieved in less than a full scan with random access. Second, for each frequent item, a relatively small independent tree is built summarizing co-occurrences. Finally, a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed. Experimental studies reveal that our Inverted Matrix approach outperform FP-Tree especially in mining very large transactional databases with a very large number of unique items. Our random access disk-based approach is particularly advantageous in a repetitive and interactive setting.
KDD	Accurate decision trees for mining high-speed data streams.	João Gama,Ricardo Rocha,Pedro Medas	2003	In this paper we study the problem of constructing accurate decision tree models from data streams. Data streams are incremental tasks that require incremental, online, and any-time learning algorithms. One of the most successful algorithms for mining data streams is VFDT. In this paper we extend the VFDT system in two directions: the ability to deal with continuous data and the use of more powerful classification techniques at tree leaves. The proposed system, VFDTc, can incorporate and classify new information online, with a single scan of the data, in time constant per example. The most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets. This is relevant due to the any-time property. We study the behaviour of VFDTc in different problems and demonstrate its utility in large and medium data sets. Under a bias-variance analysis we observe that VFDTc in comparison to C4.5 is able to reduce the variance component.
KDD	To buy or not to buy: mining airfare data to minimize ticket purchase price.	Oren Etzioni,Rattapoom Tuchinda,Craig A. Knoblock,Alexander Yates	2003	"As product prices become increasingly available on the World Wide Web, consumers attempt to understand how corporations vary these prices over time. However, corporations change prices based on proprietary algorithms and hidden variables (e.g., the number of unsold seats on a flight). Is it possible to develop data mining techniques that will enable consumers to predict price changes under these conditions?This paper reports on a pilot study in the domain of airline ticket prices where we recorded over 12,000 price observations over a 41 day period. When trained on this data, Hamlet --- our multi-strategy data mining algorithm --- generated a predictive model that saved 341 simulated passengers $198,074 by advising them when to buy and when to postpone ticket purchases. Remarkably, a clairvoyant algorithm with complete knowledge of future prices could save at most $320,572 in our simulation, thus HAMLET's savings were 61.8% of optimal. The algorithm's savings of $198,074 represents an average savings of 23.8% for the 341 passengers for whom savings are possible. Overall, HAMLET saved 4.4% of the ticket price averaged over the entire set of 4,488 simulated passengers. Our pilot study suggests that mining of price data available over the web has the potential to save consumers substantial sums of money per annum."
KDD	Online novelty detection on temporal sequences.	Junshui Ma,Simon Perkins	2003	In this paper, we present a new framework for online novelty detection on temporal sequences. This framework include a mechanism for associating each detection result with a confidence value. Based on this framework, we develop a concrete online detection algorithm, by modeling the temporal sequence using an online support vector regression algorithm. Experiments on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm.
KDD	Fragments of order.	Aristides Gionis,Teija Kujala,Heikki Mannila	2003	High-dimensional collections of 0--1 data occur in many applications. The attributes in such data sets are typically considered to be unordered. However, in many cases there is a natural total or partial order &pr; underlying the variables of the data set. Examples of variables for which such orders exist include terms in documents, courses in enrollment data, and paleontological sites in fossil data collections. The observations in such applications are flat, unordered sets; however, the data sets respect the underlying ordering of the variables. By this we mean that if A &pr; B &pr; C are three variables respecting the underlying ordering &pr;, and both of variables A and C appear in an observation, then, up to noise levels, variable B also appears in this observation. Similarly, if A1 &pr; A2 &pr; &hellip; &pr; Al-1 &pr; Ai is a longer sequence of variables, we do not expect to see many observations for which there are indices i < j < k such that Ai and Ak occur in the observation but Aj does not.In this paper we study the problem of discovering fragments of orders of variables implicit in collections of unordered observations. We define measures that capture how well a given order agrees with the observed data. We describe a simple and efficient algorithm for finding all the fragments that satisfy certain conditions. We also discuss the sometimes necessary postprocessing for selecting only the best fragments of order. Also, we relate our method with a sequencing approach that uses a spectral algorithm, and with the consecutive ones problem. We present experimental results on some real data sets (author lists of database papers, exam results data, and paleontological data).
KDD	On-line science: the world-wide telescope as a prototype for the new computational science.	Jim Gray	2003	On-line science: the world-wide telescope as a prototype for the new computational science.
KDD	Correlating synchronous and asynchronous data streams.	Sudipto Guha,Dimitrios Gunopulos,Nick Koudas	2003	In a variety of modern mining applications, data are commonly viewed as infinite time ordered data streams rather as finite data sets stored on disk. This view challenges fundamental assumptions commonly made in the context of several data mining algorithms.In this paper, we study the problem of identifying correlations between multiple data streams. In particular, we propose algorithms capable of capturing correlations between multiple continuous data streams in a highly efficient and accurate manner. Our algorithms and techniques are applicable in the case of both synchronous and asynchronous data streaming environments. We capture correlations between multiple streams using the well known technique of Singular Value Decomposition (SVD). Correlations between data items, and the SVD technique in particular, have been repeatedly utilized in an off-line (non stream) data mining problems, for example forecasting, approximate query answering, and data reduction.We propose a methodology based on a combination of dimensionality reduction and sampling to make the SVD technique suitable for a data stream context. Our techniques are approximate, trading accuracy with performance, and we analytically quantify this tradeoff. We present a through experimental evaluation, using both real and synthetic data sets, from a prototype implementation of our technique, investigating the impact of various parameters in the accuracy of the overall computation. Our results indicate, that correlations between multiple data streams can be identified very efficiently and accurately. The algorithms proposed herein, are presented as generic tools, with a multitude of applications on data stream mining problems.
KDD	A Web page prediction model based on click-stream tree representation of user behavior.	Sule Gündüz,M. Tamer Özsu	2003	A Web page prediction model based on click-stream tree representation of user behavior.
KDD	Experiments with random projections for machine learning.	Dmitriy Fradkin,David Madigan	2003	Dimensionality reduction via Random Projections has attracted considerable attention in recent years. The approach has interesting theoretical underpinnings and offers computational advantages. In this paper we report a number of experiments to evaluate Random Projections in the context of inductive supervised learning. In particular, we compare Random Projections and PCA on a number of different datasets and using different machine learning methods. While we find that the random projection approach predictively underperforms PCA, its computational advantages may make it attractive for certain applications.
KDD	Empirical Bayesian data mining for discovering patterns in post-marketing drug safety.	David M. Fram,June S. Almenoff,William DuMouchel	2003	"Because of practical limits in characterizing the safety profiles of therapeutic products prior to marketing, manufacturers and regulatory agencies perform post-marketing surveillance based on the collection of adverse reaction reports (""pharmacovigilance"").The resulting databases, while rich in real-world information, are notoriously difficult to analyze using traditional techniques. Each report may involve multiple medicines, symptoms, and demographic factors, and there is no easily linked information on drug exposure in the reporting population. KDD techniques, such as association finding, are well-matched to the problem, but are difficult for medical staff to apply and interpret.To deploy KDD effectively for pharmacovigilance, Lincoln Technologies and GlaxoSmithKline collaborated to create a webbased safety data mining web environment. The analytical core is a high-performance implementation of the MGPS (Multi-Item Gamma Poisson Shrinker) algorithm described previously by DuMouchel and Pregibon, with several significant extensions and enhancements. The environment offers an interface for specifying data mining runs, a batch execution facility, tabular and graphical methods for exploring associations, and drilldown to case details. Substantial work was involved in preparing the raw adverse event data for mining, including harmonization of drug names and removal of duplicate reports.The environment can be used to explore both drug-event and multi-way associations (interactions, syndromes). It has been used to study age/gender effects, to predict the safety profiles of proposed combination drugs, and to separate contributions of individual drugs to safety problems in polytherapy situations."
KDD	Mining hepatitis data with temporal abstraction.	Tu Bao Ho,Trong Dung Nguyen,Saori Kawasaki,Si Quang Le,DucDung Nguyen,Hideto Yokoi,Katsuhiko Takabayashi	2003	The hepatitis temporal database collected at Chiba university hospital between 1982--2001 was recently given to challenge the KDD research. The database is large where each patient corresponds to 983 tests represented as sequences of irregular timestamp points with different lengths. This paper presents a temporal abstraction approach to mining knowledge from this hepatitis database. Exploiting hepatitis background knowledge and data analysis, we introduce new notions and methods for abstracting short-term changed and long-term changed tests. The abstracted data allow us to apply different machine learning methods for finding knowledge part of which is considered as new and interesting by medical doctors.
KDD	Natural communities in large linked networks.	John E. Hopcroft,Omar Khan,Brian Kulis,Bart Selman	2003	We are interested in finding natural communities in large-scale linked networks. Our ultimate goal is to track changes over time in such communities. For such temporal tracking, we require a clustering algorithm that is relatively stable under small perturbations of the input data. We have developed an efficient, scalable agglomerative strategy and applied it to the citation graph of the NEC CiteSeer database (250,000 papers; 4.5 million citations). Agglomerative clustering techniques are known to be unstable on data in which the community structure is not strong. We find that some communities are essentially random and thus unstable while others are natural and will appear in most clusterings. These natural communities will enable us to track the evolution of communities over time.
KDD	Navigating massive data sets via local clustering.	Michael E. Houle	2003	This paper introduces a scalable method for feature extraction and navigation of large data sets by means of local clustering, where clusters are modeled as overlapping neighborhoods. Under the model, intra-cluster association and external differentiation are both assessed in terms of a natural confidence measure. Minor clusters can be identified even when they appear in the intersection of larger clusters. Scalability of local clustering derives from recent generic techniques for efficient approximate similarity search. The cluster overlap structure gives rise to a hierarchy that can be navigated and queried by users. Experimental results are provided for two large text databases.
KDD	Mining viewpoint patterns in image databases.	Wynne Hsu,Jing Dai,Mong-Li Lee	2003	The increasing number of image repositories has made image mining an important task because of its potential in discovering useful image patterns from a large set of images. In this paper, we introduce the notion of viewpoint patterns for image databases. Viewpoint patterns refer to patterns that capture the invariant relationships of one object from the point of view of another object. These patterns are unique and significant in images because the absolute positional information of objects for most images is not important, but rather, it is the relative distance and orientation of the objects from each other that is meaningful. We design a scalable and efficient algorithm to discover such viewpoint patterns. Experiments results on various image sets demonstrate that viewpoint patterns are meaningful and interesting to human users.
KDD	Information awareness: a prospective technical assessment.	David Jensen,Matthew J. Rattigan,Hannah Blau	2003	Recent proposals to apply data mining systems to problems in law enforcement, national security, and fraud detection have attracted both media attention and technical critiques of their expected accuracy and impact on privacy. Unfortunately, the majority of technical critiques have been based on simplistic assumptions about data, classifiers, inference procedures, and the overall architecture of such systems. We consider these critiques in detail, and we construct a simulation model that more closely matches realistic systems. We show how both the accuracy and privacy impact of a hypothetical system could be substantially improved, and we discuss the necessary and sufficient conditions for this improvement to be achieved. This analysis is neither a defense nor a critique of any particular system concept. Rather, our model suggests alternative technical designs that could mitigate some concerns, but also raises more specific conditions that must be met for such systems to be both accurate and socially desirable.
KDD	Playing hide-and-seek with correlations.	Chris Jermaine	2003	"We present a method for very high-dimensional correlation analysis. The method relies equally on rigorous search strategies and on human interaction. At each step, the method conservatively ""shaves off"" a fraction of the database tuples and attributes, so that most of the correlations present in the data are not affected by the decomposition. Instead, the correlations become more obvious to the user, because they are hidden in a much smaller portion of the database. This process can be repeated iteratively and interactively, until only the most important correlations remain.The main technical difficulty of the approach is figuring out how to ""shave off"" part of the database so as to preserve most correlations. We develop an algorithm for this problem that has a polynomial running time and guarantees result quality."
KDD	Interactive exploration of coherent patterns in time-series gene expression data.	Daxin Jiang,Jian Pei,Aidong Zhang	2003	Discovering coherent gene expression patterns in time-series gene expression data is an important task in bioinformatics research and biomedical applications. In this paper, we propose an interactive exploration framework for mining coherent expression patterns in time-series gene expression data. We develop a novel tool, coherent pattern index graph, to give users highly confident indications of the existences of coherent patterns. To derive a coherent pattern index graph, we devise an attraction tree structure to record the genes in the data set and summarize the information needed for the interactive exploration. We present fast and scalable algorithms to construct attraction trees and coherent pattern index graphs from gene expression data sets. We conduct an extensive performance study on some real data sets to verify our design. The experimental results strongly show that our approach is more effective than the state-of-the-art methods in mining real gene expression data, and is scalable in mining large data sets.
KDD	Efficient decision tree construction on streaming data.	Ruoming Jin,Gagan Agrawal	2003	Decision tree construction is a well studied problem in data mining. Recently, there has been much interest in mining streaming data. Domingos and Hulten have presented a one-pass algorithm for decision tree construction. Their work uses Hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed.In this paper, we revisit this problem. We make the following two contributions: 1) We present a numerical interval pruning (NIP) approach for efficiently processing numerical attributes. Our results show an average of 39% reduction in execution times. 2) We exploit the properties of the gain function entropy (and gini) to reduce the sample size required for obtaining a given bound on the accuracy. Our experimental results show a 37% reduction in the number of data instances required.
KDD	A bag of paths model for measuring structural similarity in Web documents.	Sachindra Joshi,Neeraj Agrawal,Raghu Krishnapuram,Sumit Negi	2003	Structural information (such as layout and look-and-feel) has been extensively used in the literatuce for extraction of interesting or relevant data, efficient storage, and query optimization. Traditionally, tree models (such as DOM trees) have been used to represent structural information, especially in the case of HTML and XML documents. However, computation of structural similarity between documents based on the tree model is computationally expensive. In this paper, we propose an alternative scheme for representing the structural information of documents based on the paths contained in the corresponding tree model. Since the model includes partial information about parents, children and siblings, it allows us to define a new family of meaningful (and at the same time computationally simple) structural similarity measures. Our experimental results based on the SIGMOD XML data set as well as HTML document collections from ibm.com, dell.com, and amazon.com show that the representation is powerful enough to produce good clusters of structurally similar pages.
KDD	Nantonac collaborative filtering: recommendation based on order responses.	Toshihiro Kamishima	2003	"A recommender system suggests the items expected to be preferred by the users. Recommender systems use collaborative filtering to recommend items by summarizing the preferences of people who have tendencies similar to the user preference. Traditionally, the degree of preference is represented by a scale, for example, one that ranges from one to five. This type of measuring technique is called the semantic differential (SD) method. Web adopted the ranking method, however, rather than the SD method, since the SD method is intrinsically not suited for representing individual preferences. In the ranking method, the preferences are represented by orders, which are sorted item sequences according to the users' preferences. We here propose some methods to recommed items based on these order responses, and carry out the comparison experiments of these methods."
KDD	Maximizing the spread of influence through a social network.	David Kempe,Jon M. Kleinberg,Éva Tardos	2003	"Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of ""word of mouth"" in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target?We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here, and we provide the first provable approximation guarantees for efficient algorithms. Using an analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63% of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks.We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks."
KDD	Statistical learning from relational data.	Daphne Koller	2003	Statistical learning from relational data.
KDD	A two-way visualization method for clustered data.	Yehuda Koren,David Harel	2003	We describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low-dimensional embedding, thereby gaining the benefits of both approaches. In a single image one can view all the clusters, examine the relations between them and study many of their properties. The method is based on an algorithm for low-dimensional embedding of clustered data, with the property that separation between all clusters is guaranteed, regardless of their nature. In particular, the algorithm was designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data, so that every two disjoint clusters in the hierarchy are drawn separately.
KDD	PROXIMUS: a framework for analyzing very high dimensional discrete-attributed datasets.	Mehmet Koyutürk,Ananth Grama	2003	This paper presents an efficient framework for error-bounded compression of high-dimensional discrete attributed datasets. Such datasets, which frequently arise in a wide variety of applications, pose some of the most significant challenges in data analysis. Subsampling and compression are two key technologies for analyzing these datasets. PROXIMUS provides a technique for reducing large datasets into a much smaller set of representative patterns, on which traditional (expensive) analysis algorithms can be applied with minimal loss of accuracy. We show desirable properties of PROXIMUS in terms of runtime, scalability to large datasets, and performance in terms of capability to represent data in a compact form. We also demonstrate applications of PROXIMUS in association rule mining. In doing so, we establish PROXIMUS as a tool for preprocessing data before applying computationally expensive algorithms or as a tool for directly extracting correlated patterns. Our experimental results show that use of the compressed data for association rule mining provides excellent precision and recall values (near 100%) across a range of support thresholds while reducing the time required for association rule mining drastically.
KDD	The data mining approach to automated software testing.	Mark Last,Menahem Friedman,Abraham Kandel	2003	"In today's industry, the design of software tests is mostly based on the testers' expertise, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the world's economy. The costs of the so-called ""catastrophic"" software failures (such as Mars Polar Lander shutdown in 1999) are even hard to measure. In this paper, we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data. The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications, designing a minimal set of regression tests, and evaluating the correctness of software outputs when testing new, potentially flawed releases of the system. To study the feasibility of the proposed approach, we have applied a novel data mining algorithm called Info-Fuzzy Network (IFN) to execution data of a general-purpose code for solving partial differential equations. After being trained on a relatively small number of randomly generated input-output examples, the model constructed by the IFN algorithm has shown a clear capability to discriminate between correct and faulty versions of the program."
KDD	Similarity analysis on government regulations.	Gloria T. Lau,Kincho H. Law,Gio Wiederhold	2003	"Government regulations are semi-structured text documents that are often voluminous, heavily cross-referenced between provisions and even ambiguous. Multiple sources of regulations lead to difficulties in both understanding and complying with all applicable codes. In this work, we propose a framework for regulation management and similarity analysis. An online repository for legal documents is created with the help of text mining tool, and users can access regulatory documents either through the natural hierarchy of provisions or from a taxonomy generated by knowledge engineers based on concepts. Our similarity analysis core identifies relevant provisions and brings them to the user's attention, and this is performed by utilizing both the hierarchical and referential structures of regulations to provide a better comparison between provisions. Preliminary results show that our system reveals hidden similarities that are not apparent between provisions based on node content comparisons."
KDD	Passenger-based predictive modeling of airline no-show rates.	Richard D. Lawrence,Se June Hong,Jacques Cherrier	2003	Airlines routinely overbook flights based on the expectation that some fraction of booked passengers will not show for each flight. Accurate forecasts of the expected number of no-shows for each flight can increase airline revenue by reducing the number of spoiled seats (empty seats that might otherwise have been sold) and the number of involuntary denied boardings at the departure gate. Conventional no-show forecasting methods typically average the no-show rates of historically similar flights, without the use of passenger-specific information.We develop two classes of models to predict cabin-level no-show rates using specific information on the individual passengers booked on each flight. The first of these models computes the no-show probability for each passenger, using both the cabin-level historical forecast and the extracted passenger features as explanatory variables. This passenger-level model is implemented using three different predictive methods: a C4.5 decision-tree, a segmented Naive Bayes algorithm, and a new aggregation method for an ensemble of probabilistic models. The second cabin-level model is formulated using the desired cabin-level no-show rate as the response variable. Inputs to this model include the predicted cabin-level no-show rates derived from the various passenger-level models, as well as simple statistics of the features of the cabin passenger population. The cabin-level model is implemented using either linear regression, or as a direct probability model with explicit incorporation of the cabin-level no-show rates derived from the passenger-level model outputs.The new passenger-based models are compared to a conventional historical model, using train and evaluation data sets taken from over 1 million passenger name records. Standard metrics such as lift curves and mean-square cabin-level errors establish the improved accuracy of the passenger-based models over the historical model. All models are also evaluated using a simple revenue model, and it is shown that the cabin-level passenger-based model can produce between 0.4% and 3.2% revenue gain over the conventional model, depending on the revenue-model parameters.
KDD	Empirical comparisons of various voting methods in bagging.	Kelvin T. Leung,Douglas Stott Parker Jr.	2003	Empirical comparisons of various voting methods in bagging.
KDD	Mining data records in Web pages.	Bing Liu,Robert L. Grossman,Yanhong Zhai	2003	A large amount of information on the Web is contained in regularly structured objects, which we call data records. Such data records are important because they often present the essential information of their host pages, e.g., lists of products or services. It is useful to mine such data records in order to extract information from them to provide value-added services. Existing automatic techniques are not satisfactory because of their poor accuracies. In this paper, we propose a more effective technique to perform the task. The technique is based on two observations about data records on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and non-contiguous data records. Our experimental results show that the proposed technique outperforms existing techniques substantially.
KDD	On computing, storing and querying frequent patterns.	Guimei Liu,Hongjun Lu,Wenwu Lou,Jeffrey Xu Yu	2003	"Extensive efforts have been devoted to developing efficient algorithms for mining frequent patterns. However, frequent pattern mining remains a time-consuming process, especially for very large datasets. It is therefore desirable to adopt a ""mining once and using many times"" strategy. Unfortunately, there has been little work reported on managing and organizing a large set of patterns for future use. In this paper, we propose a disk-based data structure, CFP-tree (Condensed Frequent Pattern Tree), for organizing frequent patterns discovered from transactional databases. In addition to an efficient algorithm for CFP-tree construction, we also developed algorithms to efficiently support two important types of queries, namely queries with minimum support constraints and queries with item constraints, against the stored patterns, as these two types of queries are basic building blocks for complex frequent pattern related mining tasks. Comprehensive experimental study has been conducted to demonstrate the effectiveness of CFP-tree and efficiency of related algorithms."
KDD	Experimental design for solicitation campaigns.	Uwe F. Mayer,Armand Sarkissian	2003	Data mining techniques are routinely used by fundraisers to select those prospects from a large pool of candidates who are most likely to make a financial contribution. These techniques often rely on statistical models based on trial performance data. This trial performance data is typically obtained by soliciting a smaller sample of the possible prospect pool. Collecting this trial data involves a cost; therefore the fundraiser is interested in keeping the trial size small while still collecting enough data to build a reliable statistical model that will be used to evaluate the remainder of the prospects.We describe an experimental design approach to optimally choose the trial prospects from an existing large pool of prospects. Prospects are clustered to render the problem practically tractable. We modify the standard D-optimality algorithm to prevent repeated selection of the same prospect cluster, since each prospect can only be solicited at most once.We assess the benefits of this approach on the KDD-98 data set by comparing the performance of the model based on the optimal trial data set with that of a model based on a randomly selected trial data set of equal size.
KDD	Distributed cooperative mining for information consortia.	Satoshi Morinaga,Kenji Yamanishi,Jun-ichi Takeuchi	2003	We consider the situation where a number of agents are distributed and each of them collects a data sequence generated according to an unknown probability distribution. Here each of the distributions is specified by common parameters and individual parameters e.g., a normal distribution with an identical mean and a different variance. Here we introduce a notion of an information consortium, which is a framework where the agents cannot show raw data to one another, but they like to enjoy significant information gain for estimating the respective distributions. Such an information consortium has recently received much interest in a broad range of areas including financial risk management, ubiquitous network mining, etc. In this paper we are concerned with the following three issues: 1) how to design a collaborative strategy for agents to estimate the respective distributions in the information consortium, 2) characterizing when each agent has a benefit in terms of information gain for estimating its distribution or information loss for predicting future data, and 3) charracterizing how much benefit each agent obtains. In this paper we yield a statistical formulation of information consortia and solve all of the above three problems for a general form of probability distributions. Specifically we propose a basic strategy for cooperative estimation and derive a necessary and sufficient condition for each agent to have a significant benefit.
KDD	Learning relational probability trees.	Jennifer Neville,David Jensen,Lisa Friedland,Michael Hay	2003	Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.
KDD	Graph-based anomaly detection.	Caleb C. Noble,Diane J. Cook	2003	Anomaly detection is an area that has received much attention in recent years. It has a wide variety of applications, including fraud detection and network intrusion detection. A good deal of research has been performed in this area, often using strings or attribute-value data as the medium from which anomalies are to be extracted. Little work, however, has focused on anomaly detection in graph-based data. In this paper, we introduce two techniques for graph-based anomaly detection. In addition, we introduce a new method for calculating the regularity of a graph, with applications to anomaly detection. We hypothesize that these methods will prove useful both for finding anomalies, and for determining the likelihood of successful anomaly detection within graph-based data. We provide experimental results using both real-world network intrusion data and artificially-created data.
KDD	Towards NIC-based intrusion detection.	Matthew Eric Otey,Srinivasan Parthasarathy,Amol Ghoting,G. Li,Sundeep Narravula,Dhabaleswar K. Panda	2003	We present and evaluate a NIC-based network intrusion detection system. Intrusion detection at the NIC makes the system potentially tamper-proof and is naturally extensible to work in a distributed setting. Simple anomaly detection and signature detection based models have been implemented on the NIC firmware, which has its own processor and memory. We empirically evaluate such systems from the perspective of quality and performance (bandwidth of acceptable messages) under varying conditions of host load. The preliminary results we obtain are very encouraging and lead us to believe that such NIC-based security schemes could very well be a crucial part of next generation network security systems.
KDD	Visualizing changes in the structure of data for exploratory feature selection.	Elias Pampalk,Werner Goebl,Gerhard Widmer	2003	Using visualization techniques to explore and understand high-dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays. Several visualization techniques have been developed to study the cluster structure of data, i.e., the existence of distinctive groups in the data and how these clusters are related to each other. However, only few of these techniques lend themselves to studying how this structure changes if the features describing the data are changed. Understanding this relationship between the features and the cluster structure means understanding the features themselves and is thus a useful tool in the feature extraction phase.In this paper we present a novel approach to visualizing how modification of the features with respect to weighting or normalization changes the cluster structure. We demonstrate the application of our approach in two music related data mining projects.
KDD	Carpenter: finding closed patterns in long biological datasets.	Feng Pan,Gao Cong,Anthony K. H. Tung,Jiong Yang,Mohammed Javeed Zaki	2003	The growth of bioinformatics has resulted in datasets with new characteristics. These datasets typically contain a large number of columns and a small number of rows. For example, many gene expression datasets may contain 10,000-100,000 columns but only 100-1000 rows.Such datasets pose a great challenge for existing (closed) frequent pattern discovery algorithms, since they have an exponential dependence on the average row length. In this paper, we describe a new algorithm called CARPENTER that is specially designed to handle datasets having a large number of attributes and relatively small number of rows. Several experiments on real bioinformatics datasets show that CARPENTER is orders of magnitude better than previous closed pattern mining algorithms like CLOSET and CHARM.
KDD	Aggregation-based feature invention and relational concept classes.	Claudia Perlich,Foster J. Provost	2003	Model induction from relational data requires aggregation of the values of attributes of related entities. This paper makes three contributions to the study of relational learning. (1) It presents a hierarchy of relational concepts of increasing complexity, using relational schema characteristics such as cardinality, and derives classes of aggregation operators that are needed to learn these concepts. (2) Expanding one level of the hierarchy, it introduces new aggregation operators that model the distributions of the values to be aggregated and (for classification problems) the differences in these distributions by class. (3) It demonstrates empirically on a noisy business domain that more-complex aggregation methods can increase generalization performance. Constructing features using target-dependent aggregations can transform relational prediction tasks so that well-understood feature-vector-based modeling algorithms can be applied successfully.
KDD	Data-driven validation, completion and construction of event relationship networks.	Chang-Shing Perng,David Thoenen,Genady Grabarnik,Sheng Ma,Joseph L. Hellerstein	2003	Event management is a focal point in building and maintaining high quality information infrastructures. We have witnessed the shift of the paradigm of event management in practice from root cause analysis (RCA) to action-oriented analysis (AOA). IBM has developed a pioneer event management methodology (EMD) based on the AOA paradigm and applied it to more than two hundred production sites with success. Foreseeably, more and more event management professionals will apply AOA in different incarnations in building proactive management facilities. By that, building correct and effective Event Relationship Networks (ERNs) becomes the dominating activity in AOA service design process. Currently, the quality of ERNs and the cost of building them largely depend on the knowledge of domain experts. We believe that we can utilize historical event logs in shortening the ERNs design process and perfecting the quality of ERNs. In this paper, we describe in detail how to apply this data-driven approach in ERN validation, completion and construction.
KDD	New unsupervised clustering algorithm for large datasets.	William Peter,John Chiochetti,Clare Giardina	2003	"A fast and accurate unsupervised clustering algorithm has been developed for clustering very large datasets. Though designed for very large volumes of geospatial data, the algorithm is general enough to be used in a wide variety of domain applications. The number of computations the algorithm requires is ~ O(N), and thus faster than hierarchical algorithms. Unlike the popular K-means heuristic, this algorithm does not require a series of iterations to converge to a solution. In addition, this method does not depend on initialization of a given number of cluster representatives, and so is insensitive to initial conditions. Being unsupervised, the algorithm can also ""rank"" each cluster based on density. The method relies on weighting a dataset to grid points on a mesh, and using a small number of rule-based agents to find the high density clusters. This method effectively reduces large datasets to the size of the grid, which is usually many orders of magnitude smaller. Numerical experiments are shown that demonstrate the advantages of this algorithm over other techniques."
KDD	Capturing best practice for microarray gene expression data analysis.	Gregory Piatetsky-Shapiro,Tom Khabaza,Sridhar Ramaswamy	2003	"Analyzing gene expression data from microarray devices has many important application in medicine and biology, but presents significant challenges to data mining. Microarray data typically has many attributes (genes) and few examples (samples), making the process of correctly analyzing such data difficult to formulate and prone to common mistakes. For this reason it is unusually important to capture and record good practices for this form of data mining. This paper presents a process for analyzing microarray data, including pre-processing, gene selection, randomization testing, classification and clustering; this process is captured with ""Clementine Application Templates"". The paper describes the process in detail and includes three case studies, showing how the process is applied to 2-class classification, multi-class classification and clustering analyses for publicly available microarray datasets."
KDD	Visualizing concept drift.	Kevin B. Pratt,Gleb Tschapek	2003	"We describe a visualization technique that uses brushed, parallel histograms to aid in understanding concept drift in multidimensional problem spaces. This technique illustrates the relationship between changes in distributions of multiple antecedent feature values and the outcome distribution. We can also observe effects on the relative utilization of predictive rules. Our parallel histogram technique solves the over-plotting difficulty of parallel coordinate graphs and the difficulty of comparing distributions of brushed and original data. We demonstrate our technique's usefulness in understanding concept drifts in power demand and stock investment returns."
KDD	Critical event prediction for proactive management in large-scale computer clusters.	Ramendra K. Sahoo,Adam J. Oliner,Irina Rish,Manish Gupta,José E. Moreira,Sheng Ma,Ricardo Vilalta,Anand Sivasubramaniam	2003	"As the complexity of distributed computing systems increases, systems management tasks require significantly higher levels of automation; examples include diagnosis and prediction based on real-time streams of computer events, setting alarms, and performing continuous monitoring. The core of autonomic computing, a recently proposed initiative towards next-generation IT-systems capable of 'self-healing', is the ability to analyze data in real-time and to predict potential problems. The goal is to avoid catastrophic failures through prompt execution of remedial actions.This paper describes an attempt to build a proactive prediction and control system for large clusters. We collected event logs containing various system reliability, availability and serviceability (RAS) events, and system activity reports (SARs) from a 350-node cluster system for a period of one year. The 'raw' system health measurements contain a great deal of redundant event data, which is either repetitive in nature or misaligned with respect to time. We applied a filtering technique and modeled the data into a set of primary and derived variables. These variables used probabilistic networks for establishing event correlations through prediction algorithms. We also evaluated the role of time-series methods, rule-based classification algorithms and Bayesian network models in event prediction.Based on historical data, our results suggest that it is feasible to predict system performance parameters (SARs) with a high degree of accuracy using time-series models. Rule-based classification techniques can be used to extract machine-event signatures to predict critical events with up to 70% accuracy."
KDD	Clinical and financial outcomes analysis with existing hospital patient records.	R. Bharat Rao,Sathyakama Sandilya,Radu Stefan Niculescu,Colin Germond,Harsha Rao	2003	"Existing patient records are a valuable resource for automated outcomes analysis and knowledge discovery. However, key clinical data in these records is typically recorded in unstructured form as free text and images, and most structured clinical information is poorly organized. Time-consuming interpretation and analysis is required to convert these records into structured clinical data. Thus, only a tiny fraction of this resource is utilized. We present REMIND, a Bayesian Framework for Reliable Extraction and Meaningful Inference from Nonstructured Data. REMIND integrates and blends the structured and unstructured clinical data in patient records to automatically created high-quality structured clinical data. This structuring allows existing patient records to be mined for quality assurance, regulatory compliance, and to relate financial and clinical factors. We demonstrate REMIND on two medical applications: (a) Extract ""recurrence"", the key outcome for measuring treatment effectiveness, for colon cancer patients (ii) Extract key diagnoses and complications for acute myocardial infarction (heart attack) patients, and demonstrate the impact of these clinical factors on financial outcomes."
KDD	Cross-training: learning probabilistic mappings between topics.	Sunita Sarawagi,Soumen Chakrabarti,Shantanu Godbole	2003	Classification is a well-established operation in text mining. Given a set of labels A and a set DA of training documents tagged with these labels, a classifier learns to assign labels to unlabeled test documents. Suppose we also had available a different set of labels B, together with a set of documents DB marked with labels from B. If A and B have some semantic overlap, can the availability of DB help us build a better classifier for A, and vice versa? We answer this question in the affirmative by proposing cross-training: a new approach to semi-supervised learning in presence of multiple label sets. We give distributional and discriminative algorithms for cross-training and show, through extensive experiments, that cross-training can discover and exploit probabilistic relations between two taxonomies for more accurate classification.
KDD	Improving spatial locality of programs via data mining.	Karlton Sequeira,Mohammed Javeed Zaki,Boleslaw K. Szymanski,Christopher D. Carothers	2003	In most computer systems, page fault rate is currently minimized by generic page replacement algorithms which try to model the temporal locality inherent in programs. In this paper, we propose two algorithms, one greedy and the other stochastic, designed for program specific code restructuring as a means of increasing spatial locality within a program. Both algorithms effectively decrease average working set size and hence the page fault rate. Our methods are more effective than traditional approaches due to use of domain information. We illustrate the efficacy of our algorithms on actual data mining algorithms.
KDD	Frequent-subsequence-based prediction of outer membrane proteins.	Rong She,Fei Chen,Ke Wang,Martin Ester,Jennifer L. Gardy,Fiona S. L. Brinkman	2003	"A number of medically important disease-causing bacteria (collectively called Gram-negative bacteria) are noted for the extra ""outer"" membrane that surrounds their cell. Proteins resident in this membrane (outer membrane proteins, or OMPs) are of primary research interest for antibiotic and vaccine drug design as they are on the surface of the bacteria and so are the most accessible targets to develop new drugs against. With the development of genome sequencing technology and bioinformatics, biologists can now deduce all the proteins that are likely produced in a given bacteria and have attempted to classify where proteins are located in a bacterial cell. However such protein localization programs are currently least accurate when predicting OMPs, and so there is a current need for the development of a better OMP classifier. Data mining research suggests that the use of frequent patterns has good performance in aiding the development of accurate and efficient classification algorithms. In this paper, we present two methods to identify OMPs based on frequent subsequences and test them on all Gram-negative bacterial proteins whose localizations have been determined by biological experiments. One classifier follows an association rule approach, while the other is based on support vector machines (SVMs). We compare the proposed methods with the state-of-the-art methods in the biological domain. The results demonstrate that our methods are better both in terms of accurately identifying OMPs and providing biological insights that increase our understanding of the structures and functions of these important proteins."
KDD	Experimental study of discovering essential information from customer inquiry.	Keiko Shimazu,Atsuhito Momma,Koichi Furukawa	2003	This paper reports the result of our experimental study on a new method of applying an association rule miner to discover useful information from customer inquiry database in a call center of a company. It has been claimed that association rule mining is not suited for text mining. To overcome this problem, we propose (1) to generate sequential data set of words with dependency structure from the Japanese text database, and (2) to employ a new method for extracting meaningful association rules by applying a new rule selection criterion. Each inquiry in the sequential data was represented as a list of word pairs, each of which consists of a verb and its dependent noun. The association rules were induced regarding each pair of words as an item. The rule selection criterion comes from our principle that we put heavier weights to co-occurrence of multiple items more than single item occurrence. We regarded a rule important if the existence of the items in the rule body significantly affects the occurrence of the item in the rule head. The selected rules were then categorized to form meaningful information classes. With this method, we succeeded in extracting useful information classes from the text database, which were not acquired by only simple keyword retrieval. Also, inquiries with multiple aspects were properly classified into corresponding multiple categories.
KDD	Generating English summaries of time series data using the Gricean maxims.	Somayajulu Sripada,Ehud Reiter,Jim Hunter,Jin Yu	2003	We are developing technology for generating English textual summaries of time-series data, in three domains: weather forecasts, gas-turbine sensor readings, and hospital intensive care data. Our weather-forecast generator is currently operational and being used daily by a meteorological company. We generate summaries in three steps: (a) selecting the most important trends and patterns to communicate; (b) mapping these patterns onto words and phrases; and (c) generating actual texts based on these words and phrases. In this paper we focus on the first step, (a), selecting the information to communicate, and describe how we perform this using modified versions of standard data analysis algorithms such as segmentation. The modifications arose out of empirical work with users and domain experts, and in fact can all be regarded as applications of the Gricean maxims of Quality, Quantity, Relevance, and Manner, which describe how a cooperative speaker should behave in order to help a hearer correctly interpret a text. The Gricean maxims are perhaps a key element of adapting data analysis algorithms for effective communication of information to human users, and should be considered by other researchers interested in communicating data to human users.
KDD	Discovery of climate indices using clustering.	Michael Steinbach,Pang-Ning Tan,Vipin Kumar,Steven A. Klooster,Christopher Potter	2003	"To analyze the effect of the oceans and atmosphere on land climate, Earth Scientists have developed climate indices, which are time series that summarize the behavior of selected regions of the Earth's oceans and atmosphere. In the past, Earth scientists have used observation and, more recently, eigenvalue analysis techniques, such as principal components analysis (PCA) and singular value decomposition (SVD), to discover climate indices. However, eigenvalue techniques are only useful for finding a few of the strongest signals. Furthermore, they impose a condition that all discovered signals must be orthogonal to each other, making it difficult to attach a physical interpretation to them. This paper presents an alternative clustering-based methodology for the discovery of climate indices that overcomes these limitiations and is based on clusters that represent regions with relatively homogeneous behavior. The centroids of these clusters are time series that summarize the behavior of the ocean or atmosphere in those regions. Some of these centroids correspond to known climate indices and provide a validation of our methodology; other centroids are variants of known indices that may provide better predictive power for some land areas; and still other indices may represent potentially new Earth science phenomena. Finally, we show that cluster based indices generally outperform SVD derived indices, both in terms of area weighted correlation and direct correlation with the known indices."
KDD	Mining phenotypes and informative genes from gene expression data.	Chun Tang,Aidong Zhang,Jian Pei	2003	Mining microarray gene expression data is an important research topic in bioinformatics with broad applications. While most of the previous studies focus on clustering either genes or samples, it is interesting to ask whether we can partition the complete set of samples into exclusive groups (called phenotypes) and find a set of informative genes that can manifest the phenotype structure. In this paper, we propose a new problem of simultaneously mining phenotypes and informative genes from gene expression data. Some statistics-based metrics are proposed to measure the quality of the mining results. Two interesting algorithms are developed: the heuristic search and the mutual reinforcing adjustment method. We present an extensive performance study on both real-world data sets and synthetic data sets. The mining results from the two proposed methods are clearly better than those from the previous methods. They are ready for the real-world applications. Between the two methods, the mutual reinforcing adjustment method is in general more scalable, more effective and with better quality of the mining results.
KDD	Assessment and pruning of hierarchical model based clustering.	Jeremy Tantrum,Alejandro Murua,Werner Stuetzle	2003	"The goal of clustering is to identify distinct groups in a dataset. The basic idea of model-based clustering is to approximate the data density by a mixture model, typically a mixture of Gaussians, and to estimate the parameters of the component densities, the mixing fractions, and the number of components from the data. The number of distinct groups in the data is then taken to be the number of mixture components, and the observations are partitioned into clusters (estimates of the groups) using Bayes' rule. If the groups are well separated and look Gaussian, then the resulting clusters will indeed tend to be ""distinct"" in the most common sense of the word - contiguous, densely populated areas of feature space, separated by contiguous, relatively empty regions. If the groups are not Gaussian, however, this correspondence may break down; an isolated group with a non-elliptical distribution, for example, may be modeled by not one, but several mixture components, and the corresponding clusters will no longer be well separated. We present methods for assessing the degree of separation between the components of a mixture model and between the corresponding clusters. We also propose a new clustering method that can be regarded as a hybrid between model-based and nonparametric clustering. The hybrid clustering algorithm prunes the cluster tree generated by hierarchical model-based clustering. Starting with the tree corresponding to the mixture model chosen by the Bayesian Information Criterion, it progressively merges clusters that do not appear to correspond to different modes of the data density."
KDD	Weighted Association Rule Mining using weighted support and significance framework.	Feng Tao,Fionn Murtagh,Mohsen Farid	2003	"We address the issues of discovering significant binary relationships in transaction datasets in a weighted setting. Traditional model of association rule mining is adapted to handle weighted association rule mining problems where each item is allowed to have a weight. The goal is to steer the mining focus to those significant relationships involving items with significant weights rather than being flooded in the combinatornal explosion of insignificant relationships. We identify the challenge of using weights in the iterative process of generating large itemsets. The problem of invalidation of the ""downward closure property"" in the weighted setting is solved by using an improved model of weighted support measurements and exploiting a ""weighted downward closure property"". A new algorithm called WARM (Weighted Association Rule Mining) is developed based on the improved model. The algorithm is both scalable and efficient in discovering significant relationships in weighted settings as illustrated by experiments performed on simulated datasets."
KDD	PaintingClass: interactive construction, visualization and exploration of decision trees.	Soon Tee Teoh,Kwan-Liu Ma	2003	Decision trees are commonly used for classification. We propose to use decision trees not just for classification but also for the wider purpose of knowledge discovery, because visualizing the decision tree can reveal much valuable information in the data. We introduce PaintingClass, a system for interactive construction, visualization and exploration of decision trees. PaintingClass provides an intuitive layout and convenient navigation of the decision tree. PaintingClass also provides the user the means to interactively construct the decision tree. Each node in the decision tree is displayed as a visual projection of the data. Through actual examples and comparison with other classification methods, we show that the user can effectively use PaintingClass to construct a decision tree and explore the decision tree to gain additional knowledge.
KDD	Time and sample efficient discovery of Markov blankets and direct causal relations.	Ioannis Tsamardinos,Constantin F. Aliferis,Alexander R. Statnikov	2003	Data Mining with Bayesian Network learning has two important characteristics: under conditions learned edges between variables correspond to casual influences, and second, for every variable T in the network a special subset (Markov Blanket) identifiable by the network is the minimal variable set required to predict T. However, all known algorithms learning a complete BN do not scale up beyond a few hundred variables. On the other hand, all known sound algorithms learning a local region of the network require an exponential number of training instances to the size of the learned region.The contribution of this paper is two-fold. We introduce a novel local algorithm that returns all variables with direct edges to and from a target variable T as well as a local algorithm that returns the Markov Blanket of T. Both algorithms (i) are sound, (ii) can be run efficiently in datasets with thousands of variables, and (iii) significantly outperform in terms of approximating the true neighborhood previous state-of-the-art algorithms using only a fraction of the training size required by the existing methods. A fundamental difference between our approach and existing ones is that the required sample depends on the generating graph connectivity and not the size of the local region; this yields up to exponential savings in sample relative to previously known algorithms. The results presented here are promising not only for discovery of local causal structure, and variable selection for classification, but also for the induction of complete BNs.
KDD	Privacy-preserving -means clustering over vertically partitioned data.	Jaideep Vaidya,Chris Clifton	2003	Privacy and security concerns can prevent sharing of data, derailing data mining projects. Distributed knowledge discovery, if done correctly, can alleviate this problem. The key is to obtain valid results, while providing guarantees on the (non)disclosure of data. We present a method for k-means clustering when different sites contain different attributes for a common set of entities. Each site learns the cluster of each entity, but learns nothing about the attributes at other sites.
KDD	Indexing multi-dimensional time-series with support for multiple distance measures.	Michail Vlachos,Marios Hadjieleftheriou,Dimitrios Gunopulos,Eamonn J. Keogh	2003	Although most time-series data mining research has concentrated on providing solutions for a single distance function, in this work we motivate the need for a single index structure that can support multiple distance measures. Our specific area of interest is the efficient retrieval and analysis of trajectory similarities. Trajectory datasets are very common in environmental applications, mobility experiments, video surveillance and are especially important for the discovery of certain biological patterns. Our primary similarity measure is based on the Longest Common Subsequence (LCSS) model, that offers enhanced robustness, particularly for noisy data, which are encountered very often in real world applications. However, our index is able to accommodate other distance measures as well, including the ubiquitous Euclidean distance, and the increasingly popular Dynamic Time Warping (DTW). While other researchers have advocated one or other of these similarity measures, a major contribution of our work is the ability to support all these measures without the need to restructure the index. Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision/recall. The experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the LCSS and the DTW.
KDD	Mining concept-drifting data streams using ensemble classifiers.	Haixun Wang,Wei Fan,Philip S. Yu,Jiawei Han	2003	Recently, mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection, target marketing, network intrusion detection, etc. Conventional knowledge discovery tools are facing two challenges, the overwhelming volume of the streaming data, and the concept drifts. In this paper, we propose a general framework for mining concept-drifting data streams using weighted ensemble classifiers. We train an ensemble of classification models, such as C4.5, RIPPER, naive Beyesian, etc., from sequential chunks of the data stream. The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment. Thus, the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification. Our empirical study shows that the proposed methods have substantial advantage over single-classifier approaches in prediction accuracy, and the ensemble framework is effective for a variety of classification models.
KDD	CLOSET+: searching for the best strategies for mining frequent closed itemsets.	Jianyong Wang,Jiawei Han,Jian Pei	2003	"Mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis. Extensive studies have proposed various strategies for efficient frequent closed itemset mining, such as depth-first search vs. breadthfirst search, vertical formats vs. horizontal formats, tree-structure vs. other data structures, top-down vs. bottom-up traversal, pseudo projection vs. physical projection of conditional database, etc. It is the right time to ask ""what are the pros and cons of the strategies?"" and ""what and how can we pick and integrate the best strategies to achieve higher performance in general cases?""In this study, we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET+. CLOSET+ integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET+ over existing mining algorithms, including CLOSET, CHARM and OP, in terms of runtime, memory usage and scalability."
KDD	Mining unexpected rules by pushing user dynamics.	Ke Wang,Yuelong Jiang,Laks V. S. Lakshmanan	2003	Unexpected rules are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest. In this paper, we study three important issues that have been previously ignored in mining unexpected rules. First, the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario, in addition to the knowledge itself. Second, the prior knowledge should be considered right from the start to focus the search on unexpected rules. Third, the unexpectedness of a rule depends on what other rules the user has seen so far. Thus, only rules that remain unexpected given what the user has seen should be considered interesting. We develop an approach that addresses all three problems above and evaluate it by means of experiments focusing on finding interesting rules.
KDD	On detecting differences between groups.	Geoffrey I. Webb,Shane M. Butler,Douglas A. Newlands	2003	Understanding the differences between contrasting groups is a fundamental task in data analysis. This realization has led to the development of a new special purpose data mining technique, contrast-set mining. We undertook a study with a retail collaborator to compare contrast-set mining with existing rule-discovery techniques. To our surprise we observed that straightforward application of an existing commercial rule-discovery system, Magnum Opus, could successfully perform the contrast-set-mining task. This led to the realization that contrast-set mining is a special case of the more general rule-discovery task. We present the results of our study together with a proof of this conclusion.
KDD	Analyzing customer behavior at Amazon.com.	Andreas S. Weigend	2003	Analyzing customer behavior at Amazon.com.
KDD	Knowledge-based data mining.	Sholom M. Weiss,Stephen J. Buckley,Shubir Kapoor,Søren Damgaard	2003	"We describe techniques for combining two types of knowledge systems: expert and machine learning. Both the expert system and the learning system represent information by logical decision rules or trees. Unlike the classical views of knowledge-base evaluation or refinement, our view accepts the contents of the knowledge base as completely correct. The knowledge base and the results of its stored cases will provide direction for the discovery of new relationships in the form of newly induced decision rules. An expert system called SEAS was built to discover sales leads for computer products and solutions. The system interviews executives by asking questions, and based on the responses, recommends products that may improve a business' operations. Leveraging this expert system, we record the results of the interviews and the program's recommendations. The very same data stored by the expert system is used to find new predictive rules. Among the potential advantages of this approach are (a) the capability to spot new sales trends and (b) the substitution of less expensive probabilistic rules that use database data instead of interviews."
KDD	Algorithms for estimating relative importance in networks.	Scott White,Padhraic Smyth	2003	"Large and complex graphs representing relationships among sets of entities are an increasingly common focus of interest in data analysis---examples include social networks, Web graphs, telecommunication networks, and biological networks. In interactive analysis of such data a natural query is ""which entities are most important in the network relative to a particular individual or set of individuals?"" We investigate the problem of answering such queries in this paper, focusing in particular on defining and computing the importance of nodes in a graph relative to one or more root nodes. We define a general framework and a number of different algorithms, building on ideas from social networks, graph theory, Markov models, and Web graph analysis. We experimentally evaluate the different properties of these algorithms on toy graphs and demonstrate how our approach can be used to study relative importance in real-world networks including a network of interactions among September 11th terrorists, a network of collaborative research in biotechnology among companies and universities, and a network of co-authorship relationships among computer science researchers."
KDD	Screening and interpreting multi-item associations based on log-linear modeling.	Xintao Wu,Daniel Barbará,Yong Ye	2003	Association rules have received a lot of attention in the data mining community since their introduction. The classical approach to find rules whose items enjoy high support (appear in a lot of the transactions in the data set) is, however, filled with shortcomings. It has been shown that support can be misleading as an indicator of how interesting the rule is. Alternative measures, such as lift, have been proposed. More recently, a paper by DuMouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that cannot be explained by pairwise associations between the items involved. This approach, however, has its limitations, since it stops short of considering higher order interactions (other than pairwise) among the items. In this paper, we propose a method that examines the parameters of the fitted loglinear models to find all the significant association patterns among the items. Since fitting loglinear models for large data sets can be computationally prohibitive, we apply graph-theoretical results to divide the original set of items into components (sets of items) that are statistically independent from each other. We then apply loglinear modeling to each of the components and find the interesting associations among items in them. The technique is experimentally evaluated with a real data set (insurance data) and a series of synthetic data sets. The results show that the technique is effective in finding interesting associations among the items involved.
KDD	The anatomy of a multimodal information filter.	Yi-Leh Wu,Kingshy Goh,Beitao Li,Huaxin You,Edward Y. Chang	2003	"The proliferation of objectionable information on the Internet has reached a level of serious concern. To empower end-users with the choice of blocking undesirable and offensive websites, we propose a multimodal information filter, named MORF. In this paper, we present MORF's core components: its confidence-based classifier, a Cross-bagging ensemble scheme, and multimodal classification algorithm. Empirical studies and initial statistics collected from the MORF filters deployed at sites in the U.S. and Asia show that MORF is both efficient and effective, due to our classification methods."
KDD	CloseGraph: mining closed frequent graph patterns.	Xifeng Yan,Jiawei Han	2003	Recent research on pattern discovery has progressed form mining frequent itemsets and sequences to mining structured patterns including trees, lattices, and graphs. As a general data structure, graph can model complicated relations among data with wide applications in bioinformatics, Web exploration, and etc. However, mining large graph patterns in challenging due to the presence of an exponential number of frequent subgraphs. Instead of mining all the subgraphs, we propose to mine closed frequent graph patterns. A graph g is closed in a database if there exists no proper supergraph of g that has the same support as g. A closed graph pattern mining algorithm, CloseGraph, is developed by exploring several interesting pruning methods. Our performance study shows that CloseGraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increases the efficiency of mining, especially in the presence of large graph patterns.
KDD	Applying data mining in investigating money laundering crimes.	Zhongfei (Mark) Zhang,John J. Salerno,Philip S. Yu	2003	In this paper, we study the problem of applying data mining to facilitate the investigation of money laundering crimes (MLCs). We have identified a new paradigm of problems --- that of automatic community generation based on uni-party data, the data in which there is no direct or explicit link information available. Consequently, we have proposed a new methodology for Link Discovery based on Correlation Analysis (LDCA). We have used MLC group model generation as an exemplary application of this problem paradigm, and have focused on this application to develop a specific method of automatic MLC group model generation based on timeline analysis using the LDCA methodology, called CORAL. A prototype of CORAL method has been implemented, and preliminary testing and evaluations based on a real MLC case data are reported. The contributions of this work are: (1) identification of the uni-party data community generation problem paradigm, (2) proposal of a new methodology LDCA to solve for problems in this paradigm, (3) formulation of the MLC group model generation problem as an example of this paradigm, (4) application of the LDCA methodology in developing a specific solution (CORAL) to the MLC group model generation problem, and (5) development, evaluation, and testing of the CORAL prototype in a real MLC case data.
KDD	Efficient elastic burst detection in data streams.	Yunyue Zhu,Dennis Shasha	2003	Burst detection is the activity of finding abnormal aggregates in data streams. Such aggregates are based on sliding windows over data streams. In some applications, we want to monitor many sliding window sizes simultaneously and to report those windows with aggregates significantly different from other periods. We will present a general data structure for detecting interesting aggregates over such elastic windows in near linear time. We present applications of the algorithm for detecting Gamma Ray Bursts in large-scale astrophysical data. Detection of periods with high volumes of trading activities and high stock price volatility is also demonstrated using real time Trade and Quote (TAQ) data from the New York Stock Exchange (NYSE). Our algorithm beats the direct computation approach by several orders of magnitude.
KDD	Eliminating noisy information in Web pages for data mining.	Lan Yi,Bing Liu,Xiaoli Li	2003	A commercial Web page typically contains many information blocks. Apart from the main content blocks, it usually has such blocks as navigation panels, copyright and privacy notices, and advertisements (for business purposes and for easy user access). We call these blocks that are not the main content blocks of the page the noisy blocks. We show that the information contained in these noisy blocks can seriously harm Web data mining. Eliminating these noises is thus of great importance. In this paper, we propose a noise elimination technique based on the following observation: In a given Web site, noisy blocks usually share some common contents and presentation styles, while the main content blocks of the pages are often diverse in their actual contents and/or presentation styles. Based on this observation, we propose a tree structure, called Style Tree, to capture the common presentation styles and the actual contents of the pages in a given Web site. By sampling the pages of the site, a Style Tree can be built for the site, which we call the Site Style Tree (SST). We then introduce an information based measure to determine which parts of the SST represent noises and which parts represent the main contents of the site. The SST is employed to detect and eliminate noises in any Web page of the site by mapping this page to the SST. The proposed technique is evaluated with two data mining tasks, Web page clustering and classification. Experimental results show that our noise elimination technique is able to improve the mining results significantly.
KDD	Distributed multivariate regression based on influential observations.	Hang Yu,Ee-Chien Chang	2003	Large-scale data sets are sometimes logically and physically distributed in separate databases. The issues of mining these data sets are not just their sizes, but also the distributed nature. The complication is that communicating all the data to a central database would be too slow. To reduce communication costs, one could compress the data during transmission. Another method is random sampling. We propose an approach for distributed multivariate regression based on sampling and discuss its relationship with the compression method. The central idea is motivated by the observation that, although communication is limited, each individual site can still scan and process all the data it holds. Thus it is possible for the site to communicate only influential samples without seeing data in other sites. We exploit this observation and derive a method that provides tradeoff between communication cost and accuracy. Experimental results show that it is better than the compression method and random sampling.
KDD	Efficiently handling feature redundancy in high-dimensional data.	Lei Yu,Huan Liu	2003	High-dimensional data poses a severe challenge for data mining. Feature selection is a frequently used technique in pre-processing high-dimensional data for successful data mining. Traditionally, feature selection is focused on removing irrelevant features. However, for high-dimensional data, removing redundant features is equally critical. In this paper, we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model. The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features.
KDD	Classifying large data sets using SVMs with hierarchical clusters.	Hwanjo Yu,Jiong Yang,Jiawei Han	2003	Support vector machines (SVMs) have been promising methods for classification and regression analysis because of their solid mathematical foundations which convery several salient properties that other methods hardly provide. However, despite the prominent properties of SVMs, they are not as favored for large-scale data mining as for pattern recognition or machine learning because the training complexity of SVMs is highly dependent on the size of a data set. Many real-world data mining applications involve millions or billions of data records where even multiple scans of the entire data are too expensive to perform. This paper presents a new method, Clustering-Based SVM (CB-SVM), which is specifically designed for handling very large data sets. CB-SVM applies a hierarchical micro-clustering algorithm that scans the entire data set only once to provide an SVM with high quality samples that carry the statistical summaries of the data such that the summaries maximize the benefit of learning the SVM. CB-SVM tries to generate the best SVM boundary for very large data sets given limited amount of resources. Our experiments on synthetic and real data sets show that CB-SVM is highly scalable for very large data sets while also generating high classification accuracy.
KDD	XRules: an effective structural classifier for XML data.	Mohammed Javeed Zaki,Charu C. Aggarwal	2003	XML documents have recently become ubiquitous because of their varied applicability in a number of applications. Classification is an important problem in the data mining domain, but current classification methods for XML documents use IR-based methods in which each document is treated as a bag of words. Such techniques ignore a significant amount of information hidden inside the documents. In this paper we discuss the problem of rule based classification of XML data by using frequent discriminatory substructures within XML documents. Such a technique is more capable of finding the classification characteristics of documents. In addition, the technique can also be extended to cost sensitive classification. We show the effectiveness of the method with respect to other classifiers. We note that the methodology discussed in this paper is applicable to any kind of semi-structured data.
KDD	Fast vertical mining using diffsets.	Mohammed Javeed Zaki,Karam Gouda	2003	A number of vertical mining algorithms have been proposed recently for association mining, which have shown to be very effective and usually outperform horizontal approaches. The main advantage of the vertical format is support for fast frequency counting via intersection operations on transaction ids (tids) and automatic pruning of irrelevant data. The main problem with these approaches is when intermediate results of vertical tid lists become too large for memory, thus affecting the algorithm scalability.In this paper we present a novel vertical data representation called Diffset, that only keeps track of differences in the tids of a candidate pattern from its generating frequent patterns. We show that diffsets drastically cut down the size of memory required to store intermediate results. We show how diffsets, when incorporated into previous vertical mining methods, increase the performance significantly.
KDD	Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 24 - 27, 2003	Lise Getoor,Ted E. Senator,Pedro Domingos,Christos Faloutsos	2003	Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 24 - 27, 2003
PKDD	A Skeleton-Based Approach to Learning Bayesian Networks from Data.	Steven van Dijk,Linda C. van der Gaag,Dirk Thierens	2003	A Skeleton-Based Approach to Learning Bayesian Networks from Data.
PKDD	Efficient Statistical Pruning of Association Rules.	Alan Ableson,Janice I. Glasgow	2003	Efficient Statistical Pruning of Association Rules.
PKDD	From Knowledge-Based to Skill-Based Systems: Sailing as a Machine Learning Challenge.	Pieter W. Adriaans	2003	From Knowledge-Based to Skill-Based Systems: Sailing as a Machine Learning Challenge.
PKDD	Minimal -Free Representations of Frequent Sets.	Toon Calders,Bart Goethals	2003	Minimal -Free Representations of Frequent Sets.
PKDD	Discovering Unbounded Episodes in Sequential Data.	Gemma Casas-Garriga	2003	Discovering Unbounded Episodes in Sequential Data.
PKDD	Mr-SBC: A Multi-relational Naïve Bayes Classifier.	Michelangelo Ceci,Annalisa Appice,Donato Malerba	2003	Mr-SBC: A Multi-relational Naïve Bayes Classifier.
PKDD	Majority Classification by Means of Association Rules.	Elena Baralis,Paolo Garza	2003	Majority Classification by Means of Association Rules.
PKDD	SMOTEBoost: Improving Prediction of the Minority Class in Boosting.	Nitesh V. Chawla,Aleksandar Lazarevic,Lawrence O. Hall,Kevin W. Bowyer	2003	SMOTEBoost: Improving Prediction of the Minority Class in Boosting.
PKDD	Adaptive Constraint Pushing in Frequent Pattern Mining.	Francesco Bonchi,Fosca Giannotti,Alessio Mazzanti,Dino Pedreschi	2003	Adaptive Constraint Pushing in Frequent Pattern Mining.
PKDD	ExAnte: Anticipated Data Reduction in Constrained Pattern Mining.	Francesco Bonchi,Fosca Giannotti,Alessio Mazzanti,Dino Pedreschi	2003	ExAnte: Anticipated Data Reduction in Constrained Pattern Mining.
PKDD	Two-Eyed Algorithms and Problems.	Leo Breiman	2003	Two-Eyed Algorithms and Problems.
PKDD	Using Belief Networks and Fisher Kernels for Structured Document Classification.	Ludovic Denoyer,Patrick Gallinari	2003	Using Belief Networks and Fisher Kernels for Structured Document Classification.
PKDD	On Decision Boundaries of Naïve Bayes in Continuous Domains.	Tapio Elomaa,Juho Rousu	2003	On Decision Boundaries of Naïve Bayes in Continuous Domains.
PKDD	Application of Inductive Logic Programming to Structure-Based Drug Design.	David P. Enot,Ross D. King	2003	Application of Inductive Logic Programming to Structure-Based Drug Design.
PKDD	Next Generation Data Mining Tools: Power Laws and Self-similarity for Graphs, Streams and Traditional Data.	Christos Faloutsos	2003	Next Generation Data Mining Tools: Power Laws and Self-similarity for Graphs, Streams and Traditional Data.
PKDD	An Indiscernibility-Based Clustering Method with Iterative Refinement of Equivalence Relations.	Shoji Hirano,Shusaku Tsumoto	2003	An Indiscernibility-Based Clustering Method with Iterative Refinement of Equivalence Relations.
PKDD	Preference Mining: A Novel Approach on Mining User Preferences for Personalized Applications.	Stefan Holland,Martin Ester,Werner Kießling	2003	Preference Mining: A Novel Approach on Mining User Preferences for Personalized Applications.
PKDD	Explaining Text Clustering Results Using Semantic Structures.	Andreas Hotho,Steffen Staab,Gerd Stumme	2003	Explaining Text Clustering Results Using Semantic Structures.
PKDD	Visualizing Class Probability Estimators.	Eibe Frank,Mark Hall	2003	Visualizing Class Probability Estimators.
PKDD	Analyzing Attribute Dependencies.	Aleks Jakulin,Ivan Bratko	2003	Analyzing Attribute Dependencies.
PKDD	Ranking Interesting Subspaces for Clustering High Dimensional Data.	Karin Kailing,Hans-Peter Kriegel,Peer Kröger,Stefanie Wanka	2003	Ranking Interesting Subspaces for Clustering High Dimensional Data.
PKDD	Efficiently Finding Arbitrarily Scaled Patterns in Massive Time Series Databases.	Eamonn J. Keogh	2003	Efficiently Finding Arbitrarily Scaled Patterns in Massive Time Series Databases.
PKDD	"Automated Detection of Epidemics from the Usage Logs of a Physicians' Reference Database."	Jaana Heino,Hannu Toivonen	2003	"Automated Detection of Epidemics from the Usage Logs of a Physicians' Reference Database."
PKDD	The Pattern Ordering Problem.	Taneli Mielikäinen,Heikki Mannila	2003	The Pattern Ordering Problem.
PKDD	Using Transduction and Multi-view Learning to Answer Emails.	Michael Kockelkorn,Andreas Lüneburg,Tobias Scheffer	2003	Using Transduction and Multi-view Learning to Answer Emails.
PKDD	Exploring Fringe Settings of SVMs for Classification.	Adam Kowalczyk,Bhavani Raskutti	2003	Exploring Fringe Settings of SVMs for Classification.
PKDD	Collaborative Filtering Using Restoration Operators.	Atsuyoshi Nakamura,Mineichi Kudo,Akira Tanaka	2003	Collaborative Filtering Using Restoration Operators.
PKDD	Rule Discovery and Probabilistic Modeling for Onomastic Data.	Antti Leino,Heikki Mannila,Ritva Liisa Pitkänen	2003	Rule Discovery and Probabilistic Modeling for Onomastic Data.
PKDD	Efficient Frequent Query Discovery in FARMER.	Siegfried Nijssen,Joost N. Kok	2003	Efficient Frequent Query Discovery in FARMER.
PKDD	Constraint-Based Mining of Sequential Patterns over Datasets with Consecutive Repetitions.	Marion Leleu,Christophe Rigotti,Jean-François Boulicaut,Guillaume Euvrard	2003	Constraint-Based Mining of Sequential Patterns over Datasets with Consecutive Repetitions.
PKDD	Towards Behaviometric Security Systems: Learning to Identify a Typist.	Mordechai Nisenson,Ido Yariv,Ran El-Yaniv,Ron Meir	2003	Towards Behaviometric Security Systems: Learning to Identify a Typist.
PKDD	Efficient Density Clustering Method for Spatial Data.	Fei Pan,Baoying Wang,Yi Zhang,Dongmei Ren,Xin Hu,William Perrizo	2003	Efficient Density Clustering Method for Spatial Data.
PKDD	Statistical sigma-Partition Clustering over Data Streams.	Nam Hun Park,Won Suk Lee	2003	Statistical sigma-Partition Clustering over Data Streams.
PKDD	Symbolic Distance Measurements Based on Characteristic Subspaces.	Marcus-Christopher Ludl	2003	Symbolic Distance Measurements Based on Characteristic Subspaces.
PKDD	Enriching Relational Learning with Fuzzy Predicates.	Henri Prade,Gilles Richard,Mathieu Serrurier	2003	Enriching Relational Learning with Fuzzy Predicates.
PKDD	Bottom-Up Learning of Logic Programs for Information Extraction from Hypertext Documents.	Bernd Thomas	2003	Bottom-Up Learning of Logic Programs for Information Extraction from Hypertext Documents.
PKDD	Predicting Outliers.	Luís Torgo,Rita P. Ribeiro	2003	Predicting Outliers.
PKDD	Taking Causality Seriously: Propensity Score Methodology Applied to Estimate the Effects of Marketing Interventions.	Donald B. Rubin	2003	Taking Causality Seriously: Propensity Score Methodology Applied to Estimate the Effects of Marketing Interventions.
PKDD	Mining Rules of Multi-level Diagnostic Procedure from Databases.	Shusaku Tsumoto	2003	Mining Rules of Multi-level Diagnostic Procedure from Databases.
PKDD	Text Categorisation Using Document Profiling.	Maximilien Sauban,Bernhard Pfahringer	2003	Text Categorisation Using Document Profiling.
PKDD	Learning Characteristic Rules Relying on Quantified Paths.	Teddy Turmeaux,Ansaf Salleb,Christel Vrain,Daniel Cassard	2003	Learning Characteristic Rules Relying on Quantified Paths.
PKDD	A Simple Algorithm for Topic Identification in 0-1 Data.	Jouni K. Seppänen,Ella Bingham,Heikki Mannila	2003	A Simple Algorithm for Topic Identification in 0-1 Data.
PKDD	Topic Learning from Few Examples.	Huaiyu Zhu,Shivakumar Vaithyanathan,Mahesh V. Joshi	2003	Topic Learning from Few Examples.
PKDD	Arbogodaï, a New Approach for Decision Trees.	Djamel A. Zighed,Gilbert Ritschard,Walid Erray,Vasile-Marian Scuturici	2003	Arbogodaï, a New Approach for Decision Trees.
PKDD	Knowledge Discovery in Databases: PKDD 2003, 7th European Conference on Principles and Practice of Knowledge Discovery in Databases, Cavtat-Dubrovnik, Croatia, September 22-26, 2003, Proceedings	Nada Lavrac,Dragan Gamberger,Hendrik Blockeel,Ljupco Todorovski	2003	Knowledge Discovery in Databases: PKDD 2003, 7th European Conference on Principles and Practice of Knowledge Discovery in Databases, Cavtat-Dubrovnik, Croatia, September 22-26, 2003, Proceedings
SDM	Finding Clusters of Different Sizes, Shapes, and Densities in Noisy, High Dimensional Data.	Levent Ertöz,Michael Steinbach,Vipin Kumar	2003	Finding Clusters of Different Sizes, Shapes, and Densities in Noisy, High Dimensional Data.
SDM	Mining Frequent Sequential Patterns under Regular Expressions: A Highly Adaptive Strategy for Pushing Contraints.	Hunor Albert-Lorincz,Jean-François Boulicaut	2003	Mining Frequent Sequential Patterns under Regular Expressions: A Highly Adaptive Strategy for Pushing Contraints.
SDM	Extracting Cyber Communities through Patterns.	Tassos Argyros,Charis Ermopoulos,Vassiliki Pavlaki,Nidal Al-Said	2003	Extracting Cyber Communities through Patterns.
SDM	Hierarchical Document Clustering using Frequent Itemsets.	Benjamin C. M. Fung,Ke Wang,Martin Ester	2003	Hierarchical Document Clustering using Frequent Itemsets.
SDM	Anytime Query-Tuned Kernel Machines via Cholesky Factorization.	Dennis DeCoste	2003	Anytime Query-Tuned Kernel Machines via Cholesky Factorization.
SDM	Dynamic Classification of Online Customers.	Dimitris Bertsimas,Adam J. Mersereau,Nitin R. Patel	2003	Dynamic Classification of Online Customers.
SDM	A New Gravitational Clustering Algorithm.	Jonatan Gómez,Dipankar Dasgupta,Olfa Nasraoui	2003	A New Gravitational Clustering Algorithm.
SDM	Nonparametric Density Estimation: Toward Computational Tractability.	Alexander G. Gray,Andrew W. Moore	2003	Nonparametric Density Estimation: Toward Computational Tractability.
SDM	Fast Online SVD Revisions for Lightweight Recommender Systems.	Matthew Brand	2003	Fast Online SVD Revisions for Lightweight Recommender Systems.
SDM	Mixture Models and Frequent Sets: Combining Global and Local Methods for 0-1 Data.	Jaakko Hollmén,Jouni K. Seppänen,Heikki Mannila	2003	Mixture Models and Frequent Sets: Combining Global and Local Methods for 0-1 Data.
SDM	Cube Lattices: A Framework for Multidimensional Data Mining.	Alain Casali,Rosine Cicchetti,Lotfi Lakhal	2003	Cube Lattices: A Framework for Multidimensional Data Mining.
SDM	PageRank: HITS and a Unified Framework for Link Analysis.	Chris H. Q. Ding,Xiaofeng He,Parry Husbands,Hongyuan Zha,Horst D. Simon	2003	Two popular link-based webpage ranking algorithms are (i) PageRank[1] and (ii) HITS (Hypertext Induced Topic Selection)[3]. HITS makes the crucial distinction of hubs and authorities and computes them in a mutually reinforcing way. PageRank considers the hyperlink weight normalization and the equilibrium distribution of random surfers as the citation score. We generalize and combine these key concepts into a unified framework, in which we prove that rankings produced by PageRank and HITS are both highly correlated with the ranking by in-degree and out-degree.
SDM	The Analysis of Asthma and Exposure Data using Geographic Information Systems and Data Mining Information.	Patricia B. Cerrito,George R. Barnes,Robert W. Forbes	2003	The Analysis of Asthma and Exposure Data using Geographic Information Systems and Data Mining Information.
SDM	The Application of Text Mining Software to Examine Coded Information.	Patricia B. Cerrito,James Cox	2003	The Application of Text Mining Software to Examine Coded Information.
SDM	Learning Bayesian Network Structure from Distributed Data.	R. Chen,Krishnamoorthy Sivakumar,H. Khargupta	2003	Learning Bayesian Network Structure from Distributed Data.
SDM	Estimation of Topological Dimension.	Douglas R. Hundley,Michael J. Kirby	2003	Estimation of Topological Dimension.
SDM	Feature Mining Paradigms for Scientific Data.	Ming Jiang,Tat-Sang Choy,Sameep Mehta,Matt Coatney,Steve Barr,Kaden Hazzard,David Richie,Srinivasan Parthasarathy,Raghu Machiraju,David Thompson,John Wilkins,Boyd Gatlin	2003	Feature Mining Paradigms for Scientific Data.
SDM	Communication and Memory Efficient Parallel Decision Tree Construction.	Ruoming Jin,Gagan Agrawal	2003	Communication and Memory Efficient Parallel Decision Tree Construction.
SDM	On the Techniques for Data Clustering with Numerical Constraints.	Bi-Ru Dai,Cheng-Ru Lin,Ming-Syan Chen	2003	On the Techniques for Data Clustering with Numerical Constraints.
SDM	ApproxMAP: Approximate Mining of Consensus Sequential Patterns.	Hye-Chung Kum,Jian Pei,Wei Wang,Dean Duncan	2003	ApproxMAP: Approximate Mining of Consensus Sequential Patterns.
SDM	On using Page Cooccurrences for Computing Clickstream Similarity.	Ravi Kothari,Parul A. Mittal,Vivek Jain,Mukesh K. Mohania	2003	On using Page Cooccurrences for Computing Clickstream Similarity.
SDM	Data-Mining of a Large Virtual Community: Relationship between Users DB and the Web-Log File.	Sergio M. Savaresi,Simone Garatti,Sergio Bittanti,Luca La Brocca	2003	Data-Mining of a Large Virtual Community: Relationship between Users DB and the Web-Log File.
SDM	A Comparative Study of Anomaly Detection Schemes in Network Intrusion Detection.	Aleksandar Lazarevic,Levent Ertöz,Vipin Kumar,Aysel Ozgur,Jaideep Srivastava	2003	A Comparative Study of Anomaly Detection Schemes in Network Intrusion Detection.
SDM	Active Sampling: An Effective Approach to Feature Selection.	Huan Li,Hongjun Lu,Lei Yu	2003	Active Sampling: An Effective Approach to Feature Selection.
SDM	StarClass: Interactive Visual Classification using Star Coordinates.	Soon Tee Teoh,Kwan-Liu Ma	2003	StarClass: Interactive Visual Classification using Star Coordinates.
SDM	An Outlier-based Data Association Method for Linking Criminal Incidents.	Song Lin,Donald E. Brown	2003	Serial criminals are a major threat in the modern society. Associating incidents committed by the same offender is of great importance in studying serial criminals. In this paper, we present a new outlier-based approach to resolve this criminal incident association problem. In this approach, criminal incident data are first modeled into a number of cells, and then a measurement function, called outlier score function, is defined over these cells. Incidents in a cell are determined to be associated with each other when the score is significant enough. We applied our approach to a robbery dataset from Richmond, VA. Results show that this method can effectively solve the criminal incident association problem.
SDM	Using Low-Memory Representations to Cluster Very Large Data Sets.	David Littau,Daniel Boley	2003	Using Low-Memory Representations to Cluster Very Large Data Sets.
SDM	Decision Tree Classification of Spatial Data Patterns from Videokeratography using Zernicke Polynomials.	Michael D. Twa,Srinivasan Parthasarathy,Thomas W. Raasch,Mark Bullimore	2003	Decision Tree Classification of Spatial Data Patterns from Videokeratography using Zernicke Polynomials.
SDM	Sort-Merge Feature Selection for Video Data.	Yan Liu,John R. Kender	2003	Sort-Merge Feature Selection for Video Data.
SDM	Detection of Underrepresented Biological Sequences using Class-Conditional Distribution Models.	Slobodan Vucetic,Dragoljub Pokrajac,Hongbo Xie,Zoran Obradovic	2003	Detection of Underrepresented Biological Sequences using Class-Conditional Distribution Models.
SDM	Field-Theoretic Methods for Intractable Probabilistic Models.	Dennis Lucarelli,Cheryl Resch,I-Jeng Wang,Fernando J. Pineda	2003	Field-Theoretic Methods for Intractable Probabilistic Models.
SDM	Efficient Unsupervised Mining from Noisy Data Sets: Application to Clustering Co-occurrence Data.	Hiroshi Mamitsuka	2003	Efficient Unsupervised Mining from Noisy Data Sets: Application to Clustering Co-occurrence Data.
SDM	ATLaS: A Native Extension of SQL for Data Mining.	Haixun Wang,Carlo Zaniolo	2003	ATLaS: A Native Extension of SQL for Data Mining.
SDM	Mining Changes of Classification by Correspondence Tracing.	Ke Wang,Senqiang Zhou,Ada Wai-Chee Fu,Jeffrey Xu Yu	2003	Mining Changes of Classification by Correspondence Tracing.
SDM	Mining Temporal Databases for Subsequence Patterns.	Wen Niu,Raj Bhatnagar	2003	Mining Temporal Databases for Subsequence Patterns.
SDM	CloSpan: Mining Closed Sequential Patterns in Large Databases.	Xifeng Yan,Jiawei Han,Ramin Afshar	2003	CloSpan: Mining Closed Sequential Patterns in Large Databases.
SDM	STAMP: On Discovery of Statistically Important Pattern Repeats in Long Sequential Data.	Jiong Yang,Wei Wang,Philip S. Yu	2003	STAMP: On Discovery of Statistically Important Pattern Repeats in Long Sequential Data.
SDM	Approximate Query Answering by Model Averaging.	Dmitry Pavlov,Padhraic Smyth	2003	Approximate Query Answering by Model Averaging.
SDM	CPAR: Classification based on Predictive Association Rules.	Xiaoxin Yin,Jiawei Han	2003	CPAR: Classification based on Predictive Association Rules.
SDM	Generalized Sensitivity Analysis: A Framework for Evaluating Data Analysis Results.	Ronald K. Pearson	2003	Generalized Sensitivity Analysis: A Framework for Evaluating Data Analysis Results.
SDM	Detecting Periodicity in Nonideal Datasets.	Ronald K. Pearson,Harri Lähdesmäki,Heikki Huttunen,Olli Yli-Harja	2003	Detecting Periodicity in Nonideal Datasets.
SDM	Scalable, Balanced Model-based Clustering.	Shi Zhong,Joydeep Ghosh	2003	Scalable, Balanced Model-based Clustering.
SDM	Proceedings of the Third SIAM International Conference on Data Mining, San Francisco, CA, USA, May 1-3, 2003	Daniel Barbará,Chandrika Kamath	2003	Proceedings of the Third SIAM International Conference on Data Mining, San Francisco, CA, USA, May 1-3, 2003
ICDM	Test-Cost Sensitive Naive Bayes Classification.	Xiaoyong Chai,Lin Deng,Qiang Yang,Charles X. Ling	2004	Inductive learning techniques such as the naive Bayes and decision tree algorithms have been extended in the past to handle different types of costs mainly by distinguishing different costs of classification errors. However, it is an equally important issue to consider how to handle the test costs associated with querying the missing values in a test case. When the value of an attribute is missing in a test case, it may or may not be worthwhile to take the effort to obtain its missing value, depending on how much the value will result in a potential gain in the classification accuracy. In this paper, we show how to obtain a test-cost sensitive naive Bayes classifier (csNB) by including a test strategy which determines how unknown attributes are selected to perform test on in order to minimize the sum of the mis-classification costs and test costs. We propose and evaluate several potential test strategies including one that allows several tests to be done at once. We empirically evaluate the csNB method, and show that it compares favorably with its decision tree counterpart.
ICDM	Incremental Mining of Frequent XML Query Pattern.	Yi Chen,Liang Huai Yang,Yu Guo Wang	2004	Recently, the discovering of frequent XML query patterns gains its focus due to its many applications in XML data management, and several algorithms have been proposed to discover frequent query patterns using the frequent structure mining techniques. In this paper we consider the problem of incremental mining of frequent XML query patterns. We propose a novel method to minimize the I/O and computation requirements for handling incremental updates.
ICDM	Spam Filtering using a Markov Random Field Model with Variable Weighting Schemas.	Shalendra Chhabra,William S. Yerazunis,Christian Siefkes	2004	In this paper we present a Markov Random Field model based approach to filter spam. Our approach examines the importance of the neighborhood relationship (MRF cliques) among words in an email message for the purpose of spam classification. We propose and test several different theoretical bases for weighting schemes among corresponding neighborhood windows. Our results demonstrate that unexpected side effects depending on the neighborhood window size may have larger accuracy impact than the neighborhood relationship effects of the Markov Random Field.
ICDM	Moment: Maintaining Closed Frequent Itemsets over a Stream Sliding Window.	Yun Chi,Haixun Wang,Philip S. Yu,Richard R. Muntz	2004	This paper considers the problem of mining closed frequent itemsets over a sliding window using limited memory space. We design a synopsis data structure to monitor transactions in the sliding window so that we can output the current closed frequent itemsets at any time. Due to time and memory constraints, the synopsis data structure cannot monitor all possible itemsets. However, monitoring only frequent itemsets will make it impossible to detect new itemsets when they become frequent. In this paper, we introduce a compact data structure, the closed enumeration tree (CET), to maintain a dynamically selected set of itemsets over a sliding-window. The selected itemsets consist of a boundary between closed frequent itemsets and the rest of the itemsets. Concept drifts in a data stream are reflected by boundary movements in the CET. In other words, a status change of any itemset (e.g., from non-frequent to frequent) must occur through the boundary. Because the boundary is relatively stable, the cost of mining closed frequent itemsets over a sliding window is dramatically reduced to that of mining transactions that can possibly cause boundary movements in the CET. Our experiments show that our algorithm performs much better than previous approaches.
ICDM	Using Emerging Patterns and Decision Trees in Rare-Class Classification.	Hamad Alhammady,Kotagiri Ramamohanarao	2004	The problem of classifying rarely occurring cases is faced in many real life applications. The scarcity of the rare cases makes it difficult to classify them correctly using traditional classifiers. In this paper, we propose a new approach to use emerging patterns (EPs) and decision trees (DTs) in rare-class classification (EPDT). EPs are those itemsets whose supports in one class are significantly higher than their supports in the other classes. EPDT employs the power of EPs to improve the quality of rare-case classification. To achieve this aim, we first introduce the idea of generating new non-existing rare-class instances, and then we over-sample the most important rare-class instances. Our experiments show that EPDT outperforms many classification methods.
ICDM	An Adaptive Learning Approach for Noisy Data Streams.	Fang Chu,Yizhou Wang,Carlo Zaniolo	2004	Two critical challenges typically associated with mining data streams are concept drift and data contamination. To address these challenges, we seek learning techniques and models that are robust to noise and can adapt to changes in timely fashion. We approach the stream-mining problem using a statistical estimation framework, and propose a fast and robust discriminative model for learning noisy data streams. We build an ensemble of classifiers to achieve timely adaptation by weighting classifiers in a way that maximizes the likelihood of the data. We further employ robust statistical techniques to alleviate the problem of noise sensitivity. Experimental results on both synthetic and real-life data sets demonstrate the effectiveness of this new model learning approach.
ICDM	Discovery of Functional Relationships in Multi-Relational Data using Inductive Logic Programming.	Alexessander Alves,Rui Camacho,Eugenio Oliveira	2004	ILP systems have been largely applied to datamining classification tasks with a considerable success. The use of ILP systems in regression tasks has been far less successful. Current systems have very limited numerical reasoning capabilities, which limits the application of ILP to discovery of functional relationships of numeric nature. This paper proposes improvements in numerical reasoning capabilities of ILP systems for dealing with regression tasks. It proposes the use of statistical-based techniques like Model Validation and Model Selection to improve noise handling and it introduces a new search stopping criterium based on the PAC method to evaluate learning performance. We have found these extensions essential to improve on results over machine learning and statistical-based algorithms used in the empirical evaluation study.
ICDM	Scalable Multi-Relational Association Mining.	Amanda Clare,Hugh E. Williams,Nicholas Lester	2004	We propose the new RADAR technique for multi-relational data mining. This permits the mining of very large collections and provides a new technique for discovering multi-relational associations. Results show that RADAR is reliable and scalable for mining a large yeast homology collection, and that it does not have the main-memory scalability constraints of the Farmer and Warmr tools.
ICDM	An Evaluation of Approaches to Classification Rule Selection.	Frans Coenen,Paul H. Leng	2004	In this paper a number of Classification Rule evaluation measures are considered. In particular the authors review the use of a variety of selection techniques used to order classification rules contained in a classifier, and a number of mechanisms used to classify unseen data. The authors demonstrate that rule ordering founded on the size of antecedent works well given certain conditions.
ICDM	Attribute Measurement Policies for Time and Cost Sensitive Classification.	Andrew Arnt,Shlomo Zilberstein	2004	Attribute measurement is an important component of classification algorithms, which could limit their applicability in realtime settings. The time taken to assign a value to an unknown attribute may reduce the overall utility of the final result. We identify three different costs that must be considered, including a time sensitive utility function. We model this attribute measurement problem as a Markov decision process (MDP), and build a policy to control this process using AO* heuristic search. The results offer a cost-effective approach to attribute measurement and classification for a variety of realtime applications.
ICDM	Mining Frequent Closed Patterns in Microarray Data.	Gao Cong,Kian-Lee Tan,Anthony K. H. Tung,Feng Pan	2004	Microarray data typically contains a large number of columns and a small number of rows, which poses a great challenge for existing frequent (closed) pattern mining algorithms that discover patterns in item enumeration space. In this paper, we propose two new algorithms that explore the row enumeration space to mine frequent closed patterns. Several experiments on real-life gene expression data show that the new algorithms are faster than existing algorithms, including CLOSET, CHARM, CLOSET+ and CARPENTER.
ICDM	Detection of Significant Sets of Episodes in Event Sequences.	Mikhail J. Atallah,Robert Gwadera,Wojciech Szpankowski	2004	"We present a method for a reliable detection of ""unusual"" sets of episodes in the form of many pattern sequences, scanned simultaneously for an occurrence as a subsequence in a large event stream within a window of size w. We also investigate the important special case of all permutations of the same sequence, which models the situation where the order of events in an episode does not matter, e.g., when events correspond to purchased market basket items. In order to build a reliable monitoring system we compare obtained measurements to a reference model which in our case is a probabilistic model (Bernoulli or Markov). We first present a precise analysis that leads to a construction of a threshold. The difficulties of carrying out a probabilistic analysis for an arbitrary set of patterns, stems from the possible simultaneous occurrence of many members of the set as subsequences in the same window, the fact that the different patterns typically do have common symbols or common subsequences or possibly common prefixes, and that they may have different lengths. We also report on extensive experimental results, carried out on the Wal-Mart transactions database, that show a remarkable agreement with our theoretical analysis. This paper is an extension of our previous work in [Reliable detection of episodes in event sequences] where we laid out foundation for the problem of the reliable detection of an ""unusual"" episodes, but did not consider more than one episode scanned simultaneously for an occurrence."
ICDM	Clustering on Demand for Multiple Data Streams.	Bi-Ru Dai,Jen-Wei Huang,Mi-Yen Yeh,Ming-Syan Chen	2004	In the data stream environment, the patterns generated by the mining techniques are usually distinct at different time because of the evolution of data. In order to deal with various types of multiple data streams and to support flexible mining requirements, we devise in this paper a Clustering on Demand framework, abbreviated as COD framework, to dynamically cluster multiple data streams. While providing a general framework of clustering on multiple data streams, the COD framework has two major features, namely one data scan for online statistics collection and compact multi-resolution approximations, which are designed to address, respectively, the time and the space constraints in a data stream environment. Furthermore, with the multi-resolution approximations of data streams, flexible clustering demands can be supported.
ICDM	Detecting Patterns of Appliances from Total Load Data Using a Dynamic Programming Approach.	Michael Baranski,Jürgen Voss	2004	Nonintrusive Appliance Load Monitoring (NIALM) systems require sufficient accurate total load data to separate the load into its major appliances. The most available solutions separate the whole electric energy consumption based on the measurement of all three voltages and currents. Aside from the cost for special measuring devices, the intrusion into the local installation is the main problem for reaching a high market distribution. The use of standard digital electricity meters could avoid this problem but the loss of information of the measured data has to be compensated by more intelligent algorithms and implemented rules to disaggregate the total load trace of only the active power measurements. The paper presents a new NIALM approach to analyse data, collected form a standard digital electricity meter. To disaggregate the consumption of the entire active power into its major electrical end uses, an algorithm consisting of clustering methods, a genetic algorithm and a dynamic programming approach is presented. The genetic algorithm is used to combine frequently occuring events to create hypothetical finite state machines to model detectable appliances. The time series of each finite state machine is optimized using a dynamic programming method similar to the viterbi algorithm.
ICDM	Subspace Selection for Clustering High-Dimensional Data.	Christian Baumgartner,Claudia Plant,Karin Kailing,Hans-Peter Kriegel,Peer Kröger	2004	"In high-dimensional feature spaces traditional clustering algorithms tend to break down in terms of efficiency and quality. Nevertheless, the data sets often contain clusters which are hidden in various subspaces of the original feature space. In this paper, we present a feature selection technique called SURFING (SUbspaces Relevant For clusterING) that finds all subspaces interesting for clustering and sorts them by relevance. The sorting is based on a quality criterion for the interestingness of a subspace using the k-nearest neighbor distances of the objects. As our method is more or less parameterless, it addresses the unsupervised notion of the data mining task ""clustering"" in a best possible way. A broad evaluation based on synthetic and real-world data sets demonstrates that SURFING is suitable to find all relevant subspaces in high dimensional, sparse data sets and produces better results than comparative methods."
ICDM	Multi-View Clustering.	Steffen Bickel,Tobias Scheffer	2004	We consider clustering problems in which the available attributes can be split into two independent subsets, such that either subset suffices for learning. Example applications of this multi-view setting include clustering of web pages which have an intrinsic view (the pages themselves) and an extrinsic view (e.g., anchor texts of inbound hyperlinks); multi-view learning has so far been studied in the context of classification. We develop and study partitioning and agglomerative, hierarchical multi-view clustering algorithms for text data. We find empirically that the multi-view versions of k-Means and EM greatly improve on their single-view counterparts. By contrast, we obtain negative results for agglomerative hierarchical multi-view clustering. Our analysis explains this surprising phenomenon.
ICDM	A Biobjective Model to Select Features with Good Classification Quality and Low Cost.	Emilio Carrizosa,Belen Martin-Barragan,Dolores Romero Morales	2004	In this paper we address a multi-group classification problem in which we want to take into account, together with the generalization ability, cots associated with the features. This cost is not limited to an economical payment, but can also refer to risk, computational effort, space requirements, etc. In order to get a good generalization ability, we use Support Vector Machines (SVM) as the basic mechanism by considering the maximization of the margin. We formulate the problem as a biobjective mixed integer problem, for which Pareto optimal solutions can be obtained.
ICDM	Text Classification by Boosting Weak Learners based on Terms and Concepts.	Stephan Bloehdorn,Andreas Hotho	2004	Document representations for text classification are typically based on the classical Bag-Of-Words paradigm. This approach comes with deficiencies that motivate the integration of features on a higher semantic level than single words. In this paper we propose an enhancement of the classical document representation through concepts extracted from background knowledge. Boosting is used for actual classification. Experimental evaluations on two well known text corpora support our approach through consistent improvement of the results.
ICDM	Density Connected Clustering with Local Subspace Preferences.	Christian Böhm,Karin Kailing,Hans-Peter Kriegel,Peer Kröger	2004	Many clustering algorithms tend to break down in high-dimensional feature spaces, because the clusters often exist only in specific subspaces (attribute subsets) of the original feature space. Therefore, the task of projected clustering (or subspace clustering) has been defined recently. As a novel solution to tackle this problem, we propose the concept of local subspace preferences, which captures the main directions of high point density. Using this concept we adopt density-based clustering to cope with high-dimensional data. In particular, we achieve the following advantages over existing approaches: Our proposed method has a determinate result, does not depend on the order of processing, is robust against noise, performs only one single scan over the database, and is linear in the number of dimensions. A broad experimental evaluation shows that our approach yields results of significantly better quality than recent work on clustering high-dimensional data.
ICDM	Extensible Markov Model.	Margaret H. Dunham,Yu Meng,Jie Huang	2004	A Markov Chain is a popular data modeling tool. This paper presents a variation of Markov Chain, namely Extensible Markov Model (EMM). By providing a dynamically adjustable structure, EMM overcomes the problems caused by the static nature of the traditional Markov Chain. Therefore, EMMs are particularly well suited to model spatiotemporal data such as network traffic, environmental data, weather data, and automobile traffic. Performance studies using EMMs for spatiotemporal prediction problems show the advantages of this approach.
ICDM	Using Representative-Based Clustering for Nearest Neighbor Dataset Editing.	Christoph F. Eick,Nidal M. Zeidat,Ricardo Vilalta	2004	The goal of dataset editing in instance-based learning is to remove objects from a training set in order to increase the accuracy of a classifier. For example, Wilson editing removes training examples that are misclassified by a nearest neighbor classifier so as to smooth the shape of the resulting decision boundaries. This paper revolves around the use of representative-based clustering algorithms for nearest neighbor dataset editing. We term this approach supervised clustering editing. The main idea is to replace a dataset by a set of cluster prototypes. A novel clustering approach called supervised clustering is introduced for this purpose. Our empirical evaluation using eight UCI datasets shows that both Wilson and supervised clustering editing improve accuracy on more than 50% of the datasets tested. However, supervised clustering editing achieves four times higher compression rates than Wilson editing.
ICDM	On Closed Constrained Frequent Pattern Mining.	Francesco Bonchi,Claudio Lucchese	2004	Constrained frequent patterns and closed frequent patterns are two paradigms aimed at reducing the set of extracted patterns to a smaller, more interesting, subset. Although a lot of work has been done with both these paradigms, there is still confusion around the mining problem obtained by joining closed and constrained frequent patterns in a unique framework. In this paper we shed light on this problem by providing a formal definition and a thorough characterization. Wealso study computational issues and show how to combine the most recent results in both paradigms, providing a very efficient algorithm which exploits the two requirements (satisfying constraints and being closed) together at mining time in order to reduce the computation as much as possible.
ICDM	Efficient Density-Based Clustering of Complex Objects.	Stefan Brecheisen,Hans-Peter Kriegel,Martin Pfeifle	2004	Nowadays data mining in large databases of complex objects from scientific, engineering or multimedia applications is getting more and more important. In many different application domains complex object representations along with complex distance functions are used for measuring the similarity between objects. Often not only these complex distance measures are available but also simpler distance functions which can be computed much more efficiently. Traditionally, the well known concept of multi-step query processing which is based on exact and lower-bounding approximative distance functions is used independently of data mining algorithms. In this paper, we will demonstrate how the paradigm of multi-step query processing can be integrated into the two density-based clustering algorithms DBSCAN and OPTICS resulting in a considerable efficiency boost. Our approach tries to confine itself to ¿-range queries on the simple distance functions and carries out complex distance computations only at that stage of the clustering algorithm where they are compulsory to compute the correct clustering result. In a broad experimental evaluation based on real-world test data sets, we demonstrate that our approach accelerates the generation of flat and hierarchical density-based clusterings by more than one order of magnitude.
ICDM	Matching in Frequent Tree Discovery.	Björn Bringmann	2004	"Various definitions and frameworks for discovering frequent trees in forests have been developed recently. At the heart of these frameworks lies the notion of matching, which determines when a pattern tree matches a tree in a data set. We introduce a novel notion of tree matching for use in frequent tree mining and we show that it generalizes the framework of Zaki while still being more specific than that of Termier et al. Furthermore, we show how Zaki's TreeMinerV algorithm can be adapted towards our notion of tree matching. Experiments show the promise of the approach."
ICDM	Decision Tree Evolution Using Limited Number of Labeled Data Items from Drifting Data Streams.	Wei Fan,Yi-an Huang,Philip S. Yu	2004	"Most previously proposed mining methods on data streams make an unrealistic assumption that ""labelled"" data stream is readily available and can be mined at anytime. However, in most real-world problems, labelled data streams are rarely immediately available. Due to this reason, models are reconstructed only when labelled data become available periodically. This passive stream mining model has several drawbacks. We propose a new concept of demand-driven active data mining. In active mining, the loss of the model is either continuously guessed without using any true class labels or estimated, whenever necessary, from a small number of instances whose actual class labels are verified by paying an affordable cost. When the estimated loss is more than a tolerable threshold, the model evolves by using a small number of instances with verified true class labels. Previous work on active mining concentrates on error guess and estimation. In this paper, we discuss several approaches on decision tree evolution."
ICDM	The Anatomy of a Hierarchical Clustering Engine for Web-page, News and Book Snippets.	Paolo Ferragina,Antonio Gulli	2004	In this paper, we investigate the web snippet hierarchical clustering problem in its full extent by devising an algorithmic solution, and a software prototype called SnakeT (accessible at http://roquefort.di.unipi.it/), that: (1) draws the snippets from 16 Web search engines, the Amazon collection of books a9.com, the news of Google News and the blogs of Blogline; (2) builds the clusters on-the-fly (ephemeral clustering) in response to a user query without adopting any pre-defined organization in categories; (3) labels the clusters with sentences of variable length, drawn from the snippets and possibly missing some terms, provided they are not too many;
ICDM	Communication Efficient Construction of Decision Trees Over Heterogeneously Distributed Data.	Chris Giannella,Kun Liu,Todd Olsen,Hillol Kargupta	2004	We present an algorithm designed to efficiently construct a decision tree over heterogeneously distributed data without centralizing. We compare our algorithm against a standard centralized decision tree implementation in terms of accuracy as well as the communication complexity. Our experimental results show that by using only 20% of the communication cost necessary to centralize the data we can achieve trees with accuracy at least 80% of the trees produced by the centralized version.
ICDM	A Machine Learning Approach to Improve Congestion Control over Wireless Computer Networks.	Pierre Geurts,Ibtissam El Khayat,Guy Leduc	2004	In this paper, we present the application of machine learning techniques to the improvement of the congestion control of TCP in wired/wireless networks. TCP is sub-optimal in hybrid wired/wireless networks because it reacts in the same way to losses due to congestion and losses due to link errors. We thus propose to use machine learning techniques to build automatically a loss classifier from a database obtained by simulations of random network topologies. Several machine learning algorithms are compared for this task and the best method for this application turns out to be decision tree boosting. It outperforms ad hoc classifiers proposed in the networking literature.
ICDM	LOADED: Link-Based Outlier and Anomaly Detection in Evolving Data Sets.	Amol Ghoting,Matthew Eric Otey,Srinivasan Parthasarathy	2004	In this paper, we present LOADED, an algorithm for outlier detection in evolving data sets containing both continuous and categorical attributes. LOADED is a tunable algorithm, wherein one can trade off computation for accuracy so that domain-specific response times are achieved. Experimental results show that LOADED provides very good detection and false positive rates, which are several times better than those of existing distance-based schemes.
ICDM	SVD based Term Suggestion and Ranking System.	David Gleich,Leonid Zhukov	2004	"In this paper, we consider the application of the singular value decomposition (SVD) to a search term suggestion system in a pay-for-performance search market. We propose a novel positive and negative refinement method based on orthogonal subspace projections. We demonstrate that SVD subspace-based methods: 1) expand coverage by reordering the results, and 2) enhance the clustered structure of the data. The numerical experiments reported in this paper were performed on Overture's pay-per-performance search market data."
ICDM	Non-Redundant Data Clustering.	David Gondek,Thomas Hofmann	2004	Data clustering is a popular approach for automatically finding classes, concepts, or groups of patterns. In practice this discovery process should avoid redundancies with existing knowledge about class structures or groupings, and reveal novel, previously unknown aspects of the data. In order to deal with this problem, we present an extension of the information bottleneck framework, called coordinated conditional information bottleneck, which takes negative relevance information into account by maximizing a conditional mutual information score subject to constraints. Algorithmically, one can apply an alternating optimization scheme that can be used in conjunction with different types of numeric and non-numeric attributes. We present experimental results for applications in text mining and computer vision.
ICDM	Fast and Exact Out-of-Core K-Means Clustering.	Anjan Goswami,Ruoming Jin,Gagan Agrawal	2004	Clustering has been one of the most widely studied topics in data mining and k-means clustering has been one of the popular clustering algorithms. K-means requires several passes on the entire dataset, which can make it very expensive for large disk-resident datasets. In view of this, a lot of work has been done on various approximate versions of k-means, which require only one or a small number of passes on the entire dataset. In this paper, we present a new algorithm which typically requires only one or a small numberof passes on the entire dataset, and provably produces the same cluster centers as reported by the original k-means algorithm. The algorithm uses sampling to create initial cluster centers, and then takes one or more passes over the entire dataset to adjust these cluster centers. We provide theoretical analysis to show that the cluster centers thus reported are the same as the ones computed by the original k-means algorithm. Experimental results from a number of real and synthetic datasets show speedup between a factor of 2 and 4.5, as compared to k-means.
ICDM	Mining Frequent Itemsets from Secondary Memory.	Gösta Grahne,Jianfei Zhu	2004	Mining frequent itemsets is at the core of mining association rules, and is by now quite well understood algorithmically for main memory databases. In this paper, we investigate approaches to mining frequent itemsets when the database or the data structures used in the mining are too large to fit in main memory. Experimental results show that our techniques reduce the required disk accesses by orders of magnitude, and enable truly scalable data mining.
ICDM	A Bayesian Framework for Regularized SVM Parameter Estimation.	Jens Gregor,ZhenQiu Liu	2004	The support vector machine (SVM) is considered here in the context of pattern classification. The emphasis is on the soft margin classifier which uses regularization to handle non-separable learning samples. We present an SVM parameter estimation algorithm that first identifies a subset of the learning samples that we call the support set and then determines not only the weights of the classifier but also the hyperparameter that controls the influence of the regularizing penalty term on basis thereof. We provide numerical results using several data sets from the public domain.
ICDM	Filling-in Missing Objects in Orders.	Toshihiro Kamishima,Shotaro Akaho	2004	Filling-in techniques are important, since missing values frequently appear in real data. Such techniques have been established for categorical or numerical values. Though lists of ordered objects are widely used as representational forms (e.g., Web search results, best-seller lists), filling-in techniques for orders have received little attention. We therefore propose a simple but effective technique to fill-in missing objects in orders. We built this technique into our collaborative filtering system.
ICDM	Generation of Attribute Value Taxonomies from Data for Data-Driven Construction of Accurate and Compact Classifiers.	Dae-Ki Kang,Adrian Silvescu,Jun Zhang,Vasant Honavar	2004	Attribute Value Taxonomies (AVT) have been shown to be useful in constructing compact, robust, and comprehensible classifiers. However, in many application domains, human-designed AVTs are unavailable. We introduce AVT-Learner, an algorithm for automated construction of attribute value taxonomies from data. AVT-Learner uses Hierarchical Agglomerative Clustering (HAC) to cluster attribute values based on the distribution of classes that co-occur with the values. We describe experiments on UCI data sets that compare the performance of AVT-NBL (an AVT-guided Naive Bayes Learner) with that of the standard Naive Bayes Learner (NBL) applied to the original data set. Our results show that the AVTs generated by AVT-Learner are competitive with human-generated AVTs (in cases where such AVTs are available). AVT-NBL using AVTs generated by AVT-Learner achieves classification accuracies that are comparable to or higher than those obtained by NBL; and the resulting classifiers are significantly more compact than those generated by NBL.
ICDM	Semi-Supervised Mixture-of-Experts Classification.	Grigoris J. Karakoulas,Ruslan Salakhutdinov	2004	We introduce a mixture-of-experts technique that is a generalization of mixture modeling techniques previously suggested for semi-supervised learning. We apply the bias-variance decomposition to semi-supervised classification and use the decomposition to study the effects from adding unlabeled data when learning a mixture model. Our empirical results indicate that the biggest gain from adding unlabeled data comes from the reduction of the model variance, whereas the behavior of the bias error term heavily depends on the correctness of the underlying model assumptions.
ICDM	Orthogonal Decision Trees.	Hillol Kargupta,Haimonti Dutta	2004	This paper introduces orthogonal decision trees that offer an effective way to construct a redundancy-free, accurate, and meaningful representation of large decision-tree-ensembles often created by popular techniques such as Bagging, Boosting, Random Forests and many distributed and data stream mining algorithms. Orthogonal decision trees are functionally orthogonal to each other and they correspond to the principal components of the underlying function space. This paper offers a technique to construct such trees based on eigen-analysis of the ensemble and offers experimental results to document the performance of orthogonal trees on grounds of accuracy and model complexity.
ICDM	Unimodal Segmentation of Sequences.	Niina Haiminen,Aristides Gionis	2004	We study the problem of segmenting a sequence into k pieces so that the resulting segmentation satisfies monotonicity or unimodality constraints. Unimodal functions can be used to model phenomena in which a measured variable first increases to a certain level and then decreases. We combine a well-known unimodal regression algorithm with a simple dynamic-programming approach to obtain an optimal quadratic-time algorithm for the problem of unimodal k-segmentation. In addition, we describe a more efficient greedy-merging heuristic that is experimentally shown to give solutions very close to the optimal. As a concrete application of our algorithms, we describe two methods for testing if a sequence behaves unimodally or not. Our experimental evaluation shows that our algorithms and the proposed unimodality tests give very intuitive results.
ICDM	ntegrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining.	Mehmet Kaya,Reda Alhajj	2004	ntegrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining.
ICDM	Query-Driven Support Pattern Discovery for Classification Learning.	Yiqiu Han,Wai Lam	2004	We propose a novel query-driven lazy learning algorithm which attempts to discover useful local patterns, called support patterns, for classifying a given query. The learning is customized to the query to avoid the horizon effect. We show that this query-driven learning algorithm can guarantee to discover all support patterns with perfect expected accuracy in polynomial time. The experimental results on benchmark data sets also demonstrate that our learning algorithm really has prominent learning performance.
ICDM	Feature-Based Prediction of Unknown Preferences for Nearest-Neighbor Collaborative Filtering.	Hyungil Kim,Juntae Kim,Jonathan L. Herlocker	2004	"Recommendation systems analyze user preferences and recommend items to a user by predicting the user's preference for those items.Among various kinds of recommendation methods, collaborative filtering (CF) has been widely used and successfully applied to practical applications.However, collaborative filtering has two inherent problems: data sparseness and the cold-start problems.In this paper, we propose a method of integrating additional feature information of users and items into CF to overcome the difficulties caused by sparseness and improve the accuracy of recommendation. Several experimental results that show the effectiveness of the proposed method are also presented."
ICDM	Dependencies between Transcription Factor Binding Sites: Comparison between ICA, NMF, PLSA and Frequent Sets.	Heli Hiisilä,Ella Bingham	2004	"Gene expression of eucaryotes is regulated through transcription factors, which are molecules able to attach to the binding sites in the DNA sequence. These binding sites are small pieces of DNA usually found upstream from the gene they regulate. As the binding sites play an important role in the gene expression, it is of interest to find out their characteristics. In this paper we look for dependencies and independencies between these binding sites using independent component analysis (ICA), non-negative matrix factorization (NMF), probabilistic latent semantic analysis (PLSA) and the method of frequent sets. The data used are human gene upstream regions and possible binding sites listed in a biological database. Also, results on the baker's yeast (S.Cerevisiae) upstream regions are briefly discussed for comparison. ICA, NMF and PLSA are latent variable methods that decompose the observed data into smaller components. Of these, ICA and NMF were originally aimed for continuous data. We show that these methods can be successfully used on discrete DNA data as well. PLSA and the method of frequent sets were created for discrete data sets. The above methods reveal partially overlapping sets of possible binding sites such that the binding sites within a set are dependent of each other. The methods of frequent sets and NMF give a good overview of the most common data structures, whereas using ICA and PLSA we find large sets that are surprisingly frequent. That is, sets of very frequently occurring possible binding sites can be found near hundreds or thousands of genes; also interesting but less frequent ones co-occur surprisingly often."
ICDM	Evolutionary Algorithms for Clustering Gene-Expression Data.	Eduardo R. Hruschka,Leandro Nunes de Castro,Ricardo J. G. B. Campello	2004	This work deals with the problem of automatically finding optimal partitions in bioinformatics datasets. We propose incremental improvements for a Clustering Genetic Algorithm (CGA), culminating in the Evolutionary Algorithm for Clustering (EAC). The CGA and its modified versions are evaluated in five gene-expression datasets, showing that the proposed EAC is a promising tool for clustering gene-expression data.
ICDM	Mining Ratio Rules Via Principal Sparse Non-Negative Matrix Factorization.	Chenyong Hu,Benyu Zhang,Shuicheng Yan,Qiang Yang,Jun Yan,Zheng Chen,Wei-Ying Ma	2004	"Association rules are traditionally designed to capture statistical relationship among itemsets in a given database. To additionally capture the quantitative association knowledge, F.Korn et al recently proposed a paradigm named Ratio Rules for quantifiable data mining. However, their approach is mainly based on Principle Component Analysis (PCA) and as a result, it cannot guarantee that the ratio coefficient is non-negative. This may lead to serious problems in the rules' application. In this paper, we propose a new method, called Principal Sparse Non-Negative Matrix Factorization (PSNMF), for learning the associations between itemsets in the form of Ratio Rules. In addition, we provide a support measurement to weigh the importance of each rule for the entire dataset."
ICDM	Transduction and Typicalness for Quality Assessment of Individual Classifications in Machine Learning and Data Mining.	Matjaz Kukar	2004	"In the past machine learning algorithms have been successfully used in many problems, and are emerging as valuable data analysis tools. However, their serious practical use is affected by the fact, that more often than not, they cannot produce reliable and unbiased assessments of their predictions' quality. In last years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper we propose a joint approach that compensates the mentioned weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense (e.g., a confidence level of 95% means that in 95% the predicted class is also a true class), as well as provides us with a general principle that is independent of to the particular underlying classifier We perform a series of tests with several different machine learning algorithms in several problem domains. We compare our results with that of a proprietary TCM-NN method as well as with kernel density estimation. We show that the proposed method significantly outperforms density estimation methods, and how it may be used to improve their performance."
ICDM	Mining Temporal Patterns Without Predefined Time Windows.	Tao Li,Sheng Ma	2004	"This paper proposes algorithms for discovering temporal patterns without predefined time windows.The problem of discovering temporal patterns is divided into two sub-tasks: (1) using ""cheap statistics"" for dependence testing and candidates removal (2) identifying the temporal relationships between dependent event types.The dependence problem is formulated as the problem of comparing two probability distributions and is solved using a technique reminiscent of the distance methods used in spatial point process, while the latter problem is solved using an approach based on Chi-Squared tests.Experiments are conducted to evalaute the effectiveness and scalability of the proposed methods."
ICDM	Mass Spectrum Labeling: Theory and Practice.	Zheng Huang,Lei Chen,Jin-yi Cai,Deborah S. Gross,David R. Musicant,Raghu Ramakrishnan,James J. Schauer,Stephen J. Wright	2004	"We introduce the problem of labeling a particle's mass spectrum with the substances it contains, and develop several formal representations of the problem, taking into account practical complications such as unknown compounds and noise. This task is currently a bottle-neck in analyzing data from a new generation of instruments for real-time environmental monitoring."
ICDM	Feature Selection via Supervised Model Construction.	Ye Huang,Paul J. McCullagh,Norman D. Black	2004	ReliefF is a feature mining technique, which has been successfully used in data mining applications.However, ReliefF is sensitive to the definition of relevance that is used in its implementation and when handling a large data set, it is computationally expensive.This paper presents an optimisation (Feature Selection via Supervised Model Construction) for data transformation and starter selection, and evaluates its effectiveness with C4.5.Experiments indicate that the proposed method gave improvement of computation efficiency whilst maintaining classification accuracy of trial data sets.
ICDM	GREW-A Scalable Frequent Subgraph Discovery Algorithm.	Michihiro Kuramochi,George Karypis	2004	Existing algorithms that mine graph datasets to discover patterns corresponding to frequently occurring subgraphs can operate efficiently on graphs that are sparse, contain a large number of relatively small connected components, have vertices with low and bounded degrees, and contain well-labeled vertices and edges. However, for graphs that do not share these characteristics, these algorithms become highly unscalable. In this paper we present a heuristic algorithm called GREW to overcome the limitations of existing complete or heuristic frequent subgraph discovery algorithms. GREW is designed to operate on a large graph and to find patterns corresponding to connected subgraphs that have a large number of vertex-disjoint embeddings. Our experimental evaluation shows that GREW is efficient, can scale to very large graphs, and find non-trivial patterns.
ICDM	Predicting Density-Based Spatial Clusters Over Time.	Chih Lai,Nga T. Nguyen	2004	Most of existing clustering algorithms are designed to discover snapshot clusters that reflect only the current status of a database. Snapshot clusters do not reveal the fact that clusters may either persist over a period of time, or slowly fade away as other clusters may gradually develop. Predicting dynamic cluster evolutions and their occurring periods are important because this information can guide users to prepare appropriate actions toward the right areas during the right time for the most effective results. In this paper we developed a simple but effective approach in predicting the future distance among object pairs. Objects that will be close in distance over different periods of time are then processed to discover density-based clusters that may occur or change over time.
ICDM	Mining Generalized Substructures from a Set of Labeled Graphs.	Akihiro Inokuchi	2004	"The problem of mining frequent itemsets in transactional data has been studied frequently and has yielded several algorithms that can find the itemsets within a limited amount of time. Some of them can derive ""generalized"" frequent itemsets consisting of items at any level of a taxonomy. Recently, several approaches have been proposed to mine frequent substructures (patterns) from a set of labeled graphs. The graph mining approaches are easily extended to mine generalized patterns where some vertices and/or edges have labels at any level of a taxonomy of the labels by extending the definition of ""subgraph"". However, the extended method outputs a massive set of the patterns most of which are over-generalized, which causes computation explosion. In this paper, an efficient and novel method is proposed to discover all frequent patterns which are not over-generalized from labeled graphs, when taxonomies on vertex and edge labels are available."
ICDM	Dynamic Daily-Living Patterns and Association Analyses in Tele-Care Systems.	Beng-Seuk Lee,Trevor P. Martin,Nick P. Clarke,Basim A. Majeed,Detlef Nauck	2004	"Tele-care systems aim to carry out intelligent analyses of a person's wellbeing using data about their daily activities. This is a very challenging task because the massive dataset is likely to be erroneous, possibly with misleading sections due to noise or missing values. Furthermore, the interpretation of the data is highly sensitive to the lifestyle of the monitored person and the environment in which they interact. In our tele-care project, sensor-network domain knowledge is used to overcome the difficulties of monitoring long-term wellbeing with an imperfect data source. In addition, a fuzzy association analysis is leveraged to implement a dynamic and flexible analysis over individual- and environment-dependent data."
ICDM	Divide and Prosper: Comparing Models of Customer Behavior From Populations to Individuals.	Tianyi Jiang,Alexander Tuzhilin	2004	This paper compares customer segmentation, 1-to-1, and aggregate marketing approaches across a broad range of experimental settings, including multiple segmentation levels, marketing datasets, dependent variables, and different types of classifiers, segmentation techniques, and predictive measures. Our experimental results show that, overall, 1-to-1 modeling significantly outperforms the aggregate approach among high-volume customers and is never worse than aggregate approach among low-volume customers. Moreover, the best segmentation techniques tend to outperform 1-to-1 modeling among low-volume customers.
ICDM	Classifying Biomedical Citations without Labeled Training Examples.	Xiaoli Li,Rohit Joshi,Sreeram Ramachandaran,Tze-Yun Leong	2004	"In this paper we introduce a novel technique for classifying text citations without labeled training examples. We first utilize the search results of a general search engine as original training data. We then proposed a mutually reinforcing learning algorithm (MRL) to mine the classification knowledge and to ""clean"" the training data. With the help of a set of established domain-specific ontological terms or keywords, the MRL mining step derives the relevant classification knowledge. The MRL cleaning step then builds a Naive Bayes classifier based on the mined classification knowledge and tries to clean the training set. The MRL algorithm is iteratively applied until a clean training set is obtained. We show the effectiveness of the proposed technique in the classification of biomedical citations from a large medical literature database."
ICDM	Efficient Relationship Pattern Mining Using Multi-Relational Iceberg-Cubes.	Dawit Yimam Seid,Sharad Mehrotra	2004	Efficient Relationship Pattern Mining Using Multi-Relational Iceberg-Cubes.
ICDM	Mining Associations by Linear Inequalities.	Tsau Young Lin	2004	"The main theorem is: Generalized associations of a relational table can be found by a finite set of linear inequalities within polynomial time. It is derived from the following three results, which were established in ICDM0'02 and are re-developed here. They are (1) Isomorphic Theorem: Isomorphic relations have isomorphic patterns. Such an isomorphism classifies relational tables into isomorphic classes. (2) A variant of the classical bitmaps indexes uniquely exists in each isomorphic class. We take it as the canonical model of the class. (3) All possible attributes/features can be generated by a generalized procedure of the classical AOG (attribute oriented generalization). Then, (4) the main theorem for canonical model is established. By isomorphism theorem, we had the final result (5)."
ICDM	Improving the Reliability of Decision Tree and Naive Bayes Learners.	David G. Lindsay,Siân Cox	2004	The C4.5 Decision Tree and Naive Bayes learners are known to produce unreliable probability forecasts. We have used simple Binning and Laplace Transform techniques to improve the reliability of these learners and compare their effectiveness with that of the newly developed Venn Probability Machine (VPM) meta-learner. We assess improvements in reliability using loss functions, Receiver Operator Characteristic (ROC) curves and Empirical Reliability Curves (ERC). The VPM outperforms the simple techniques to improve reliability, although at the cost of increased computational intensity and slight increase in error rate. These trade-offs are discussed.
ICDM	MMSS: Multi-Modal Story-Oriented Video Summarization.	Jia-Yu Pan,Hyung-Jeong Yang,Christos Faloutsos	2004	We propose multi-modal story-oriented video summarization (MMSS) which, unlike previous works that use fine-tuned, domain-specific heuristics, provides a domain-independent, graph-based framework. MMSS uncovers correlation between information of different modalities which gives meaningful story-oriented news video summaries. MMSS can also be applied for video retrieval, giving performance that matches the best traditional retrieval techniques (OKAPI and LSI), with no fine-tuned heuristics such as tf/idf.
ICDM	Improving Text Classification using Local Latent Semantic Indexing.	Tao Liu,Zheng Chen,Benyu Zhang,Wei-Ying Ma,Gongyi Wu	2004	"Latent Semantic Indexing (LSI) has been shown to be extremely useful in information retrieval, but it is not an optimal representation for text classification. It always drops the text classification performance when being applied to the whole training set (global LSI) because this completely unsupervised method ignores class discrimination while only concentrating on representation. Some local LSI methods have been proposed to improve the classification by utilizing class discrimination information. However, their performance improvements over original term vectors are still very limited. In this paper, we propose a new local LSI method called ""Local Relevancy Weighted LSI"" to improve text classification by performing a separate Single Value Decomposition (SVD) on the transformed local region of each class. Experimental results show that our method is much better than global LSI and traditional local LSI methods on classification within a much smaller LSI dimension."
ICDM	A Comparative Study of Linear and Nonlinear Feature Extraction Methods.	Cheong Hee Park,Haesun Park,Panos M. Pardalos	2004	This paper presents theoretical relationships among several generalized LDA algorithms and proposes computationally efficient approaches for them utilizing the relationships. Generalized LDA algorithms are extended nonlinearly by kernel methods resulting in nonlinear discriminant analysis. Performances and computational complexities of these linear and nonlinear discriminant analysis algorithms are compared.
ICDM	Revealing True Subspace Clusters in High Dimensions.	Jinze Liu,Karl Strohmaier,Wei Wang	2004	Subspace clustering is one of the best approaches for discovering meaningful clusters in high dimensional space. One cluster in high dimensional space may be transcribed into multiple distinct maximal clusters by projecting onto different subspaces. A direct consequence of clustering independently in each subspace is an overwhelmingly large set of overlapping clusters which may be significantly similar. To reveal the true underlying clusters, we propose a similarity measurement of the overlapping clusters. We adopt the model of Gaussian tailed hyper-rectangles to capture the distribution of any subspace cluster. A set of experiments on a synthetic dataset demonstrates the effectiveness of our approach. Application to real gene expression data also reveals impressive meta-clusters expected by biologists.
ICDM	Hybrid Pre-Query Term Expansion using Latent Semantic Analysis.	Laurence A. F. Park,Kotagiri Ramamohanarao	2004	Latent semantic retrieval methods (unlike vector space methods) take the document and query vectors and map them into a topic space to cluster related terms and documents. This produces a more precise retrieval but also a long query time. We present a new method of document retrieval which allows us to process the latent semantic information into a hybrid Latent Semantic-Vector Space query mapping. This mapping automatically expands the users query based on the latent semantic information in the document set. This expanded query is processed using a fast vector space method. Since we have the latent semantic data in a mapping, we are able to store and retrieve vector information in the same fast manner that the vector space method offers. Multiple mappings are combined to produce hybrid latent semantic retrieval which provide precision results 5% greater than the vector space method and fast query times.
ICDM	Learning Conditional Independence Tree for Ranking.	Jiang Su,Harry Zhang	2004	Accurate ranking is desired in many real-world data mining applications. Traditional learning algorithms, however, aim only at high classification accuracy. It has been observed that both traditional decision trees and naive Bayes produce good classification accuracy but poor probability estimates. In this paper, we use a new model, conditional independence tree (CITree), which is a combination of decision tree and naive Bayes and more suitable for ranking and more learnable in practice. We propose a novel algorithm for learning CITree for ranking, and the experiments show that the CITree algorithm outperforms the state-of-the-art decision tree learning algorithm C4.4 and naive Bayes significantly in yielding accurate rankings. Our work provides an effective data mining algorithm for applications in which an accurate ranking is required.
ICDM	Sparse Kernel Least Squares Classifier.	Ping Sun	2004	In this paper, we propose a new learning algorithm for constructing kernel least squares classifier. The new algorithm adopts a recursive learning way and a novel two-step sparsification procedure is incorporated into learning phase. These two most importantfeatures not only provide a feasible approach for large-scale problems as it is not necessary to store the entire kernel matrix, but also produce a very sparse model with fast training and testing time. Experimental results on a number of data classification problems are presented to demonstrate the competitiveness of new proposed algorithm.
ICDM	SCHISM: A New Approach for Interesting Subspace Mining.	Karlton Sequeira,Mohammed Javeed Zaki	2004	High-dimensional data pose challenges to traditional clustering algorithms due to their inherent sparsity and data tend to cluster in different and possibly overlapping subspaces of the entire feature space. Finding such subspaces is called subspace mining. We present SCHISM, a new algorithm for mining interesting subspaces, using the notions of support and Chernoff-Hoeffding bounds. We use a vertical representation of the dataset, and use a depth-first search with backtracking to find maximal interesting subspaces. We test our algorithm on a number of high-dimensional synthetic and real datasets to test its effectiveness.
ICDM	Finding Constrained Frequent Episodes Using Minimal Occurrences.	Xi Ma,HweeHwa Pang,Kian-Lee Tan	2004	Recurrent combinations of events within an event sequence, known as episodes, oftenreveal useful information. Most of the proposed episode mining algorithms adopt an apriori-like approach that generates candidates and then calculates their support levels. Obviously, such an approach is computationally expensive. Moreover, those algorithms are capable ofhandling only a limited range of constraints. In this paper, we introduce two miningalgorithms - Episode Prefix Tree (EPT) and Position Pairs Set (PPS) - based on a prefix-growth approach to overcome the above limitations. Both algorithms push constraints systematically into the mining process. Performance study shows that the proposed algorithms run considerably faster than MINEPI.
ICDM	A Transaction-Based Neighbourhood-Driven Approach to Quantifying Interestingness of Association Rules.	B. Shekar,Rajesh Natarajan	2004	"In this paper, we present a data-driven approach for ranking association rules (ARs) based on interestingness. The occurrence of unrelated or weakly related item-pairs in an AR is interesting. In the retail market-basket context, items may be related through various relationships arising due to mutual interaction, 'substitutability' and 'complementarity.' Item-relatedness is a composite of these relationships. We introduce three relatedness measures for capturing relatedness between item-pairs. These measures use the concept of function embedding to appropriately weigh the relatedness contributions due tocomplementarity and substitutability between items. We propose an interestingness coefficient by combining the three relatedness measures. We compare this with two objective measures of interestingness and show the intuitiveness of the proposed interestingness coefficient."
ICDM	An Adaptive Density-Based Clustering Algorithm for Spatial Database with Noise.	Daoying Ma,Aidong Zhang	2004	Clustering spatial data has various applications. Several clustering algorithms have been proposed to cluster objects in spatial databases. Spatial object distribution has significant effect on the results of clustering. Few of current algorithms consider the distribution of objects while processing clusters. In this paper, we propose an adaptive density-based clustering algorithm, ADBC, which uses a novel adaptive strategy for neighbor selection based on spatial object distribution to improve clustering accuracy. We perform a series of experiments on simulated data sets and real data sets. A comparison with DBSCAN and OPTICS shows the superiority of our new approach.
ICDM	Estimation of False Negatives in Classification.	Sandeep Mane,Jaideep Srivastava,San-Yih Hwang,Jamshid A. Vayghan	2004	"In many classification problems such as spam detection and network intrusion, a large number of unlabeled test instances are predicted negative by the classifier. However, the high costsas well as time constraints on an expert's time prevent further analysis of the ""predicted false"" class instances in order to segregate the false negatives from the true negatives. A systematic method is thus required to obtain an estimate of the number of false negatives. A capture-recapture based method can be used to obtain an ML-estimate of false negatives when two or more independent classifiers are available. In the case for which independence does not hold, we can apply log-linear models to obtain an estimate of false negatives. However, as shown in this paper, lesser the dependencies among the classifiers, better is the estimate obtained for false negatives. Thus, ideally independent classifiers should be used to estimate the false negatives in an unlabeled dataset. Experimental results on the spam dataset from the UCI Machine Learning Repository are presented."
ICDM	Aligning Boundary in Kernel Space for Learning Imbalanced Dataset.	Gang Wu,Edward Y. Chang	2004	An imbalanced training dataset poses serious problem for many real-world supervised learning tasks. In this paper, we propose a kernel-boundary-alignment algorithm, which considers training-data imbalance as prior information to augment SVMs to improve class-prediction accuracy. Using a simple example, we first show that SVMs can suffer from high incidences of false negatives when the training instances of the target class are heavily outnumbered by the training instances of a non-target class. The remedy we propose is to adjust the class boundary by modifying the kernel matrix, according to the imbalanced data distribution. Through theoretical analysis backed by empirical study, we show that our kernel-boundary-alignment algorithm works effectively on several datasets.
ICDM	SVM and Graphical Algorithms: A Cooperative Approach.	François Poulet	2004	"We present a cooperative approach using both Support Vector Machine (SVM) algorithms and visualization methods. SVM are widely used today and often give high quality results, but they are used as ""black-box"" (it is very difficult to explain the obtained results) and cannot treat easily very large datasets. We have developed graphical methods to help the user to evaluate and explain the SVM results. The first method is a graphical representation of the separating frontier quality, it is then linked with other visualization tools to help the user explaining SVM results. The information provided by these graphical methods is also used for SVM parameter tuning, they are then used together with automatic algorithms to deal with very large datasets on standard computers. We present an evaluation of our approach with the UCI and the Kent Ridge Bio-medical data sets."
ICDM	Correlation Preserving Discretization.	Sameep Mehta,Srinivasan Parthasarathy,Hui Yang	2004	Discretization is a crucial preprocessing primitive for a variety of data warehousing and mining tasks. In this article we present a novel PCA-based unsupervised algorithm for the discretization of continuous attributes in multivariate datasets. The algorithm leverages the underlying correlation structure in the dataset to obtain the discrete intervals, and ensures that the inherent correlations are preserved. The approach also extends easily to datasets containing missing values. We demonstrate the efficacy of the approach on real datasets and as a preprocessing step for both classification and frequent itemset mining tasks. We also show that the intervals are meaningful and can uncover hidden patterns in data.
ICDM	Active Feature-Value Acquisition for Classifier Induction.	Prem Melville,Maytal Saar-Tsechansky,Foster J. Provost,Raymond J. Mooney	2004	"Many induction problems include missing data that can be acquired at a cost. For building accurate predictive models, acquiring complete information for all instances is often expensive or unnecessary, while acquiring information for a random subset of instances may not be most effective. Active feature-value acquisition tries to reduce the cost of achieving a desired model accuracy by identifying instances for which obtaining complete information is most informative. We present an approach in which instances are selected for acquisition based on the current model's accuracy and its confidence in the prediction. Experimental results demonstrate that our approach can induce accurate models using substantially fewer feature-value acquisitions as compared to alternative policies."
ICDM	RDF: A Density-Based Outlier Detection Method using Vertical Data Representation.	Dongmei Ren,Baoying Wang,William Perrizo	2004	Outlier detection can lead to discovering unexpected and interesting knowledge, which is critical important to some areas such as monitoring of criminal activities in electronic commerce, credit card fraud, etc. In this paper, we developed an efficient density-based outlier detection method for large datasets. Our contributions are: a) We introduce a relative density factor (RDF); b) Based on RDF, we propose an RDF-based outlier detection method which can efficiently prune the data points which are deep in clusters, and detect outliers only within the remaining small subset of the data; c) The performance of our method is further improved by means of a vertical data representation, P-trees. We tested our method with NHL and NBA data. Our method shows an order of magnitude speed improvement compared to the contemporary approaches.
ICDM	Privacy-Sensitive Bayesian Network Parameter Learning.	Da Meng,Krishnamoorthy Sivakumar,Hillol Kargupta	2004	This paper considers the problem of learning the parameters of a Bayesian Network, assuming the structure of the network is given, from a privacy-sensitive dataset that is distributed between multiple parties. For a binary-valued dataset, we show that the count information required to estimate the conditional probabilities in a Bayesian network can be obtained as a solution to a set of linear equations involving some inner product between the relevantdifferent feature vectors. We consider a random projection-based method that was proposed elsewhere to securely compute the inner product (with a modified implementation of that method).
ICDM	Quantitative Association Rules Based on Half-Spaces: An Optimization Approach.	Ulrich Rückert,Lothar Richter,Stefan Kramer	2004	We tackle the problem of finding association rules for quantitative data. Whereas most of the previous approaches operate on hyperrectangles, we propose a representation based on half-spaces. Consequently, the left-hand side and right-hand side of an association rule does not contain a conjunction of items or intervals, but a weighted sum of variables tested against a threshold. Since the downward closure property does not hold for such rules, we propose an optimization setting for finding locally optimal rules. A simple gradient descent algorithm optimizes a parameterized score function, where iterations optimizing the first separating hyperplane alternate with iterations optimizing the second. Experiments with two real-world data sets show that the approach finds non-random patterns and scales up well. We therefore propose quantitative association rules based on half-spaces as an interesting new class of patterns with a high potential for applications.
ICDM	Evaluating Attraction in Spatial Point Patterns with an Application in the Field of Cultural History.	Marko Salmenkivi	2004	Spatial collocation rules are often useful for describing dependencies between spatial features. Still, the commonly used criteria for the interestingness of the rules and the selected neighbourhood constraints for spatial objects may be too rough for capturing the essentials of such dependencies. We demonstrate the difficulties with concrete examples on a large place-name data set. We propose a technique based on simple density estimation for assessing the interestingness with different neighbouring constraints.
ICDM	Dependency Networks for Relational Data.	Jennifer Neville,David Jensen	2004	Instance independence is a critical assumption of traditional machine learning methods contradicted by many relational datasets. For example, in scientific literature datasets there are dependencies among the references of a paper. Recent work on graphical models for relational data has demonstrated significant performance gains for models that exploit the dependencies among instances. In this paper, we present relational dependency networks (RDNs), a new form of graphical model capable of reasoning with such dependencies in a relational setting. We describe the details of RDN models and outline their strengths, most notably the ability to learn and reason with cyclic relational dependencies. We present RDN models learned on a number of real-world datasets, and evaluate the models in a classification context, showing significant performance improvements. In addition, we use synthetic data to evaluate the quality of model learning and inference procedures.
ICDM	Cluster Cores-Based Clustering for High Dimensional Data.	Yi-Dong Shen,Zhiyong Shen,Shi-Ming Zhang,Qiang Yang	2004	We propose a new approach to clustering high dimensional data based on a novel notion of cluster cores, instead of on nearest neighbors. A cluster core is a fairly dense group with a maximal number of pairwise similar objects. It represents the core of a cluster, as all objects in a cluster are with a great degree attracted to it. As a result, building clusters from cluster cores achieves high accuracy. Other major characteristics of the approach include: (1) It uses a semantics-based similarity measure. (2) It does not incur the curse of dimensionality and is scalable linearly with the dimensionality of data. (3) It outperforms the well-known clustering algorithm, ROCK, with both lower time complexity and higher accuracy.
ICDM	Metric Incremental Clustering of Nominal Data.	Dan A. Simovici,Namita Singla,Michael Kuperberg	2004	We present an algorithm for clustering nominal data that is based on a metric on the set of partitions of a finite set of objects; this metric is defined starting from a lower valuation of the lattice of partitions. The proposed algorithm seeks to determine a clustering partition such that the total distance between this partition and the partitions determined by the attributes of the objects has a local minimum. The resulting clustering is quite stable relative to the ordering of the objects.
ICDM	Probabilistic Principal Surfaces for Yeast Gene Microarray Data Mining.	Antonino Staiano,Lara De Vinco,Angelo Ciaramella,Giancarlo Raiconi,Roberto Tagliaferri,Roberto Amato,Giuseppe Longo,Ciro Donalek,Gennaro Miele,Diego di Bernardo	2004	The recent technological advances are producing huge data sets in almost all fields of scientific research, from astronomy to genetics. Although each research field often requires ad-hoc, fine tuned, procedures to properly exploit all the available information inherently present in the data, there is an urgent need for a new generation of general computational theories and tools capable to boost most human activities of data analysis. Here we propose Probabilistic Principal Surfaces (PPS) as an effective high-D data visualization and clustering tool for data mining applications, emphasizing its flexibility and generality of use in data-rich field. In order to better illustrate the potentialities of the method, we also provide a real world case-study by discussing the use of PPS for the analysis of yeast gene expression levels from microarray chips.
ICDM	n Ranking Refinements in the Step-by-Step Searching through a Product Catalogue.	Nenad Stojanovic	2004	n Ranking Refinements in the Step-by-Step Searching through a Product Catalogue.
ICDM	On Local Spatial Outliers.	Pei Sun,Sanjay Chawla	2004	"We propose a measure, Spatial Local Outlier Measure (SLOM) which captures the local behaviour of datum in their spatial neighborhood. With the help of SLOM we are able to discern local spatial outliers which are usually missed by global techniques like ""three standard deviations away from the mean"". Furthermore the measure takes into account the local stability around a data point and supresses the reporting of outliers in highly unstable areas, where data is too heterogeneous and the notion of outliers is not meaningful. We prove several properties of SLOM and report experiments on synthetic and real data sets which show that our approach is novel and scalable to large data sets."
ICDM	Supervised Latent Semantic Indexing for Document Categorization.	Jian-Tao Sun,Zheng Chen,Hua-Jun Zeng,Yuchang Lu,Chun-Yi Shi,Wei-Ying Ma	2004	Latent Semantic Indexing (LSI) is a successful technology in information retrieval (IR) which attempts to explore the latent semantics implied by a query or a document through representing them in a dimension-reduced space. However, LSI is not optimal for document categorization tasks because it aims to find the most representative features for document representation rather than the most discriminative ones. In this paper, we propose Supervised LSI (SLSI) which selects the most discriminative basis vectors using the training data iteratively. The extracted vectors are then used to project the documents into a reduced dimensional space for better classification. Experimental evaluations show that the SLSI approach leads to dramatic dimension reduction while achieving good classification results.
ICDM	DRYADE: A New Approach for Discovering Closed Frequent Trees in Heterogeneous Tree Databases.	Alexandre Termier,Marie-Christine Rousset,Michèle Sebag	2004	In this paper we present a novel algorithm for discovering tree patterns in a tree database. This algorithm uses a relaxed tree inclusion definition, making the problem more complex (checking tree inclusion is NP-complete), but allowing to mine highly heterogeneous databases. To obtain good performances, our DRYADE algorithm discovers only closed frequent tree patterns.
ICDM	MMAC: A New Multi-Class, Multi-Label Associative Classification Approach.	Fadi A. Thabtah,Peter I. Cowling,Yonghong Peng	2004	Building fast and accurate classifiers for large-scale databases is an important task in data mining. There is growing evidence that integrating classification and association rule mining together can produce more efficient and accurate classifiers than traditional classification techniques. In this paper, the problem of producing rules with multiple labels is investigated. We propose a new associative classification approach called multi-class, multi-label associative classification (MMAC). This paper also presents three measures for evaluating the accuracy of data mining classification approaches to a wide range of traditional and multi-label classification problems. Results for 28 different datasets show that the MMAC approach is an accurate and effective classification technique, highly competitive and scalable in comparison with other classification approaches.
ICDM	Analysis of Consensus Partition in Cluster Ensemble.	Alexander P. Topchy,Martin H. C. Law,Anil K. Jain,Ana L. N. Fred	2004	"In combination of multiple partitions, one is usually interested in deriving a consensus solution with a quality better than that of given partitions. Several recent studies have empirically demonstrated improved accuracy of clustering ensembles on a number of artificial and real-world data sets. Unlike certain multiple supervised classifier systems, convergence properties of unsupervised clustering ensembles remain unknown for conventional combination schemes. In this paper we present formal arguments on the effectiveness of cluster ensemble from two perspectives. The first is based on a stochastic partition generation model related to re-labeling and consensus function with plurality voting. The second is to study the property of the ""mean"" partition of an ensemble with respect to a metric on the space of all possible partitions. In both the cases, the consensus solution can be shown to converge to a true underlying clustering solution as the number of partitions in the ensemble increases. This paper provides a rigorous justification for the use of cluster ensemble."
ICDM	A Greedy Algorithm for Selecting Models in Ensembles.	Andrei L. Turinsky,Robert L. Grossman	2004	We are interested in ensembles of models built over k data sets. Common approaches are either to combine models by vote averaging, or to build a meta-model on the outputs of the local models. In this paper, we consider the model assignment approach, in which a meta-model selects one of the local statistical models for scoring. We introduce an algorithm called Greedy Data Labeling (GDL) that improves the initial data partition by reallocating some data, so that when each model is built on its local data subset, the resulting hierarchical system has minimal error. We present evidence that model assignment may in certain situations be more natural than traditional ensemble learning, and if enhanced by GDL, it often outperforms traditional ensembles.
ICDM	Privacy-Preserving Outlier Detection.	Jaideep Vaidya,Chris Clifton	2004	Outlier detection can lead to the discovery of truly unexpected knowledge in many areas such as electronic commerce, credit card fraud and especially national security. We look at the problem of finding outliers in large distributed databases where privacy/security concerns restrict the sharing of data. Both homogeneous and heterogeneous distribution of data is considered. We propose techniques to detect outliers in such scenarios while giving formal guarantees on the amount of information disclosed.
ICDM	Mining Web Data to Create Online Navigation Recommendations.	Juan D. Velásquez,Alejandro Bassi,Hiroshi Yasuda,Terumasa Aoki	2004	A system to provide online navigation recommendation for web visitors is introduced. We call visitor the anonymous user, i.e., when only data about her/his browsing behavior (web logs) are available. We first apply clustering techniques over a large sample of web data. Next, from thesignificant patterns that are discovered, a set of rules about how to use them is created. Finally, comparing the current web visitor session with the patterns, online navigation recommendations are proposed using the mentioned rules. The system was tested using data from a real web site, showing its effectiveness.
ICDM	Alpha Galois Lattices.	Véronique Ventos,Henry Soldano,Thibaut Lamadon	2004	"In many applications there is a need to represent a large number of data by clustering them in a hierarchy of classes. Our basic representation is a Galois lattice, a structure that exhaustively represents the whole set of concepts that are distinguishable given the instance set and the representation language. What we propose here is a method to reduce the size of the lattice, and thus simplify our view of the data, while conserving its formal structure and exhaustivity. For that purpose we use a preliminary partition of the instance set, representing the association of a ""type"" to each instance. By redefining the notion of extent of a term in order to cope, to a certain degree (denoted as ¿), with this partition, we define a particular family of Galois lattices denoted as Alpha Galois lattices. We also discuss the related implication rules defined as inclusion of such ¿-extents."
ICDM	SUMMARY: Efficiently Summarizing Transactions for Clustering.	Jianyong Wang,George Karypis	2004	Frequent itemset mining was initially proposed and has been studied extensively in the context of association rule mining. In recent years, several studies have also extended its applicationto the transaction (or document) classification and clustering. However, most of the frequent-itemset based clustering algorithms need to first mine a large intermediate set of frequent itemsets in order to identify a subset of the most promising ones that can be used for clustering. In this paper, we study how to directly find a subset of high quality frequent itemsets that can be used as a concise summary of the transaction database and to clusterthe categorical data. By exploring some properties of the subset of itemsets that we are interested in, we proposed several search space pruning methods and designed an efficient algorithm called SUMMARY. Our empirical results have shown that SUMMARY runs very fast even when the minimum support is extremely low and scales very well with respect to the database size, and surprisingly, as a pure frequent itemset mining algorithm it is very effectivein clustering the categorical data and smmarizing the dense transaction databases.
ICDM	Bottom-Up Generalization: A Data Mining Solution to Privacy Protection.	Ke Wang,Philip S. Yu,Sourav Chakraborty	2004	The well-known privacy-preserved data mining modifies existing data mining techniques to randomized data. In this paper, we investigate data mining as a technique for masking data, therefore, termed data mining based privacy protection. This approach incorporates partially the requirement of a targeted data mining task into the process of masking data so that essential structure is preserved in the masked data. The idea is simple but novel: we explore the data generalization concept from data mining as a way to hide detailed information, rather than discover trends and patterns. Once the data is masked, standard data mining techniques can be applied without modification. Our work demonstrated another positive use of data mining technology: not only can it discover useful patterns, but also mask private information. We consider the following privacy problem: a data holder wants to release a version of datafor building classification models, but wants to protect against linking the released data to an external source for inferring sensitive information. We adapt an iterative bottom-up generalization from data mining to generalize the data. The generalized data remains useful to classification but becomes difficult to link to other sources. The generalization space is specified by a hierarchical structure of generalizations. A key is identifying the best generalization to climb up the hierarchy at each iteration. Enumerating all candidate generalizations is impractical. We present a scalable solution that examines at most one generalization in each iteration for each attribute involved in the linking.
ICDM	A Probabilistic Approach for Adapting Information Extraction Wrappers and Discovering New Attributes.	Tak-Lam Wong,Wai Lam	2004	We develop a probabilistic framework for adapting information extraction wrappers with new attribute discovery. Wrapper adaptation aims at automatically adapting a previously learned wrapper from the source Web site to a new unseen site for information extraction. One unique characteristic of our framework is that it can discover new or previously unseen attributes as well as headers from the new site. It is based on a generative model for the generation of text fragments related to attribute items and formatting data in a Web page. To solve the wrapper adaptation problem, we consider two kinds of information from the source Web site. The first kind of information is the extraction knowledge contained in the previously learned wrapper from the source Web site. The second kind of information is the previously extracted or collected items. We employ a Bayesian learning approach to automatically select a set of training examples for adapting a wrapper for the new unseen site. To solve the new attribute discovery problem, we develop a model which analyzes the surrounding text fragments of the attributes in the new unseen site. A Bayesian learning method is developed to discover the new attributes and their headers. EM technique is employed in both Bayesian learning models. We conducted extensive experiments from a number of real-world Web sites to demonstrate the effectiveness of our framework.
ICDM	IRC: An Iterative Reinforcement Categorization Algorithm for Interrelated Web Objects.	Gui-Rong Xue,Dou Shen,Qiang Yang,Hua-Jun Zeng,Zheng Chen,Yong Yu,Wensi Xi,Wei-Ying Ma	2004	Most existing categorization algorithms deal with homogeneous Web data objects, and consider interrelated objects as additional features when taking the interrelationships withother types of objects into account. However, focusing on any single aspects of these interrelationships and objects will not fully reveal their true categories. In this paper, wepropose a novel categorization algorithm, the Iterative Reinforcement Categorization algorithm (IRC), to exploit the full interrelationships between the heterogeneous objects on the Web.IRC attempts to classify the interrelated Web objects by iterative reinforcement between individual classification results of different types via the interrelationships. Experiments on a clickthrough log dataset from MSN search engine show that, with the F1 measures, IRC achieves a 26.4% improvement over a pure content-based classification method, a 21% improvement over a query metadata-based method, and a 16.4% improvement over a virtual document-based method. Furthermore, our experiments show that IRC converges rapidly.
ICDM	A Polygonal Line Algorithm based Nonlinear Feature Extraction Method.	Feng Zhang	2004	We propose a polygonal line based principal curve algorithm for nonlinear feature extraction, in which the nonlinearities among the multivariable data can be described by a set of local linear models. The proposed algorithm integrates the linear PCA approach with the polygonal line algorithm to represent complicated nonlinear data structure. Statistical redundancy elimination for high dimensional data is also discussed for describing the underlying principal curves without much loss of information among the original data sets. The polygonal line algorithm can produce robust and accurate nonlinear curve estimation for different multivariate data types, and it is helpful in reducing the computation complexity for existing principal curve approaches when the sample size is large.
ICDM	Learning Rules from Highly Unbalanced Data Sets.	Jianping Zhang,Eric Bloedorn,Lowell Rosen,Daniel Venese	2004	This paper presents a simple and effective rule learning algorithm for highly unbalanced data sets. By using the small size of the minority class to its advantage this algorithm can conduct an almost exhaustive search for patterns within the known fraudulent cases. This algorithm was designed for and successfully applied to a law enforcement problem, which involves discovering common patterns of fraudulent transactions.
ICDM	AGILE: A General Approach to Detect Transitions in Evolving Data Streams.	Jiong Yang,Wei Wang	2004	In many applications such as e-commerce, system diagnosis and telecommunication services, data arrives in streams at a high speed. It is common that the underlying process generating the stream may change over time, either as a result of the fundamental evolution or in response to some external stimulus. Detecting these changes is a very challenging problem of great practical importance. The overall volume of the stream usually far exceeds the available main memory and access to the data stream is typically performed via a linear scan in ascending order of the indices of the records. In this paper, we propose a novel approach, AGILE, to monitor streaming data and to detect distinguishable transitions of the underlying processes. AGILE has many advantages over the traditional Hidden Markov Model, e.g., AGILE only requires one scan of the data.
ICDM	AVT-NBL: An Algorithm for Learning Compact and Accurate Naïve Bayes Classifiers from Attribute Value Taxonomies and Data.	Jun Zhang,Vasant Honavar	2004	AVT-NBL: An Algorithm for Learning Compact and Accurate Naïve Bayes Classifiers from Attribute Value Taxonomies and Data.
ICDM	Learning Weighted Naive Bayes with Accurate Ranking.	Harry Zhang,Shengli Sheng	2004	Naive Bayes is one of most effective classification algorithms. In many applications, however, a ranking of examples are more desirable than just classification. How to extend naive Bayes to improve its ranking performance is an interesting and useful question in practice. Weighted naive Bayes is an extension of naive Bayes, in which attributes have different weights. This paper investigates how to learn a weighted naive Bayes with accurate ranking from data, or more precisely, how to learn the weights of a weighted naive Bayes to produce accurate ranking. We explore various methods: the gain ratio method, the hill climbing method, and the Markov Chain Monte Carlo method, the hill climbing method combined with the gain ratio method, and the Markov Chain Monte Carlo method combined with the gain ratio method. Our experiments show that a weighted naive Bayes trained to produce accurate ranking outperforms naive Bayes.
ICDM	Relational Peculiarity Oriented Data Mining.	Ning Zhong,Chunnian Liu,Yiyu Yao,Muneaki Ohshima,Mingxin Huang,Jiajin Huang	2004	Peculiarity rules are a new type of interesting rules which can be discovered by searching the relevance among peculiar data. A main task of mining peculiarity rules is the identification of peculiarity. Traditional methods of finding peculiar data are attribute-based approaches. This paper extends peculiarity oriented mining to relational peculiarity oriented mining. Peculiar data are identified on record level, and peculiar rules are mined and explained in a relational mining framework. The results from preliminary experiments show that relational peculiarity oriented mining is very effective.
ICDM	Scalable Construction of Topic Directory with Nonparametric Closed Termset Mining.	Hwanjo Yu,Duane Searsmith,Xiaolei Li,Jiawei Han	2004	A topic directory, e.g., Yahoo directory, provides a view of a document set at different levelsof abstraction and is ideal for the interactive exploration and visualization of the document set. We present a method that dynamically generates a topic directory from a document set usinga frequent closed termset mining algorithm. Our method shows experimental results of equal quality to recent document clustering methods and has additional benefits such as automatic generation of topic labels and determination of a clustering parameter.
ICDM	Cost-Guided Class Noise Handling for Effective Cost-Sensitive Learning.	Xingquan Zhu,Xindong Wu	2004	Recent research in machine learning, data mining and related areas has produced a wide variety of algorithms for cost-sensitive (CS) classification, where instead of maximizing the classification accuracy, minimizing the misclassification cost becomes the objective. However, these methods assume that training sets do not contain significant noise, which is rarely the case in real-world environments. In this paper, we systematically study the impacts of class noise on CS learning, and propose a cost-guided class noise handling algorithm to identify noise for effective CS learning. We call it Cost-guided Iterative Classification Filter (CICF), because it seamlessly integrates costs and an existing Classification Filter for noise identification. Instead of putting equal weights to handle noise in all classes in existing efforts, CICF puts more emphasis on expensive classes, which makes it especially successful in dealing with datasets with a large cost-ratio. Experimental results and comparative studies from real-world datasets indicate that the existence of noise may seriously corrupt the performance of CS classifiers, and by adopting the proposed CICF algorithm, we can significantly reduce the misclassification cost of a CS classifier in noisy environments.
ICDM	Dynamic Classifier Selection for Effective Mining from Noisy Data Streams.	Xingquan Zhu,Xindong Wu,Ying Yang	2004	"Recently, mining from data streams has become an important and challenging task for many real-world applications such as credit card fraud protection and sensor networking. One popular solution is to separate stream data into chunks, learn a base classifier from each chunk, and then integrate all base classifiers for effective classification. In this paper, we propose a new dynamic classifier selection (DCS) mechanism to integrate base classifiers for effective mining from data streams. The proposed algorithm dynamically selects a single ""best"" classifier to classify each test instance at run time. Our scheme uses statistical information from attribute values, and uses each attribute to partition the evaluation set into disjoint subsets, followed by a procedure that evaluates the classification accuracy of each base classifier on these subsets. Given a test instance, its attribute values determine the subsets that the similar instances in the evaluation set have constructed, and the classifier with the highest classification accuracy on those subsets is selected to classify the test instance. Experimental results and comparative studies demonstrate the efficiency and efficacy of our method. Such a DCS scheme appears to be promising in mining data streams with dramatic concept drifting or with a significant amount of noise, where the base classifiers are likely conflictive or have low confidence."
ICDM	Proceedings of the 4th IEEE International Conference on Data Mining (ICDM 2004), 1-4 November 2004, Brighton, UK		2004	Proceedings of the 4th IEEE International Conference on Data Mining (ICDM 2004), 1-4 November 2004, Brighton, UK
KDD	Fast nonlinear regression via eigenimages applied to galactic morphology.	Brigham Anderson,Andrew W. Moore,Andrew Connolly,Robert Nichol	2004	Astronomy increasingly faces the issue of massive, unwieldly data sets. The Sloan Digital Sky Survey (SDSS) [11] has so far generated tens of millions of images of distant galaxies, of which only a tiny fraction have been morphologically classified. Morphological classification in this context is achieved by fitting a parametric model of galaxy shape to a galaxy image. This is a nonlinear regression problem, whose challenges are threefold, 1) blurring of the image caused by atmosphere and mirror imperfections, 2) large numbers of local minima, and 3) massive data sets.Our strategy is to use the eigenimages of the parametric model to form a new feature space, and then to map both target image and the model parameters into this feature space. In this low-dimensional space we search for the best image-to-parameter match. To search the space, we sample it by creating a database of many random parameter vectors (prototypes) and mapping them into the feature space. The search problem then becomes one of finding the best prototype match, so the fitting process a nearest-neighbor search.In addition to the savings realized by decomposing the original space into an eigenspace, we can use the fact that the model is a linear sum of functions to reduce the prototypes further: the only prototypes stored are the components of the model function. A modified form of nearest neighbor is used to search among them.Additional complications arise in the form of missing data and heteroscedasticity, both of which are addressed with weighted linear regression. Compared to existing techniques, speed-ups ach-ieved are between 2 and 3 orders of magnitude. This should enable the analysis of the entire SDSS dataset.
KDD	Clustering time series from ARMA models with clipped data.	Anthony J. Bagnall,Gareth J. Janacek	2004	Clustering time series is a problem that has applications in a wide variety of fields, and has recently attracted a large amount of research. In this paper we focus on clustering data derived from Autoregressive Moving Average (ARMA) models using k-means and k-medoids algorithms with the Euclidean distance between estimated model parameters. We justify our choice of clustering technique and distance metric by reproducing results obtained in related research. Our research aim is to assess the affects of discretising data into binary sequences of above and below the median, a process known as clipping, on the clustering of time series. It is known that the fitted AR parameters of clipped data tend asymptotically to the parameters for unclipped data. We exploit this result to demonstrate that for long series the clustering accuracy when using clipped data from the class of ARMA models is not significantly different to that achieved with unclipped data. Next we show that if the data contains outliers then using clipped data produces significantly better clusterings. We then demonstrate that using clipped series requires much less memory and operations such as distance calculations can be much faster. Finally, we demonstrate these advantages on three real world data sets.
KDD	ANN quality diagnostic models for packaging manufacturing: an industrial data mining case study.	Nicolás de Abajo,Alberto B. Diez,Vanesa Lobato,Sergio R. Cuesta	2004	"World steel trade becomes more competitive every day and new high international quality standards and productivity levels can only be achieved by applying the latest computational technologies. Data driven analysis of complex processes is necessary in many industrial applications where analytical modeling is not possible. This paper presents the deployment of KDD technology in one real industrial problem: the development of new tinplate quality diagnostic models.The electrodeposition of tin on steel strips is the most critical stage of a complex process that involves a great amount of variables and operating conditions. Its optimization is not only a great commercial and economic challenge but also a compulsion due to the social impact of the tinplate product-more than 90% of the production is used for food packaging. The necessary certification with standards, like ISO 9000, requires the use of diagnostic models to minimize the costs and the environmental impact. This aim has been achieved following the multi-stage DM methodology CRISP-DM and a novel application of pro-active maintenance methods, as FMEA, for the identification of the specific process anomalies. Three DM tools have been used for the development of the models. The final results include two ANN tinplate quality diagnostic models, that provide the estimated quality of the final product just seconds after its production and only based on the process data. The results have much better performance than the classical Faraday's models widely used for the estimation."
KDD	Cross channel optimized marketing by reinforcement learning.	Naoki Abe,Naval K. Verma,Chidanand Apté,Robert Schroko	2004	The issues of cross channel integration and customer life time value modeling are two of the most important topics surrounding customer relationship management (CRM) today. In the present paper, we describe and evaluate a novel solution that treats these two important issues in a unified framework of Markov Decision Processes (MDP). In particular, we report on the results of a joint project between IBM Research and Saks Fifth Avenue to investigate the applicability of this technology to real world problems. The business problem we use as a testbed for our evaluation is that of optimizing direct mail campaign mailings for maximization of profits in the store channel. We identify a problem common to cross-channel CRM, which we call the Cross-Channel Challenge, due to the lack of explicit linking between the marketing actions taken in one channel and the customer responses obtained in another. We provide a solution for this problem based on old and new techniques in reinforcement learning. Our in-laboratory experimental evaluation using actual customer interaction data show that as much as 7 to 8 per cent increase in the store profits can be expected, by employing a mailing policy automatically generated by our methodology. These results confirm that our approach is valid in dealing with the cross channel CRM scenarios in the real world.
KDD	A generalized maximum entropy approach to bregman co-clustering and matrix approximation.	Arindam Banerjee,Inderjit S. Dhillon,Joydeep Ghosh,Srujana Merugu,Dharmendra S. Modha	2004	Co-clustering, or simultaneous clustering of rows and columns of a two-dimensional data matrix, is rapidly becoming a powerful data analysis technique. Co-clustering has enjoyed wide success in varied application domains such as text clustering, gene-microarray analysis, natural language processing and image, speech and video analysis. In this paper, we introduce a partitional co-clustering formulation that is driven by the search for a good matrix approximation---every co-clustering is associated with an approximation of the original data matrix and the quality of co-clustering is determined by the approximation error. We allow the approximation error to be measured using a large class of loss functions called Bregman divergences that include squared Euclidean distance and KL-divergence as special cases. In addition, we permit multiple structurally different co-clustering schemes that preserve various linear statistics of the original data matrix. To accomplish the above tasks, we introduce a new minimum Bregman information (MBI) principle that simultaneously generalizes the maximum entropy and standard least squares principles, and leads to a matrix approximation that is optimal among all generalized additive models in a certain natural parameter space. Analysis based on this principle yields an elegant meta algorithm, special cases of which include most previously known alternate minimization based clustering algorithms such as kmeans and co-clustering algorithms such as information theoretic (Dhillon et al., 2003b) and minimum sum-squared residue co-clustering (Cho et al., 2004). To demonstrate the generality and flexibility of our co-clustering framework, we provide examples and empirical evidence on a variety of problem domains and also describe novel co-clustering applications such as missing value prediction and compression of categorical data matrices.
KDD	An iterative method for multi-class cost-sensitive learning.	Naoki Abe,Bianca Zadrozny,John Langford	2004	Cost-sensitive learning addresses the issue of classification in the presence of varying costs associated with different types of misclassification. In this paper, we present a method for solving multi-class cost-sensitive learning problems using any binary classification algorithm. This algorithm is derived using hree key ideas: 1) iterative weighting; 2) expanding data space; and 3) gradient boosting with stochastic ensembles. We establish some theoretical guarantees concerning the performance of this method. In particular, we show that a certain variant possesses the boosting property, given a form of weak learning assumption on the component binary classifier. We also empirically evaluate the performance of the proposed method using benchmark data sets and verify that our method generally achieves better results than representative methods for cost-sensitive learning, in terms of predictive performance (cost minimization) and, in many cases, computational efficiency.
KDD	An objective evaluation criterion for clustering.	Arindam Banerjee,John Langford	2004	We propose and test an objective criterion for evaluation of clustering performance: How well does a clustering algorithm run on unlabeled data aid a classification algorithm? The accuracy is quantified using the PAC-MDL bound [3] in a semisupervised setting. Clustering algorithms which naturally separate the data according to (hidden) labels with a small number of clusters perform well. A simple extension of the argument leads to an objective model selection method. Experimental results on text analysis datasets demonstrate that this approach empirically results in very competitive bounds on test set performance on natural datasets.
KDD	Approximating a collection of frequent sets.	Foto N. Afrati,Aristides Gionis,Heikki Mannila	2004	One of the most well-studied problems in data mining is computing the collection of frequent item sets in large transactional databases. One obstacle for the applicability of frequent-set mining is that the size of the output collection can be far too large to be carefully examined and understood by the users. Even restricting the output to the border of the frequent item-set collection does not help much in alleviating the problem.In this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem: What are the k sets that best approximate a collection of frequent item sets? Our measure of approximating a collection of sets by k sets is defined to be the size of the collection covered by the the k sets, i.e., the part of the collection that is included in one of the k sets. We also specify a bound on the number of extra sets that are allowed to be covered. We examine different problem variants for which we demonstrate the hardness of the corresponding problems and we provide simple polynomial-time approximation algorithms. We give empirical evidence showing that the approximation methods work well in practice.
KDD	A probabilistic framework for semi-supervised clustering.	Sugato Basu,Mikhail Bilenko,Raymond J. Mooney	2004	Unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints, i.e., pairs of instances labeled as belonging to same or different clusters. In recent years, a number of algorithms have been proposed for enhancing clustering quality by employing such supervision. Such methods use the constraints to either modify the objective function, or to learn the distance measure. We propose a probabilistic model for semi-supervised clustering based on Hidden Markov Random Fields (HMRFs) that provides a principled framework for incorporating supervision into prototype-based clustering. The model generalizes a previous approach that combines constraints and Euclidean distance learning, and allows the use of a broad range of clustering distortion measures, including Bregman divergences (e.g., Euclidean distance and I-divergence) and directional similarity measures (e.g., cosine similarity). We present an algorithm that performs partitional semi-supervised clustering of data by minimizing an objective function derived from the posterior energy of the HMRF model. Experimental results on several text data sets demonstrate the advantages of the proposed framework.
KDD	On demand classification of data streams.	Charu C. Aggarwal,Jiawei Han,Jianyong Wang,Philip S. Yu	2004	Current models of the classification problem do not effectively handle bursts of particular classes coming in at different times. In fact, the current model of the classification problem simply concentrates on methods for one-pass classification modeling of very large data sets. Our model for data stream classification views the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and testing streams are used for dynamic classification of data sets. This model reflects real life situations effectively, since it is desirable to classify test streams in real time over an evolving training and test stream. The aim here is to create a classification system in which the training model can adapt quickly to the changes of the underlying data stream. In order to achieve this goal, we propose an on-demand classification process which can dynamically select the appropriate window of past training data to build the classifier. The empirical results indicate that the system maintains a high classification accuracy in an evolving data stream, while providing an efficient solution to the classification task.
KDD	Mining reference tables for automatic text segmentation.	Eugene Agichtein,Venkatesh Ganti	2004	Automatically segmenting unstructured text strings into structured records is necessary for importing the information contained in legacy sources and text collections into a data warehouse for subsequent querying, analysis, mining and integration. In this paper, we mine tables present in data warehouses and relational databases to develop an automatic segmentation system. Thus, we overcome limitations of existing supervised text segmentation approaches, which require comprehensive manually labeled training data. Our segmentation system is robust, accurate, and efficient, and requires no additional manual effort. Thorough evaluation on real datasets demonstrates the robustness and accuracy of our system, with segmentation accuracy exceeding state of the art supervised approaches.
KDD	Recovering latent time-series from their observed sums: network tomography with particle filters.	Edoardo Airoldi,Christos Faloutsos	2004	"Hidden variables, evolving over time, appear in multiple settings, where it is valuable to recover them, typically from observed sums. Our driving application is 'network tomography', where we need to estimate the origin-destination (OD) traffic flows to determine, e.g., who is communicating with whom in a local area network. This information allows network engineers and managers to solve problems in design, routing, configuration debugging, monitoring and pricing. Unfortunately the direct measurement of the OD traffic is usually difficult, or even impossible; instead, we can easily measure the loads on every link, that is, sums of desirable OD flows.In this paper we propose i-FILTER, a method to solve this problem, which improves the state-of-the-art by (a) introducing explicit time dependence, and by (b) using realistic, non-Gaussian marginals in the statistical models for the traffic flows, as never attempted before. We give experiments on real data, where i-FILTER scales linearly with new observations and out-performs the best existing solutions, in a wide variety of settings. Specifically, on real network traffic measured at CMU, and at AT&T, i-FILTER reduced the estimation errors between 15% and 46% in all cases."
KDD	Interactive training of advanced classifiers for mining remote sensing image archives.	Selim Aksoy,Krzysztof Koperski,Carsten Tusk,Giovanni B. Marchisio	2004	Advances in satellite technology and availability of downloaded images constantly increase the sizes of remote sensing image archives. Automatic content extraction, classification and content-based retrieval have become highly desired goals for the development of intelligent remote sensing databases. The common approach for mining these databases uses rules created by analysts. However, incorporating GIS information and human expert knowledge with digital image processing improves remote sensing image analysis. We developed a system that uses decision tree classifiers for interactive learning of land cover models and mining of image archives. Decision trees provide a promising solution for this problem because they can operate on both numerical (continuous) and categorical (discrete) data sources, and they do not require any assumptions about neither the distributions nor the independence of attribute values. This is especially important for the fusion of measurements from different sources like spectral data, DEM data and other ancillary GIS data. Furthermore, using surrogate splits provides the capability of dealing with missing data during both training and classification, and enables handling instrument malfunctions or the cases where one or more measurements do not exist for some locations. Quantitative and qualitative performance evaluation showed that decision trees provide powerful tools for modeling both pixel and region contents of images and mining of remote sensing image archives.
KDD	TiVo: making show recommendations using a distributed collaborative filtering architecture.	Kamal Ali,Wijnand van Stam	2004	"We describe the TiVo television show collaborative recommendation system which has been fielded in over one million TiVo clients for four years. Over this install base, TiVo currently has approximately 100 million ratings by users over approximately 30,000 distinct TV shows and movies. TiVo uses an item-item (show to show) form of collaborative filtering which obviates the need to keep any persistent memory of each user's viewing preferences at the TiVo server. Taking advantage of TiVo's client-server architecture has produced a novel collaborative filtering system in which the server does a minimum of work and most work is delegated to the numerous clients. Nevertheless, the server-side processing is also highly scalable and parallelizable. Although we have not performed formal empirical evaluations of its accuracy, internal studies have shown its recommendations to be useful even for multiple user households. TiVo's architecture also allows for throttling of the server so if more server-side resources become available, more correlations can be computed on the server allowing TiVo to make recommendations for niche audiences."
KDD	Column-generation boosting methods for mixture of kernels.	Jinbo Bi,Tong Zhang,Kristin P. Bennett	2004	We devise a boosting approach to classification and regression based on column generation using a mixture of kernels. Traditional kernel methods construct models based on a single positive semi-definite kernel with the type of kernel predefined and kernel parameters chosen according to cross-validation performance. Our approach creates models that are mixtures of a library of kernel models, and our algorithm automatically determines kernels to be used in the final model. The 1-norm and 2-norm regularization methods are employed to restrict the ensemble of kernel models. The proposed method produces sparser solutions, and thus significantly reduces the testing time. By extending the column generation (CG) optimization which existed for linear programs with 1-norm regularization to quadratic programs with 2-norm regularization, we are able to solve many learning formulations by leveraging various algorithms for constructing single kernel models. By giving different priorities to columns to be generated, we are able to scale CG boosting to large datasets. Experimental results on benchmark data are included to demonstrate its effectiveness.
KDD	Systematic data selection to mine concept-drifting data streams.	Wei Fan	2004	"One major problem of existing methods to mine data streams is that it makes ad hoc choices to combine most recent data with some amount of old data to search the new hypothesis. The assumption is that the additional old data always helps produce a more accurate hypothesis than using the most recent data only. We first criticize this notion and point out that using old data blindly is not better than ""gambling""; in other words, it helps increase the accuracy only if we are ""lucky."" We discuss and analyze the situations where old data will help and what kind of old data will help. The practical problem on choosing the right example from old data is due to the formidable cost to compare different possibilities and models. This problem will go away if we have an algorithm that is extremely efficient to compare all sensible choices with little extra cost. Based on this observation, we propose a simple, efficient and accurate cross-validation decision tree ensemble method."
KDD	Exploring the community structure of newsgroups.	Christian Borgs,Jennifer T. Chayes,Mohammad Mahdian,Amin Saberi	2004	We propose to use the community structure of Usenet for organizing and retrieving the information stored in newsgroups. In particular, we study the network formed by cross-posts, messages that are posted to two or more newsgroups simultaneously. We present what is, to our knowledge, by far the most detailed data that has been collected on Usenet cross-postings. We analyze this network to show that it is a small-world network with significant clustering. We also present a spectral algorithm which clusters newsgroups based on the cross-post matrix. The result of our clustering provides a topical classification of newsgroups. Our clustering gives many examples of significant relationships that would be missed by semantic clustering methods.
KDD	Exploiting dictionaries in named entity extraction: combining semi-Markov extraction processes and data integration methods.	William W. Cohen,Sunita Sarawagi	2004	We consider the problem of improving named entity recognition (NER) systems by using external dictionaries---more specifically, the problem of extending state-of-the-art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary. This is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name; however, the most useful similarity measures score entire candidate names. To correct this mismatch we formalize a semi-Markov extraction process, which is based on sequentially classifying segments of several adjacent words, rather than single words. In addition to allowing a natural way of coupling high-performance NER methods and high-performance similarity functions, this formalism also allows the direct use of other useful entity-level features, and provides a more natural formulation of the NER problem than sequential word classification. Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER.
KDD	Predicting customer shopping lists from point-of-sale purchase data.	Chad M. Cumby,Andrew E. Fano,Rayid Ghani,Marko Krema	2004	"This paper describes a prototype that predicts the shopping lists for customers in a retail store. The shopping list prediction is one aspect of a larger system we have developed for retailers to provide individual and personalized interactions with customers as they navigate through the retail store. Instead of using traditional personalization approaches, such as clustering or segmentation, we learn separate classifiers for each customer from historical transactional data. This allows us to make very fine-grained and accurate predictions about what items a particular individual customer will buy on a given shopping trip.We formally frame the shopping list prediction as a classification problem, describe the algorithms and methodology behind our system, its impact on the business case in which we frame it, and explore some of the properties of the data source that make it an interesting testbed for KDD algorithms. Our results show that we can predict a shopper's shopping list with high levels of accuracy, precision, and recall. We believe that this work impacts both the data mining and the retail business community. The formulation of shopping list prediction as a machine learning problem results in algorithms that should be useful beyond retail shopping list prediction. For retailers, the result is not only a practical system that increases revenues by up to 11%, but also enhances customer experience and loyalty by giving them the tools to individually interact with customers and anticipate their needs."
KDD	Adversarial classification.	Nilesh N. Dalvi,Pedro Domingos,Mausam,Sumit K. Sanghai,Deepak Verma	2004	"Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations."
KDD	Feature selection in scientific applications.	Erick Cantú-Paz,Shawn D. Newsam,Chandrika Kamath	2004	Numerous applications of data mining to scientific data involve the induction of a classification model. In many cases, the collection of data is not performed with this task in mind, and therefore, the data might contain irrelevant or redundant features that affect negatively the accuracy of the induction algorithms. The size and dimensionality of typical scientific data make it difficult to use any available domain information to identify features that discriminate between the classes of interest. Similarly, exploratory data analysis techniques have limitations on the amount and dimensionality of the data they can process effectively. In this paper, we describe applications of efficient feature selection methods to data sets from astronomy, plasma physics, and remote sensing. We use variations of recently proposed filter methods as well as traditional wrapper approaches, where practical. We discuss the general challenges of feature selection in scientific datasets, the strategies for success that were common among our diverse applications, and the lessons learned in solving these problems.
KDD	Belief state approaches to signaling alarms in surveillance systems.	Kaustav Das,Andrew W. Moore,Jeff G. Schneider	2004	Surveillance systems have long been used to monitor industrial processes and are becoming increasingly popular in public health and anti-terrorism applications. Most early detection systems produce a time series of p-values or some other statistic as their output. Typically, the decision to signal an alarm is based on a threshold or other simple algorithm such as CUSUM that accumulates detection information temporally.We formulate a POMDP model of underlying events and observations from a detector. We solve the model and show how it is used for single-output detectors. When dealing with spatio-temporal data, scan statistics are a popular method of building detectors. We describe the use of scan statistics in surveillance and how our POMDP model can be used to perform alarm signaling with them. We compare the results obtained by our method with simple thresholding and CUSUM on synthetic and semi-synthetic health data.
KDD	Data mining in metric space: an empirical analysis of supervised learning performance criteria.	Rich Caruana,Alexandru Niculescu-Mizil	2004	Many criteria can be used to evaluate the performance of supervised learning. Different criteria are appropriate in different settings, and it is not always clear which criteria to use. A further complication is that learning methods that perform well on one criterion may not perform well on other criteria. For example, SVMs and boosting are designed to optimize accuracy, whereas neural nets typically optimize squared error or cross entropy. We conducted an empirical study using a variety of learning methods (SVMs, neural nets, k-nearest neighbor, bagged and boosted trees, and boosted stumps) to compare nine boolean classification performance metrics: Accuracy, Lift, F-Score, Area under the ROC Curve, Average Precision, Precision/Recall Break-Even Point, Squared Error, Cross Entropy, and Probability Calibration. Multidimensional scaling (MDS) shows that these metrics span a low dimensional manifold. The three metrics that are appropriate when predictions are interpreted as probabilities: squared error, cross entropy, and calibration, lay in one part of metric space far away from metrics that depend on the relative order of the predicted values: ROC area, average precision, break-even point, and lift. In between them fall two metrics that depend on comparing predictions to a threshold: accuracy and F-score. As expected, maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy, but perform poorly on probability metrics such as squared error. What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision. We introduce a new metric, SAR, that combines squared error, accuracy, and ROC area into one metric. MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics, suggesting that it is a good general purpose metric to use when more specific criteria are not known.
KDD	A general approach to incorporate data quality matrices into data mining algorithms.	Ian Davidson,Ashish Grover,Ashwin Satyanarayana,Giri Kumar Tayi	2004	"Data quality is a central issue for many information-oriented organizations. Recent advances in the data quality field reflect the view that a database is the product of a manufacturing process. While routine errors, such as non-existent zip codes, can be detected and corrected using traditional data cleansing tools, many errors systemic to the manufacturing process cannot be addressed. Therefore, the product of the data manufacturing process is an imprecise recording of information about the entities of interest (i.e. customers, transactions or assets). In this way, the database is only one (flawed) version of the entities it is supposed to represent. Quality assurance systems such as Motorola's Six-Sigma and other continuous improvement methods document the data manufacturing process's shortcomings. A widespread method of documentation is quality matrices. In this paper, we explore the use of the readily available data quality matrices for the data mining classification task. We first illustrate that if we do not factor in these quality matrices, then our results for prediction are sub-optimal. We then suggest a general-purpose ensemble approach that perturbs the data according to these quality matrices to improve the predictive accuracy and show the improvement is due to a reduction in variance."
KDD	Locating secret messages in images.	Ian Davidson,Goutam Paul	2004	Steganography involves hiding messages in innocuous media such as images, while steganalysis is the field of detecting these secret messages. The ultimate goal of steganalysis is two-fold: making a binary classification of a file as stego-bearing or innocent, and secondly, locating the hidden message with an aim to extracting, sterilizing or manipulating it. Almost all steganalysis approaches (known as attacks) focus on the first of these two issues. In this paper, we explore the difficult related problem: given that we know an image file contains steganography, locate which pixels contain the message. We treat the hidden message location problem as outlier detection using probability/energy measures of images motivated by the image restoration community. Pixels contributing the most to the energy calculations of an image are deemed outliers. Typically, of the top third of one percent of most energized pixels (outliers), we find that 87% are stego-bearing in color images and 61% in grayscale images. In all image types only 1% of all pixels are stego-bearing indicating our techniques provides a substantial lift over random guessing.
KDD	Mining and summarizing customer reviews.	Minqing Hu,Bing Liu	2004	Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.
KDD	Fully automatic cross-associations.	Deepayan Chakrabarti,Spiros Papadimitriou,Dharmendra S. Modha,Christos Faloutsos	2004	"Large, sparse binary matrices arise in numerous data mining applications, such as the analysis of market baskets, web graphs, social networks, co-citations, as well as information retrieval, collaborative filtering, sparse matrix reordering, etc. Virtually all popular methods for the analysis of such matrices---e.g., k-means clustering, METIS graph partitioning, SVD/PCA and frequent itemset mining---require the user to specify various parameters, such as the number of clusters, number of principal components, number of partitions, and ""support."" Choosing suitable values for such parameters is a challenging problem.Cross-association is a joint decomposition of a binary matrix into disjoint row and column groups such that the rectangular intersections of groups are homogeneous. Starting from first principles, we furnish a clear, information-theoretic criterion to choose a good cross-association as well as its parameters, namely, the number of row and column groups. We provide scalable algorithms to approach the optimal. Our algorithm is parameter-free, and requires no user intervention. In practice it scales linearly with the problem size, and is thus applicable to very large matrices. Finally, we present experiments on multiple synthetic and real-life datasets, where our method gives high-quality, intuitive results."
KDD	A rank sum test method for informative gene discovery.	Lin Deng,Jian Pei,Jinwen Ma,Dik Lun Lee	2004	Finding informative genes from microarray data is an important research problem in bioinformatics research and applications. Most of the existing methods rank features according to their discriminative capability and then find a subset of discriminative genes (usually top k genes). In particular, t-statistic criterion and its variants have been adopted extensively. This kind of methods rely on the statistics principle of t-test, which requires that the data follows a normal distribution. However, according to our investigation, the normality condition often cannot be met in real data sets.To avoid the assumption of the normality condition, in this paper, we propose a rank sum test method for informative gene discovery. The method uses a rank-sum statistic as the ranking criterion. Moreover, we propose using the significance level threshold, instead of the number of informative genes, as the parameter. The significance level threshold as a parameter carries the quality specification in statistics. We follow the Pitman efficiency theory to show that the rank sum method is more accurate and more robust than the t-statistic method in theory.To verify the effectiveness of the rank sum method, we use support vector machine (SVM) to construct classifiers based on the identified informative genes on two well known data sets, namely colon data and leukemia data. The prediction accuracy reaches 96.2% on the colon data and 100% on the leukemia data. The results are clearly better than those from the previous feature ranking methods. By experiments, we also verify that using significance level threshold is more effective than directly specifying an arbitrary k.
KDD	Parallel computation of high dimensional robust correlation and covariance matrices.	James Chilson,Raymond T. Ng,Alan Wagner,Ruben H. Zamar	2004	The computation of covariance and correlation matrices are critical to many data mining applications and processes. Unfortunately the classical covariance and correlation matrices are very sensitive to outliers. Robust methods, such as QC and the Maronna method, have been proposed. However, existing algorithms for QC only give acceptable performance when the dimensionality of the matrix is in the hundreds; and the Maronna method is rarely used in practice because of its high computational cost.In this paper, we develop parallel algorithms for both QC and the Maronna method. We evaluate these parallel algorithms using a real data set of the gene expression of over 6,000 genes, giving rise to a matrix of over 18 million entries. In our experimental evaluation, we explore scalability in dimensionality and in the number of processors. We also compare the parallel behaviours of the two methods. After thorough experimentation, we conclude that for many data mining applications, both QC and Maronna are viable options. Less robust, but faster, QC is the recommended choice for small parallel platforms. On the other hand, the Maronna method is the recommended choice when a high degree of robustness is required, or when the parallel platform features a high number of processors.
KDD	Kernel k-means: spectral clustering and normalized cuts.	Inderjit S. Dhillon,Yuqiang Guan,Brian Kulis	2004	Kernel k-means and spectral clustering have both been used to identify clusters that are non-linearly separable in input space. Despite significant research, these methods have remained only loosely related. In this paper, we give an explicit theoretical connection between them. We show the generality of the weighted kernel k-means objective function, and derive the spectral clustering objective of normalized cut as a special case. Given a positive definite similarity matrix, our results lead to a novel weighted kernel k-means algorithm that monotonically decreases the normalized cut. This has important implications: a) eigenvector-based algorithms, which can be computationally prohibitive, are not essential for minimizing normalized cuts, b) various techniques, such as local search and acceleration schemes, may be used to improve the quality as well as speed of kernel k-means. Finally, we present results on several interesting data sets, including diametrical clustering of large gene-expression matrices and a handwriting recognition data set.
KDD	IncSpan: incremental mining of sequential patterns in large database.	Hong Cheng,Xifeng Yan,Jiawei Han	2004	Many real life sequence databases grow incrementally. It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow, or when some new sequences are added into the database. Incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates. However, it is nontrivial to mine sequential patterns incrementally, especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones. In this study, we develop an efficient algorithm, IncSpan, for incremental mining of sequential patterns, by exploring some interesting properties. Our performance study shows that IncSpan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin.
KDD	Early detection of insider trading in option markets.	Steve Donoho	2004	"""Inside information"" comes in many forms: knowledge of a corporate takeover, a terrorist attack, unexpectedly poor earnings, the FDA's acceptance of a new drug, etc. Anyone who knows some piece of soon-to-break news possesses inside information. Historically, insider trading has been detected after the news is public, but this is often too late: fraud has been perpetrated, innocent investors have been disadvantaged, or terrorist acts have been carried out. This paper explores early detection of insider trading - detection before the news breaks. Data mining holds great promise for this emerging application, but the problem also poses significant challenges. We present the specific problem of insider trading in option markets, compare decision tree, logistic regression, and neural net results to results from an expert model, and discuss insights that knowledge discovery techniques shed upon this problem."
KDD	Efficient closed pattern mining in the presence of tough block constraints.	Krishna Gade,Jianyong Wang,George Karypis	2004	"Various constrained frequent pattern mining problem formulations and associated algorithms have been developed that enable the user to specify various itemset-based constraints that better capture the underlying application requirements and characteristics. In this paper we introduce a new class of block constraints that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern's items and its associated set of transactions. Block constraints provide a natural framework by which a number of important problems can be specified and make it possible to solve numerous problems on binary and real-valued datasets. However, developing computationally efficient algorithms to find these block constraints poses a number of challenges as unlike the different itemset-based constraints studied earlier, these block constraints are tough as they are neither anti-monotone, monotone, nor convertible. To overcome this problem, we introduce a new class of pruning methods that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called CBMiner to find the closed itemsets that satisfy the block constraints."
KDD	A microeconomic data mining problem: customer-oriented catalog segmentation.	Martin Ester,Rong Ge,Wen Jin,Zengjian Hu	2004	The microeconomic framework for data mining [7] assumes that an enterprise chooses a decision maximizing the overall utility over all customers where the contribution of a customer is a function of the data available on that customer. In Catalog Segmentation, the enterprise wants to design k product catalogs of size r that maximize the overall number of catalog products purchased. However, there are many applications where a customer, once attracted to an enterprise, would purchase more products beyond the ones contained in the catalog. Therefore, in this paper, we investigate an alternative problem formulation, that we call Customer-Oriented Catalog Segmentation, where the overall utility is measured by the number of customers that have at least a specified minimum interest t in the catalogs. We formally introduce the Customer-Oriented Catalog Segmentation problem and discuss its complexity. Then we investigate two different paradigms to design efficient, approximate algorithms for the Customer-Oriented Catalog Segmentation problem, greedy (deterministic) and randomized algorithms. Since greedy algorithms may be trapped in a local optimum and randomized algorithms crucially depend on a reasonable initial solution, we explore a combination of these two paradigms. Our experimental evaluation on synthetic and real data demonstrates that the new algorithms yield catalogs of significantly higher utility compared to classical Catalog Segmentation algorithms.
KDD	k-TTP: a new privacy model for large-scale distributed environments.	Bobi Gilburd,Assaf Schuster,Ran Wolff	2004	Secure multiparty computation allows parties to jointly compute a function of their private inputs without revealing anything but the output. Theoretical results [2] provide a general construction of such protocols for any function. Protocols obtained in this way are, however, inefficient, and thus, practically speaking, useless when a large number of participants are involved.The contribution of this paper is to define a new privacy model -- k-privacy -- by means of an innovative, yet natural generalization of the accepted trusted third party model. This allows implementing cryptographically secure efficient primitives for real-world large-scale distributed systems.As an example for the usefulness of the proposed model, we employ k-privacy to introduce a technique for obtaining knowledge -- by way of an association-rule mining algorithm -- from large-scale Data Grids, while ensuring that the privacy is cryptographically secure.
KDD	Regularized multi--task learning.	Theodoros Evgeniou,Massimiliano Pontil	2004	Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.
KDD	Fast discovery of connection subgraphs.	Christos Faloutsos,Kevin S. McCurley,Andrew Tomkins	2004	We define a connection subgraph as a small subgraph of a large graph that best captures the relationship between two nodes. The primary motivation for this work is to provide a paradigm for exploration and knowledge discovery in large social networks graphs. We present a formal definition of this problem, and an ideal solution based on electricity analogues. We then show how to accelerate the computations, to produce approximate, but high-quality connection subgraphs in real time on very large (disk resident) graphs.We describe our operational prototype, and we demonstrate results on a social network graph derived from the World Wide Web. Our graph contains 15 million nodes and 96 million edges, and our system still produces quality responses within seconds.
KDD	User-centered design for KDD.	Eric Haseltine	2004	"During initial development, KDD solutions often focus heavily on algorithms, architectures, software, hardware, and systems engineering challenges, without first thoroughly exploring how end-users will employ the new KDD technology. As a result of such ""system-centered"" design, many useless features are implemented that prolong development and significantly add to life cycle cost, while making the system hard to operate and use. This presentation will describe an alternate ""user-centered"" approach -- borrowed from the consumer products industry -- that can produce KDD solutions with shorter development cycles, lower costs, and much better usability."
KDD	Discovering complex matchings across web query interfaces: a correlation mining approach.	Bin He,Kevin Chen-Chuan Chang,Jiawei Han	2004	"To enable information integration, schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources. While complex matchings are common, because of their far more complex search space, most existing techniques focus on simple 1:1 matchings. To tackle this challenge, this paper takes a conceptually novel approach by viewing schema matching as correlation mining, for our task of matching Web query interfaces to integrate the myriad databases on the Internet. On this ""deep Web,"" query interfaces generally form complex matchings between attribute groups (e.g., [author] corresponds to [first name, last name] in the Books domain). We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships: grouping attributes (e.g., [first name, last name]) tend to be co-present in query interfaces and thus positively correlated. In contrast, synonym attributes are negatively correlated because they rarely co-occur. This insight enables us to discover complex matchings by a correlation mining approach. In particular, we develop the DCM framework, which consists of data preparation, dual mining of positive and negative correlations, and finally matching selection. Unlike previous correlation mining algorithms, which mainly focus on finding strong positive correlations, our algorithm cares both positive and negative correlations, especially the subtlety of negative correlations, due to its special importance in schema matching. This leads to the introduction of a new correlation measure, $H$-measure, distinct from those proposed in previous work. We evaluate our approach extensively and the results show good accuracy for discovering complex matchings."
KDD	Graphical models for data mining.	David Heckerman	2004	I will discuss the use of graphical models for data mining. I will review key research areas including structure learning, variational methods, a relational modeling, and describe applications ranging from web traffic analysis to AIDS vaccine design.
KDD	Diagnosing extrapolation: tree-based density estimation.	Giles Hooker	2004	There has historically been very little concern with extrapolation in Machine Learning, yet extrapolation can be critical to diagnose. Predictor functions are almost always learned on a set of highly correlated data comprising a very small segment of predictor space. Moreover, flexible predictors, by their very nature, are not controlled at points of extrapolation. This becomes a problem for diagnostic tools that require evaluation on a product distribution. It is also an issue when we are trying to optimize a response over some variable in the input space. Finally, it can be a problem in non-static systems in which the underlying predictor distribution gradually drifts with time or when typographical errors misrecord the values of some predictors.We present a diagnosis for extrapolation as a statistical test for a point originating from the data distribution as opposed to a null hypothesis uniform distribution. This allows us to employ general classification methods for estimating such a test statistic. Further, we observe that CART can be modified to accept an exact distribution as an argument, providing a better classification tool which becomes our extrapolation-detection procedure. We explore some of the advantages of this approach and present examples of its practical application.
KDD	Discovering additive structure in black box functions.	Giles Hooker	2004	Many automated learning procedures lack interpretability, operating effectively as a black box: providing a prediction tool but no explanation of the underlying dynamics that drive it. A common approach to interpretation is to plot the dependence of a learned function on one or two predictors. We present a method that seeks not to display the behavior of a function, but to evaluate the importance of non-additive interactions within any set of variables. Should the function be close to a sum of low dimensional components, these components can be viewed and even modeled parametrically. Alternatively, the work here provides an indication of where intrinsically high-dimensional behavior takes place.The calculations used in this paper correspond closely with the functional ANOVA decomposition; a well-developed construction in Statistics. In particular, the proposed score of interaction importance measures the loss associated with the projection of the prediction function onto a space of additive models. The algorithm runs in linear time and we present displays of the output as a graphical model of the function for interpretation purposes.
KDD	Cyclic pattern kernels for predictive graph mining.	Tamás Horváth,Thomas Gärtner,Stefan Wrobel	2004	With applications in biology, the world-wide web, and several other areas, mining of graph-structured objects has received significant interest recently. One of the major research directions in this field is concerned with predictive data mining in graph databases where each instance is represented by a graph. Some of the proposed approaches for this task rely on the excellent classification performance of support vector machines. To control the computational cost of these approaches, the underlying kernel functions are based on frequent patterns. In contrast to these approaches, we propose a kernel function based on a natural set of cyclic and tree patterns independent of their frequency, and discuss its computational aspects. To practically demonstrate the effectiveness of our approach, we use the popular NCI-HIV molecule dataset. Our experimental results show that cyclic pattern kernels can be computed quickly and offer predictive performance superior to recent graph kernels based on frequent patterns.
KDD	SPIN: mining maximal frequent subgraphs from graph databases.	Jun Huan,Wei Wang,Jan Prins,Jiong Yang	2004	One fundamental challenge for mining recurring subgraphs from semi-structured data sets is the overwhelming abundance of such patterns. In large graph databases, the total number of frequent subgraphs can become too large to allow a full enumeration using reasonable computational resources. In this paper, we propose a new algorithm that mines only maximal frequent subgraphs, i.e. subgraphs that are not a part of any other frequent subgraphs. This may exponentially decrease the size of the output set in the best case; in our experiments on practical data sets, mining maximal frequent subgraphs reduces the total number of mined patterns by two to three orders of magnitude.Our method first mines all frequent trees from a general graph database and then reconstructs all maximal subgraphs from the mined trees. Using two chemical structure benchmarks and a set of synthetic graph data sets, we demonstrate that, in addition to decreasing the output size, our algorithm can achieve a five-fold speed up over the current state-of-the-art subgraph mining algorithms.
KDD	Eigenspace-based anomaly detection in computer systems.	Tsuyoshi Idé,Hisashi Kashima	2004	"We report on an automated runtime anomaly detection method at the application layer of multi-node computer systems. Although several network management systems are available in the market, none of them have sufficient capabilities to detect faults in multi-tier Web-based systems with redundancy. We model a Web-based system as a weighted graph, where each node represents a ""service"" and each edge represents a dependency between services. Since the edge weights vary greatly over time, the problem we address is that of anomaly detection from a time sequence of graphs.In our method, we first extract a feature vector from the adjacency matrix that represents the activities of all of the services. The heart of our method is to use the principal eigenvector of the eigenclusters of the graph. Then we derive a probability distribution for an anomaly measure defined for a time-series of directional data derived from the graph sequence. Given a critical probability, the threshold value is adaptively updated using a novel online algorithm.We demonstrate that a fault in a Web application can be automatically detected and the faulty services are identified without using detailed knowledge of the behavior of the system."
KDD	On detecting space-time clusters.	Vijay S. Iyengar	2004	Detection of space-time clusters is an important function in various domains (e.g., epidemiology and public health). The pioneering work on the spatial scan statistic is often used as the basis to detect and evaluate such clusters. State-of-the-art systems based on this approach detect clusters with restrictive shapes that cannot model growth and shifts in location over time. We extend these methods significantly by using the flexible square pyramid shape to model such effects. A heuristic search method is developed to detect the most likely clusters using a randomized algorithm in combination with geometric shapes processing. The use of Monte Carlo methods in the original scan statistic formulation is continued in our work to address the multiple hypothesis testing issues. Our method is applied to a real data set on brain cancer occurrences over a 19 year period. The cluster detected by our method shows both growth and movement which could not have been modeled with the simpler cylindrical shapes used earlier. Our general framework can be extended quite easily to handle other flexible shapes for the space-time clusters.
KDD	Interestingness of frequent itemsets using Bayesian networks as background knowledge.	Szymon Jaroszewicz,Dan A. Simovici	2004	The paper presents a method for pruning frequent itemsets based on background knowledge represented by a Bayesian network. The interestingness of an itemset is defined as the absolute difference between its support estimated from data and from the Bayesian network. Efficient algorithms are presented for finding interestingness of a collection of frequent itemsets, and for finding all attribute sets with a given minimum interestingness. Practical usefulness of the algorithms and their efficiency have been verified experimentally.
KDD	Mining the space of graph properties.	Glen Jeh,Jennifer Widom	2004	Existing data mining algorithms on graphs look for nodes satisfying specific properties, such as specific notions of structural similarity or specific measures of link-based importance. While such analyses for predetermined properties can be effective in well-understood domains, sometimes identifying an appropriate property for analysis can be a challenge, and focusing on a single property may neglect other important aspects of the data. In this paper, we develop a foundation for mining the properties themselves. We present a theoretical framework defining the space of graph properties, a variety of mining queries enabled by the framework, techniques to handle the enormous size of the query space, and an experimental system called F-Miner that demonstrates the utility and feasibility of property mining.
KDD	Why collective inference improves relational classification.	David Jensen,Jennifer Neville,Brian Gallagher	2004	Procedures for collective inference make simultaneous statistical judgments about the same variables for a set of related data instances. For example, collective inference could be used to simultaneously classify a set of hyperlinked documents or infer the legitimacy of a set of related financial transactions. Several recent studies indicate that collective inference can significantly reduce classification error when compared with traditional inference techniques. We investigate the underlying mechanisms for this error reduction by reviewing past work on collective inference and characterizing different types of statistical models used for making inference in relational data. We show important differences among these models, and we characterize the necessary and sufficient conditions for reduced classification error based on experiments with real and simulated data.
KDD	Mining coherent gene clusters from gene-sample-time microarray data.	Daxin Jiang,Jian Pei,Murali Ramanathan,Chun Tang,Aidong Zhang	2004	Extensive studies have shown that mining microarray data sets is important in bioinformatics research and biomedical applications. In this paper, we explore a novel type of gene-sample-time microarray data sets, which records the expression levels of various genes under a set of samples during a series of time points. In particular, we propose the mining of coherent gene clusters from such data sets. Each cluster contains a subset of genes and a subset of samples such that the genes are coherent on the samples along the time series. The coherent gene clusters may identify the samples corresponding to some phenotypes (e.g., diseases), and suggest the candidate genes correlated to the phenotypes. We present two efficient algorithms, namely the Sample-Gene Search and the Gene-Sample Search, to mine the complete set of coherent gene clusters. We empirically evaluate the performance of our approaches on both a real microarray data set and synthetic data sets. The test results have shown that our approaches are both efficient and effective to find meaningful coherent gene clusters.
KDD	Web usage mining based on probabilistic latent semantic analysis.	Xin Jin,Yanzan Zhou,Bamshad Mobasher	2004	The primary goal of Web usage mining is the discovery of patterns in the navigational behavior of Web users. Standard approaches, such as clustering of user sessions and discovering association rules or frequent navigational paths, do not generally provide the ability to automatically characterize or quantify the unobservable factors that lead to common navigational patterns. It is, therefore, necessary to develop techniques that can automatically discover hidden semantic relationships among users as well as between users and Web objects. Probabilistic Latent Semantic Analysis (PLSA) is particularly useful in this context, since it can uncover latent semantic associations among users and pages based on the co-occurrence patterns of these pages in user sessions. In this paper, we develop a unified framework for the discovery and analysis of Web navigational patterns based on PLSA. We show the flexibility of this framework in characterizing various relationships among users and Web objects. Since these relationships are measured in terms of probabilities, we are able to use probabilistic inference to perform a variety of analysis tasks such as user segmentation, page classification, as well as predictive tasks such as collaborative recommendations. We demonstrate the effectiveness of our approach through experiments performed on real-world data sets.
KDD	A system for automated mapping of bill-of-materials part numbers.	Jayant Kalagnanam,Moninder Singh,Sudhir Verma,Michael Patek,Yuk Wah Wong	2004	"Part numbers are widely used within an enterprise throughout the manufacturing process. The point of entry of such part numbers into this process is normally via a Bill of Materials, or BOM, sent by a contact manufacturer or supplier. Each line of the BOM provides information about one part such as the supplier part number, the BOM receiver's corresponding internal part number, an unstructured textual part description, the supplier name, etc. However, in a substantial number of cases, the BOM receiver's internal part number is absent. Hence, before this part can be incorporated into the receiver's manufacturing process, it has to be mapped to an internal part (of the BOM receiver) based on the information of the part in the BOM. Historically, this mapping process has been done manually which is a highly time-consuming, labor intensive and error-prone process. This paper describes a system for automating the mapping of BOM part numbers. The system uses a two step modeling and mapping approach. First, the system uses historical BOM data, receiver's part specifications data and receiver's part taxonomic data along with domain knowledge to automatically learn classification models for mapping a given BOM part description to successively lower levels of the receiver's part taxonomy to reduce the set of potential internal parts to which the BOM part could map to. Then, information about various part parameters is extracted from the BOM part description and compared to the specifications data of the potential internal parts to choose the final mapped internal part. Mappings done by the system are very accurate, and the system is currently being deployed within IBM for mapping BOMs received by the corporate procurement/manufacturing divisions."
KDD	When do data mining results violate privacy?	Murat Kantarcioglu,Jiashun Jin,Chris Clifton	2004	Privacy-preserving data mining has concentrated on obtaining valid results when the input data is private. An extreme example is Secure Multiparty Computation-based methods, where only the results are revealed. However, this still leaves a potential privacy breach: Do the results themselves violate privacy? This paper explores this issue, developing a framework under which this question can be addressed. Metrics are proposed, along with analysis that those metrics are consistent in the face of apparent problems.
KDD	Towards parameter-free data mining.	Eamonn J. Keogh,Stefano Lonardi,Chotirat (Ann) Ratanamahatana	2004	Most data mining algorithms require the setting of many input parameters. Two main dangers of working with parameter-laden algorithms are the following. First, incorrect settings may cause an algorithm to fail in finding the true patterns. Second, a perhaps more insidious problem is that the algorithm may report spurious patterns that do not really exist, or greatly overestimate the significance of the reported patterns. This is especially likely when the user fails to understand the role of parameters in the data mining process.Data mining algorithms should have as few parameters as possible, ideally none. A parameter-free algorithm would limit our ability to impose our prejudices, expectations, and presumptions on the problem at hand, and would let the data itself speak to us. In this work, we show that recent results in bioinformatics and computational theory hold great promise for a parameter-free data-mining paradigm. The results are motivated by observations in Kolmogorov complexity theory. However, as a practical matter, they can be implemented using any off-the-shelf compression algorithm with the addition of just a dozen or so lines of code. We will show that this approach is competitive or superior to the state-of-the-art approaches in anomaly/interestingness detection, classification, and clustering with empirical tests on time series/DNA/text/video datasets.
KDD	Improved robustness of signature-based near-replica detection via lexicon randomization.	Aleksander Kolcz,Abdur Chowdhury,Joshua Alspector	2004	Detection of near duplicate documents is an important problem in many data mining and information filtering applications. When faced with massive quantities of data, traditional duplicate detection techniques relying on direct inter-document similarity computation (e.g., using the cosine measure) are often not feasible given the time and memory performance constraints. On the other hand, fingerprint-based methods, such as I-Match, are very attractive computationally but may be brittle with respect to small changes to document content. We focus on approaches to near-replica detection that are based upon large-collection statistics and present a general technique of increasing their robustness via multiple lexicon randomization. In experiments with large web-page and spam-email datasets the proposed method is shown to consistently outperform traditional I-Match, with the relative improvement in duplicate-document recall reaching as high as 40-60%. The large gains in detection accuracy are offset by only small increases in computational requirements.
KDD	Learning to detect malicious executables in the wild.	Jeremy Z. Kolter,Marcus A. Maloof	2004	In this paper, we describe the development of a fielded application for detecting malicious executables in the wild. We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the roc curve of 0.996. Results also suggest that our methodology will scale to larger collections of executables. To the best of our knowledge, ours is the only fielded application for this task developed using techniques from machine learning and data mining.
KDD	A graph-theoretic approach to extract storylines from search results.	Ravi Kumar,Uma Mahadevan,D. Sivakumar	2004	We present a graph-theoretic approach to discover storylines from search results. Storylines are windows that offer glimpses into interesting themes latent among the top search results for a query; they are different from, and complementary to, clusters obtained through traditional approaches. Our framework is axiomatically developed and combinatorial in nature, based on generalizations of the maximum induced matching problem on bipartite graphs. The core algorithmic task involved is to mine for signature structures in a robust graph representation of the search results. We present a very fast algorithm for this task based on local search. Experiments show that the collection of storylines extracted through our algorithm offers a concise organization of the wealth of information hidden beyond the first page of search results.
KDD	Learning spatially variant dissimilarity (SVaD) measures.	Krishna Kummamuru,Raghu Krishnapuram,Rakesh Agrawal	2004	Clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed (dis)similarity measure between the data points in feature space. This makes the type of clusters identified highly dependent on the assumed similarity measure. Building on recent work in this area, we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data. The idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure. Our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature.
KDD	Effective localized regression for damage detection in large complex mechanical structures.	Aleksandar Lazarevic,Ramdev Kanapady,Chandrika Kamath	2004	In this paper, we propose a novel data mining technique for the efficient damage detection within the large-scale complex mechanical structures. Every mechanical structure is defined by the set of finite elements that are called structure elements. Large-scale complex structures may have extremely large number of structure elements, and predicting the failure in every single element using the original set of natural frequencies as features is exceptionally time-consuming task. Traditional data mining techniques simply predict failure in each structure element individually using global prediction models that are built considering all data records. In order to reduce the time complexity of these models, we propose a localized clustering-regression based approach that consists of two phases: (1) building a local cluster around a data record of interest and (2) predicting an intensity of damage only in those structure elements that correspond to data records from the built cluster. For each test data record, we first build a cluster of data records from training data around it. Then, for each data record that belongs to discovered cluster, we identify corresponding structure elements and we build a localized regression model for each of these structure elements. These regression models for specific structure elements are constructed using only a specific set of relevant natural frequencies and merely those data records that correspond to the failure of that structure element. Experiments performed on the problem of damage prediction in a large electric transmission tower frame indicate that the proposed localized clustering-regression based approach is significantly more accurate and more computationally efficient than our previous hierarchical clustering approach, as well as global prediction models.
KDD	Incremental maintenance of quotient cube for median.	Cuiping Li,Gao Cong,Anthony K. H. Tung,Shan Wang	2004	Data cube pre-computation is an important concept for supporting OLAP(Online Analytical Processing) and has been studied extensively. It is often not feasible to compute a complete data cube due to the huge storage requirement. Recently proposed quotient cube addressed this issue through a partitioning method that groups cube cells into equivalence partitions. Such an approach is not only useful for distributive aggregate functions such as SUM but can also be applied to the holistic aggregate functions like MEDIAN.Maintaining a data cube for holistic aggregation is a hard problem since its difficulty lies in the fact that history tuple values must be kept in order to compute the new aggregate when tuples are inserted or deleted. The quotient cube makes the problem harder since we also need to maintain the equivalence classes. In this paper, we introduce two techniques called addset data structure and sliding window to deal with this problem. We develop efficient algorithms for maintaining a quotient cube with holistic aggregation functions that takes up reasonably small storage space. Performance study shows that our algorithms are effective, efficient and scalable over large databases.
KDD	Clustering moving objects.	Yifan Li,Jiawei Han,Jiong Yang	2004	Due to the advances in positioning technologies, the real time information of moving objects becomes increasingly available, which has posed new challenges to the database research. As a long-standing technique to identify overall distribution patterns in data, clustering has achieved brilliant successes in analyzing static datasets. In this paper, we study the problem of clustering moving objects, which could catch interesting pattern changes during the motion process and provide better insight into the essence of the mobile data points. In order to catch the spatial-temporal regularities of moving objects and handle large amounts of data, micro-clustering [20] is employed. Efficient techniques are proposed to keep the moving micro-clusters geographically small. Important events such as the collisions among moving micro-clusters are also identified. In this way, high quality moving micro-clusters are dynamically maintained, which leads to fast and competitive clustering result at any given time instance. We validate our approaches with a through experimental evaluation, where orders of magnitude improvement on running time is observed over normal K-Means clustering method [14].
KDD	Visually mining and monitoring massive time series.	Jessica Lin,Eamonn J. Keogh,Stefano Lonardi,Jeffrey P. Lankford,Donna M. Nystrom	2004	Moments before the launch of every space vehicle, engineering discipline specialists must make a critical go/no-go decision. The cost of a false positive, allowing a launch in spite of a fault, or a false negative, stopping a potentially successful launch, can be measured in the tens of millions of dollars, not including the cost in morale and other more intangible detriments. The Aerospace Corporation is responsible for providing engineering assessments critical to the go/no-go decision for every Department of Defense space vehicle. These assessments are made by constantly monitoring streaming telemetry data in the hours before launch. We will introduce VizTree, a novel time-series visualization tool to aid the Aerospace analysts who must make these engineering assessments. VizTree was developed at the University of California, Riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry. The use of a single tool for both aspects of the task allows a natural and intuitive transfer of mined knowledge to the monitoring task. Our visualization approach works by transforming the time series into a symbolic representation, and encoding the data in a modified suffix tree in which the frequency and other properties of patterns are mapped onto colors and other visual properties. We demonstrate the utility of our system by comparing it with state-of-the-art batch algorithms on several real and synthetic datasets.
KDD	A framework for ontology-driven subspace clustering.	Jinze Liu,Wei Wang,Jiong Yang	2004	Traditional clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes. While domain knowledge is always the best way to justify clustering, few clustering algorithms have ever take domain knowledge into consideration. In this paper, the domain knowledge is represented by hierarchical ontology. We develop a framework by directly incorporating domain knowledge into clustering process, yielding a set of clusters with strong ontology implication. During the clustering process, ontology information is utilized to efficiently prune the exponential search space of the subspace clustering algorithms. Meanwhile, the algorithm generates automatical interpretation of the clustering result by mapping the natural hierarchical organized subspace clusters with significant categorical enrichment onto the ontology hierarchy. Our experiments on a set of gene expression data using gene ontology demonstrate that our pruning technique driven by ontology significantly improve the clustering performance with minimal degradation of the cluster quality. Meanwhile, many hierarchical organizations of gene clusters corresponding to a sub-hierarchies in gene ontology were also successfully captured.
KDD	The IOC algorithm: efficient many-class non-parametric classification for high-dimensional data.	Ting Liu,Ke Yang,Andrew W. Moore	2004	This paper is about a variant of k nearest neighbor classification on large many-class high dimensional datasets.K nearest neighbor remains a popular classification technique, especially in areas such as computer vision, drug activity prediction and astrophysics. Furthermore, many more modern classifiers, such as kernel-based Bayes classifiers or the prediction phase of SVMs, require computational regimes similar to k-NN. We believe that tractable k-NN algorithms therefore continue to be important.This paper relies on the insight that even with many classes, the task of finding the majority class among the k nearest neighbors of a query need not require us to explicitly find those k nearest neighbors. This insight was previously used in (Liu et al., 2003) in two algorithms called KNS2 and KNS3 which dealt with fast classification in the case of two classes. In this paper we show how a different approach, IOC (standing for the International Olympic Committee) can apply to the case of n classes where n > 2.IOC assumes a slightly different processing of the datapoints in the neighborhood of the query. This allows it to search a set of metric trees, one for each class. During the searches it is possible to quickly prune away classes that cannot possibly be the majority.We give experimental results on datasets of up to 5.8 x 105 records and 1.5 x 103 attributes, frequently showing an order of magnitude acceleration compared with each of (i) conventional linear scan, (ii) a well-known independent SR-tree implementation of conventional k-NN and (iii) a highly optimized conventional k-NN metric tree search.
KDD	Mining, indexing, and querying historical spatiotemporal data.	Nikos Mamoulis,Huiping Cao,George Kollios,Marios Hadjieleftheriou,Yufei Tao,David W. Cheung	2004	In many applications that track and analyze spatiotemporal data, movements obey periodic patterns; the objects follow the same routes (approximately) over regular time intervals. For example, people wake up at the same time and follow more or less the same route to their work everyday. The discovery of hidden periodic patterns in spatiotemporal data, apart from unveiling important information to the data analyst, can facilitate data management substantially. Based on this observation, we propose a framework that analyzes, manages, and queries object movements that follow such patterns. We define the spatiotemporal periodic pattern mining problem and propose an effective and fast mining algorithm for retrieving maximal periodic patterns. We also devise a novel, specialized index structure that can benefit from the discovered patterns to support more efficient execution of spatiotemporal queries. We evaluate our methods experimentally using datasets with object trajectories that exhibit periodicity.
KDD	Sleeved coclustering.	Avraham A. Melkman,Eran Shaham	2004	A coCluster of a m x n matrix X is a submatrix determined by a subset of the rows and a subset of the columns. The problem of finding coClusters with specific properties is of interest, in particular, in the analysis of microarray experiments. In that case the entries of the matrix X are the expression levels of $m$ genes in each of $n$ tissue samples. One goal of the analysis is to extract a subset of the samples and a subset of the genes, such that the expression levels of the chosen genes behave similarly across the subset of the samples, presumably reflecting an underlying regulatory mechanism governing the expression level of the genes.We propose to base the similarity of the genes in a coCluster on a simple biological model, in which the strength of the regulatory mechanism in sample j is Hj, and the response strength of gene i to the regulatory mechanism is Gi. In other words, every two genes participating in a good coCluster should have expression values in each of the participating samples, whose ratio is a constant depending only on the two genes. Noise in the expression levels of genes is taken into account by allowing a deviation from the model, measured by a relative error criterion. The sleeve-width of the coCluster reflects the extent to which entry i,j in the coCluster is allowed to deviate, relatively, from being expressed as the product GiHj.We present a polynomial-time Monte-Carlo algorithm which outputs a list of coClusters whose sleeve-widths do not exceed a prespecified value. Moreover, we prove that the list includes, with fixed probability, a coCluster which is near-optimal in its dimensions. Extensive experimentation with synthetic data shows that the algorithm performs well.
KDD	Tracking dynamics of topic trends using a finite mixture model.	Satoshi Morinaga,Kenji Yamanishi	2004	In a wide range of business areas dealing with text data streams, including CRM, knowledge management, and Web monitoring services, it is an important issue to discover topic trends and analyze their dynamics in real-time. Specifically we consider the following three tasks in topic trend analysis: 1)Topic Structure Identification; identifying what kinds of main topics exist and how important they are, 2)Topic Emergence Detection; detecting the emergence of a new topic and recognizing how it grows, 3)Topic Characterization; identifying the characteristics for each of main topics. For real topic analysis systems, we may require that these three tasks be performed in an on-line fashion rather than in a retrospective way, and be dealt with in a single framework. This paper proposes a new topic analysis framework which satisfies this requirement from a unifying viewpoint that a topic structure is modeled using a finite mixture model and that any change of a topic trend is tracked by learning the finite mixture model dynamically. In this framework we propose the usage of a time-stamp based discounting learning algorithm in order to realize real-time topic structure identification. This enables tracking the topic structure adaptively by forgetting out-of-date statistics. Further we apply the theory of dynamic model selection to detecting changes of main components in the finite mixture model in order to realize topic emergence detection. We demonstrate the effectiveness of our framework using real data collected at a help desk to show that we are able to track dynamics of topic trends in a timely fashion.
KDD	Machine learning for online query relaxation.	Ion Muslea	2004	"In this paper we provide a fast, data-driven solution to the failing query problem: given a query that returns an empty answer, how can one relax the query's constraints so that it returns a non-empty set of tuples? We introduce a novel algorithm, loqr, which is designed to relax queries that are in the disjunctive normal form and contain a mixture of discrete and continuous attributes. loqr discovers the implicit relationships that exist among the various domain attributes and then uses this knowledge to relax the constraints from the failing query.In a first step, loqr uses a small, randomly-chosen subset of the target database to learn a set of decision rules that predict whether an attribute's value satisfies the constraints in the failing query; this query-driven operation is performed online for each failing query. In the second step, loqr uses nearest-neighbor techniques to find the learned rule that is the most similar to the failing query; then it uses the attributes' values from this rule to relax the failing query's constraints. Our experiments on six application domains show that loqr is both robust and fast: it successfully relaxes more than 95% of the failing queries, and it takes under a second for processing queries that consist of up to 20 attributes (larger queries of up to 93 attributes are processed in several seconds)."
KDD	Mining traffic data from probe-car system for travel time prediction.	Takayuki Nakata,Jun-ichi Takeuchi	2004	We are developing a technique to predict travel time of a vehicle for an objective road section, based on real time traffic data collected through a probe-car system. In the area of Intelligent Transport System (ITS), travel time prediction is an important subject. Probe-car system is an upcoming data collection method, in which a number of vehicles are used as moving sensors to detect actual traffic situation. It can collect data concerning much larger area, compared with traditional fixed detectors. Our prediction technique is based on statistical analysis using AR model with seasonal adjustment and MDL (Minimum Description Length) criterion. Seasonal adjustment is used to handle periodicities of 24 hours in traffic data. Alternatively, we employ state space model, which can handle time series with periodicities. It is important to select really effective data for prediction, among the data from widespread area, which are collected via probe-car system. We do this using MDL criterion. That is, we find the explanatory variables that really have influence on the future travel time. In this paper, we experimentally show effectiveness of our method using probe-car data collected in Nagoya Metropolitan Area in 2002.
KDD	Semantic representation: search and mining of multimedia content.	Apostol Natsev,Milind R. Naphade,John R. Smith	2004	Semantic understanding of multimedia content is critical in enabling effective access to all forms of digital media data. By making large media repositories searchable, semantic content descriptions greatly enhance the value of such data. Automatic semantic understanding is a very challenging problem and most media databases resort to describing content in terms of low-level features or using manually ascribed annotations. Recent techniques focus on detecting semantic concepts in video, such as indoor, outdoor, face, people, nature, etc. This approach works for a fixed lexicon for which annotated training examples exist. In this paper we consider the problem of using such semantic concept detection to map the video clips into semantic spaces. This is done by constructing a model vector that acts as a compact semantic representation of the underlying content. We then present experiments in the semantic spaces leveraging such information for enhanced semantic retrieval, classification, visualization, and data mining purposes. We evaluate these ideas using a large video corpus and demonstrate significant performance gains in retrieval effectiveness.
KDD	Rapid detection of significant spatial clusters.	Daniel B. Neill,Andrew W. Moore	2004	"Given an N x N grid of squares, where each square has a count cij and an underlying population pij, our goal is to find the rectangular region with the highest density, and to calculate its significance by randomization. An arbitrary density function D, dependent on a region's total count C and total population P, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff's spatial scan statistic DK to find the most significant spatial disease cluster. A naive approach to finding the maximum density region requires O(N4) time, and is generally computationally infeasible. We present a multiresolution algorithm which partitions the grid into overlapping regions using a novel overlap-kd tree data structure, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufficiently dense regions, this method finds the maximum density region in O((N log N)2) time, in practice resulting in significant (20-2000x) speedups on both real and simulated datasets."
KDD	A quickstart in frequent structure mining can make a difference.	Siegfried Nijssen,Joost N. Kok	2004	"Given a database, structure mining algorithms search for substructures that satisfy constraints such as minimum frequency, minimum confidence, minimum interest and maximum frequency. Examples of substructures include graphs, trees and paths. For these substructures many mining algorithms have been proposed. In order to make graph mining more efficient, we investigate the use of the ""quickstart principle"", which is based on the fact that these classes of structures are contained in each other, thus allowing for the development of structure mining algorithms that split the search into steps of increasing complexity. We introduce the GrAph/Sequence/Tree extractiON (Gaston) algorithm that implements this idea by searching first for frequent paths, then frequent free trees and finally cyclic graphs. We investigate two alternatives for computing the frequency of structures and present experimental results to relate these alternatives."
KDD	Programming the K-means clustering algorithm in SQL.	Carlos Ordonez	2004	Using SQL has not been considered an efficient and feasible way to implement data mining algorithms. Although this is true for many data mining, machine learning and statistical algorithms, this work shows it is feasible to get an efficient SQL implementation of the well-known K-means clustering algorithm that can work on top of a relational DBMS. The article emphasizes both correctness and performance. From a correctness point of view the article explains how to compute Euclidean distance, nearest-cluster queries and updating clustering results in SQL. From a performance point of view it is explained how to cluster large data sets defining and indexing tables to store and retrieve intermediate and final results, optimizing and avoiding joins, optimizing and simplifying clustering aggregations, and taking advantage of sufficient statistics. Experiments evaluate scalability with synthetic data sets varying size and dimensionality. The proposed K-means implementation can cluster large data sets and exhibits linear scalability.
KDD	Automatic multimedia cross-modal correlation discovery.	Jia-Yu Pan,Hyung-Jeong Yang,Christos Faloutsos,Pinar Duygulu	2004	"Given an image (or video clip, or audio song), how do we automatically assign keywords to it? The general problem is to find correlations across the media in a collection of multimedia objects like video clips, with colors, and/or motion, and/or audio, and/or text scripts. We propose a novel, graph-based approach, ""MMG"", to discover such cross-modal correlations.Our ""MMG"" method requires no tuning, no clustering, no user-determined constants; it can be applied to any multimedia collection, as long as we have a similarity function for each medium; and it scales linearly with the database size. We report auto-captioning experiments on the ""standard"" Corel image database of 680 MB, where it outperforms domain specific, fine-tuned methods by up to 10 percentage points in captioning accuracy (50% relative improvement)."
KDD	Document preprocessing for naive Bayes classification and clustering with mixture of multinomials.	Dmitry Pavlov,Ramnath Balasubramanyan,Byron Dom,Shyam Kapur,Jignashu Parikh	2004	Naive Bayes classifier has long been used for text categorization tasks. Its sibling from the unsupervised world, the probabilistic mixture of multinomial models, has likewise been successfully applied to text clustering problems. Despite the strong independence assumptions that these models make, their attractiveness come from low computational cost, relatively low memory consumption, ability to handle heterogeneous features and multiple classes, and often competitiveness with the top of the line models. Recently, there has been several attempts to alleviate the problems of Naive Bayes by performing heuristic feature transformations, such as IDF, normalization by the length of the documents and taking the logarithms of the counts. We justify the use of these techniques and apply them to two problems: classification of products in Yahoo! Shopping and clustering the vectors of collocated terms in user queries to Yahoo! Search. The experimental evaluation allows us to draw conclusions about the promise that these transformations carry with regard to alleviating the strong assumptions of the multinomial model.
KDD	Incorporating prior knowledge with weighted margin support vector machines.	Xiaoyun Wu,Rohini K. Srihari	2004	Like many purely data-driven machine learning methods, Support Vector Machine (SVM) classifiers are learned exclusively from the evidence presented in the training dataset; thus a larger training dataset is required for better performance. In some applications, there might be human knowledge available that, in principle, could compensate for the lack of data. In this paper, we propose a simple generalization of SVM: Weighted Margin SVM (WMSVMs) that permits the incorporation of prior knowledge. We show that Sequential Minimal Optimization can be used in training WMSVM. We discuss the issues of incorporating prior knowledge using this rather general formulation. The experimental results show that the proposed methods of incorporating prior knowledge is effective.
KDD	Estimating the size of the telephone universe: a Bayesian Mark-recapture approach.	David Poole	2004	Mark-recapture models have for many years been used to estimate the unknown sizes of animal and bird populations. In this article we adapt a finite mixture mark-recapture model in order to estimate the number of active telephone lines in the USA. The idea is to use the calling patterns of lines that are observed on the long distance network to estimate the number of lines that do not appear on the network. We present a Bayesian approach and use Markov chain Monte Carlo methods to obtain inference from the posterior distributions of the model parameters. At the state level, our results are in fairly good agreement with recent published reports on line counts. For lines that are easily classified as business or residence, the estimates have low variance. When the classification is unknown, the variability increases considerably. Results are insensitive to changes in the prior distributions. We discuss the significant computational and data mining challenges caused by the scale of the data, approximately 350 million call-detail records per day observed over a number of weeks.
KDD	Cluster-based concept invention for statistical relational learning.	Alexandrin Popescul,Lyle H. Ungar	2004	"We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving ""topics"", and authors can be clustered based on documents they co-author giving ""communities"". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data."
KDD	Identifying early buyers from purchase data.	Paat Rusmevichientong,Shenghuo Zhu,David Selinger	2004	Market research has shown that consumers exhibit a variety of different purchasing behaviors; specifically, some tend to purchase products earlier than other consumers. Identifying such early buyers can help personalize marketing strategies, potentially improving their effectiveness. In this paper, we present a non-parametric approach to the problem of identifying early buyers from purchase data. Our formulation takes as inputs the detailed purchase information of each consumer, with which we construct a weighted directed graph whose nodes correspond to consumers and whose edges correspond to purchases consumers have in common; the edge weights indicate how frequently consumers purchase products earlier than other consumers.Identifying early buyers corresponds to the problem of finding a subset of nodes in the graph with maximum difference between the weights of the outgoing and incoming edges. This problem is a variation of the maximum cut problem in a directed graph. We provide an approximation algorithm based on semidefinite programming (SDP) relaxations pioneered by Goemans and Williamson, and analyze its performance. We apply the algorithm to real purchase data from Amazon.com, providing new insights into consumer behaviors.
KDD	Turning CARTwheels: an alternating algorithm for mining redescriptions.	Naren Ramakrishnan,Deept Kumar,Bud Mishra,Malcolm Potts,Richard F. Helm	2004	We present an unusual algorithm involving classification trees---CARTwheels---where two trees are grown in opposite directions so that they are joined at their leaves. This approach finds application in a new data mining task we formulate, called redescription mining. A redescription is a shift-of-vocabulary, or a different way of communicating information about a given subset of data; the goal of redescription mining is to find subsets of data that afford multiple descriptions. We highlight the importance of this problem in domains such as bioinformatics, which exhibit an underlying richness and diversity of data descriptors (e.g., genes can be studied in a variety of ways). CARTwheels exploits the duality between class partitions and path partitions in an induced classification tree to model and mine redescriptions. It helps integrate multiple forms of characterizing datasets, situates the knowledge gained from one dataset in the context of others, and harnesses high-level abstractions for uncovering cryptic and subtle features of data. Algorithm design decisions, implementation details, and experimental results are presented.
KDD	Privacy preserving regression modelling via distributed computation.	Ashish P. Sanil,Alan F. Karr,Xiaodong Lin,Jerome P. Reiter	2004	Reluctance of data owners to share their possibly confidential or proprietary data with others who own related databases is a serious impediment to conducting a mutually beneficial data mining analysis. We address the case of vertically partitioned data -- multiple data owners/agencies each possess a few attributes of every data record. We focus on the case of the agencies wanting to conduct a linear regression analysis with complete records without disclosing values of their own attributes. This paper describes an algorithm that enables such agencies to compute the exact regression coefficients of the global regression equation and also perform some basic goodness-of-fit diagnostics while protecting the confidentiality of their data. In more general settings beyond the privacy scenario, this algorithm can also be viewed as method for the distributed computation for regression analyses.
KDD	Dense itemsets.	Jouni K. Seppänen,Heikki Mannila	2004	Frequent itemset mining has been the subject of a lot of work in data mining research ever since association rules were introduced. In this paper we address a problem with frequent itemsets: that they only count rows where all their attributes are present, and do not allow for any noise. We show that generalizing the concept of frequency while preserving the performance of mining algorithms is nontrivial, and introduce a generalization of frequent itemsets, dense itemsets. Dense itemsets do not require all attributes to be present at the same time; instead, the itemset needs to define a sufficiently large submatrix that exceeds a given density threshold of attributes present.We consider the problem of computing all dense itemsets in a database. We give a levelwise algorithm for this problem, and also study the top-$k$ variations, i.e., finding the k densest sets with a given support, or the k best-supported sets with a given density. These algorithms select the other parameter automatically, which simplifies mining dense itemsets in an explorative way. We show that the concept captures natural facets of data sets, and give extensive empirical results on the performance of the algorithms. Combining the concept of dense itemsets with set cover ideas, we also show that dense itemsets can be used to obtain succinct descriptions of large datasets. We also discuss some variations of dense itemsets.
KDD	Selection, combination, and evaluation of effective software sensors for detecting abnormal computer usage.	Jude W. Shavlik,Mark Shavlik	2004	"We present and empirically analyze a machine-learning approach for detecting intrusions on individual computers. Our Winnow-based algorithm continually monitors user and system behavior, recording such properties as the number of bytes transferred over the last 10 seconds, the programs that currently are running, and the load on the CPU. In all, hundreds of measurements are made and analyzed each second. Using this data, our algorithm creates a model that represents each particular computer's range of normal behavior. Parameters that determine when an alarm should be raised, due to abnormal activity, are set on a per-computer basis, based on an analysis of training data. A major issue in intrusion-detection systems is the need for very low false-alarm rates. Our empirical results suggest that it is possible to obtain high intrusion-detection rates (95%) and low false-alarm rates (less than one per day per computer), without ""stealing"" too many CPU cycles (less than 1%). We also report which system measurements are the most valuable in terms of detecting intrusions. A surprisingly large number of different measurements prove significantly useful."
KDD	A Bayesian network framework for reject inference.	Andrew T. Smith,Charles Elkan	2004	Most learning methods assume that the training set is drawn randomly from the population to which the learned model is to be applied. However in many applications this assumption is invalid. For example, lending institutions create models of who is likely to repay a loan from training sets consisting of people in their records to whom loans were given in the past; however, the institution approved loan applications previously based on who was thought unlikely to default. Learning from only approved loans yields an incorrect model because the training set is a biased sample of the general population of applicants. The issue of including rejected samples in the learning process, or alternatively using rejected samples to adjust a model learned from accepted samples only, is called reject inference.The main contribution of this paper is a systematic analysis of different cases that arise in reject inference, with explanations of which cases arise in various real-world situations. We use Bayesian networks to formalize each case as a set of conditional independence relationships and identify eight cases, including the familiar missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) cases. For each case we present an overview of available learning algorithms. These algorithms have been published in separate fields of research, including epidemiology, econometrics, clinical trial evaluation, sociology, and credit scoring; our second major contribution is to describe these algorithms in a common framework.
KDD	Support envelopes: a technique for exploring the structure of association patterns.	Michael Steinbach,Pang-Ning Tan,Vipin Kumar	2004	This paper introduces support envelopes---a new tool for analyzing association patterns---and illustrates some of their properties, applications, and possible extensions. Specifically, the support envelope for a transaction data set and a specified pair of positive integers (m,n) consists of the items and transactions that need to be searched to find any association pattern involving m or more transactions and n or more items. For any transaction data set with M transactions and N items, there is a unique lattice of at most M*N support envelopes that captures the structure of the association patterns in that data set. Because support envelopes are not encumbered by a support threshold, this support lattice provides a complete view of the association structure of the data set, including association patterns that have low support. Furthermore, the boundary of the support lattice---the support boundary---has at most min(M,N) envelopes and is especially interesting since it bounds the maximum sizes of potential association patterns---not only for frequent, closed, and maximal itemsets, but also for patterns, such as error-tolerant itemsets, that are more general. The association structure can be represented graphically as a two-dimensional scatter plot of the (m,n) values associated with the support envelopes of the data set, a feature that is useful in the exploratory analysis of association patterns. Finally, the algorithm to compute support envelopes is simple and computationally efficient, and it is straightforward to parallelize the process of finding all the support envelopes.
KDD	Generalizing the notion of support.	Michael Steinbach,Pang-Ning Tan,Hui Xiong,Vipin Kumar	2004	The goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis to non-traditional types of patterns and non-binary data. To that end, we describe a framework for generalizing support that is based on the simple, but useful observation that support can be viewed as the composition of two functions: a function that evaluates the strength or presence of a pattern in each object (transaction) and a function that summarizes these evaluations with a single number. A key goal of any framework is to allow people to more easily express, explore, and communicate ideas, and hence, we illustrate how our support framework can be used to describe support for a variety of commonly used association patterns, such as frequent itemsets, general Boolean patterns, and error-tolerant itemsets. We also present two examples of the practical usefulness of generalized support. One example shows the usefulness of support functions for continuous data. Another example shows how the hyperclique pattern---an association pattern originally defined for binary data---can be extended to continuous data by generalizing a support function.
KDD	Probabilistic author-topic models for information discovery.	Mark Steyvers,Padhraic Smyth,Michal Rosen-Zvi,Thomas L. Griffiths	2004	"We propose a new unsupervised learning technique for extracting information from large text collections. We model documents as if they were generated by a two-stage stochastic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer."
KDD	Ordering patterns by combining opinions from multiple sources.	Pang-Ning Tan,Rong Jin	2004	Pattern ordering is an important task in data mining because the number of patterns extracted by standard data mining algorithms often exceeds our capacity to manually analyze them. In this paper, we present an effective approach to address the pattern ordering problem by combining the rank information gathered from disparate sources. Although rank aggregation techniques have been developed for applications such as meta-search engines, they are not directly applicable to pattern ordering for two reasons. First, the techniques are mostly supervised, i.e., they require a sufficient amount of labeled data. Second, the objects to be ranked are assumed to be independent and identically distributed (i.i.d), an assumption that seldom holds in pattern ordering. The method proposed in this paper is an adaptation of the original Hedge algorithm, modified to work in an unsupervised learning setting. Techniques for addressing the i.i.d. violation in pattern ordering are also presented. Experimental results demonstrate that our unsupervised Hedge algorithm outperforms many alternative techniques such as those based on weighted average ranking and singular value decomposition.
KDD	A generative probabilistic approach to visualizing sets of symbolic sequences.	Peter Tiño,Ata Kabán,Yi Sun	2004	There is a notable interest in extending probabilistic generative modeling principles to accommodate for more complex structured data types. In this paper we develop a generative probabilistic model for visualizing sets of discrete symbolic sequences. The model, a constrained mixture of discrete hidden Markov models, is a generalization of density-based visualization methods previously developed for static data sets. We illustrate our approach on sequences representing web-log data and chorals by J.S. Bach.
KDD	Learning a complex metabolomic dataset using random forests and support vector machines.	Young Truong,Xiaodong Lin,Chris Beecher	2004	"Metabolomics is the ""omics"" science of biochemistry. The associated data include the quantitative measurements of all small molecule metabolites in a biological sample. These datasets provide a window into dynamic biochemical networks and conjointly with other ""omic"" data, genes and proteins, have great potential to unravel complex human diseases. The dataset used in this study has 63 individuals, normal and diseased, and the diseased are drug treated or not, so there are three classes. The goal is to classify these individuals using the observed metabolite levels for 317 measured metabolites. There are a number of statistical challenges: non-normal data, the number of samples is less than the number of metabolites; there are missing data and the fact that data are missing is informative (assay values below detection limits can point to a specific class); also, there are high correlations among the metabolites. We investigate support vector machines (SVM), and random forest (RF), for outlier detection, variable selection and classification. We use the variables selected with RF in SVM and visa versa. The benefit of this study is insight into interplay of variable selection and classification methods. We link our selected predictors to the biochemistry of the disease."
KDD	Rotation invariant distance measures for trajectories.	Michail Vlachos,Dimitrios Gunopulos,Gautam Das	2004	For the discovery of similar patterns in 1D time-series, it is very typical to perform a normalization of the data (for example a transformation so that the data follow a zero mean and unit standard deviation). Such transformations can reveal latent patterns and are very commonly used in datamining applications. However, when dealing with multidimensional time-series, which appear naturally in applications such as video-tracking, motion-capture etc, similar motion patterns can also be expressed at different orientations. It is therefore imperative to provide support for additional transformations, such as rotation. In this work, we transform the positional information of moving data, into a space that is translation, scale and rotation invariant. Our distance measure in the new space is able to detect elastic matches and can be efficiently lower bounded, thus being computationally tractable. The proposed methods are easy to implement, fast to compute and can have many applications for real world problems, in areas such as handwriting recognition and posture estimation in motion-capture data. Finally, we empirically demonstrate the accuracy and the efficiency of the technique, using real and synthetic handwriting data.
KDD	1-dimensional splines as building blocks for improving accuracy of risk outcomes models.	David S. Vogel,Morgan C. Wang	2004	Transformation of both the response variable and the predictors is commonly used in fitting regression models. However, these transformation methods do not always provide the maximum linear correlation between the response variable and the predictors, especially when there are non-linear relationships between predictors and the response such as the medical data set used in this study. A spline based transformation method is proposed that is second order smooth, continuous, and minimizes the mean squared error between the response and each predictor. Since the computation time for generating this spline is O(n), the processing time is reasonable with massive data sets. In contrast to cubic smoothing splines, the resulting transformation equations also display a high level of efficiency for scoring. Data used for predicting health outcomes contains an abundance of non-linear relationships between predictors and the outcomes requiring an algorithm for modeling them accurately. Thus, a transformation that fits an adaptive cubic spline to each of a set of variables is proposed. These curves are used as a set of transformation functions on the predictors. A case study of how the transformed variables can be fed into a simple linear regression model to predict risk outcomes is presented. The results show significant improvement over the performance of the original variables in both linear and non-linear models.
KDD	Scalable mining of large disk-based graph databases.	Chen Wang,Wei Wang,Jian Pei,Yongtai Zhu,Baile Shi	2004	Mining frequent structural patterns from graph databases is an interesting problem with broad applications. Most of the previous studies focus on pruning unfruitful search subspaces effectively, but few of them address the mining on large, disk-based databases. As many graph databases in applications cannot be held into main memory, scalable mining of large, disk-based graph databases remains a challenging problem. In this paper, we develop an effective index structure, ADI (for <u>ad</u>jacency <u>i</u>ndex), to support mining various graph patterns over large databases that cannot be held into main memory. The index is simple and efficient to build. Moreover, the new index structure can be easily adopted in various existing graph pattern mining algorithms. As an example, we adapt the well-known gSpan algorithm by using the ADI structure. The experimental results show that the new index structure enables the scalable graph pattern mining over large databases. In one set of the experiments, the new disk-based method can mine graph databases with one million graphs, while the original gSpan algorithm can only handle databases of up to 300 thousand graphs. Moreover, our new method is faster than gSpan when both can run in main memory.
KDD	Privacy-preserving Bayesian network structure computation on distributed heterogeneous data.	Rebecca N. Wright,Zhiqiang Yang	2004	"As more and more activities are carried out using computers and computer networks, the amount of potentially sensitive data stored by business, governments, and other parties increases. Different parties may wish to benefit from cooperative use of their data, but privacy regulations and other privacy concerns may prevent the parties from sharing their data. Privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data is not revealed.In this paper, we present a privacy-preserving protocol for a particular data mining task: learning the Bayesian network structure for distributed heterogeneous data. In this setting, two parties owning confidential databases wish to learn the structure of Bayesian network on the combination of their databases without revealing anything about their data to each other. We give an efficient and privacy-preserving version of the K2 algorithm to construct the structure of a Bayesian network for the parties' joint data."
KDD	Mining scale-free networks using geodesic clustering.	Andrew Y. Wu,Michael Garland,Jiawei Han	2004	Many real-world graphs have been shown to be scale-free---vertex degrees follow power law distributions, vertices tend to cluster, and the average length of all shortest paths is small. We present a new model for understanding scale-free networks based on multilevel geodesic approximation, using a new data structure called a multilevel mesh.Using this multilevel framework, we propose a new kind of graph clustering for data reduction of very large graph systems such as social, biological, or electronic networks. Finally, we apply our algorithms to real-world social networks and protein interaction graphs to show that they can reveal knowledge embedded in underlying graph structures. We also demonstrate how our data structures can be used to quickly answer approximate distance and shortest path queries on scale-free networks.
KDD	A cross-collection mixture model for comparative text mining.	ChengXiang Zhai,Atulya Velivelli,Bei Yu	2004	In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme. This general problem subsumes many interesting applications, including business intelligence and opinion summarization. We propose a generative probabilistic mixture model for comparative text mining. The model simultaneously performs cross-collection clustering and within-collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model.
KDD	"Exploiting a support-based upper bound of Pearson's correlation coefficient for efficiently identifying strongly correlated pairs."	Hui Xiong,Shashi Shekhar,Pang-Ning Tan,Vipin Kumar	2004	"Given a user-specified minimum correlation threshold θ and a market basket database with N items and T transactions, an all-strong-pairs correlation query finds all item pairs with correlations above the threshold θ. However, when the number of items and transactions are large, the computation cost of this query can be very high. In this paper, we identify an upper bound of Pearson's correlation coefficient for binary variables. This upper bound is not only much cheaper to compute than Pearson's correlation coefficient but also exhibits a special monotone property which allows pruning of many item pairs even without computing their upper bounds. A Two-step All-strong-Pairs corrElation que Ry (TAPER) algorithm is proposed to exploit these properties in a filter-and-refine manner. Furthermore, we provide an algebraic cost model which shows that the computation savings from pruning is independent or improves when the number of items is increased in data sets with common Zipf or linear rank-support distributions. Experimental results from synthetic and real data sets exhibit similar trends and show that the TAPER algorithm can be an order of magnitude faster than brute-force alternatives."
KDD	Fast mining of spatial collocations.	Xin Zhang,Nikos Mamoulis,David W. Cheung,Yutao Shou	2004	Spatial collocation patterns associate the co-existence of non-spatial features in a spatial neighborhood. An example of such a pattern can associate contaminated water reservoirs with certain deceases in their spatial neighborhood. Previous work on discovering collocation patterns converts neighborhoods of feature instances to itemsets and applies mining techniques for transactional data to discover the patterns. We propose a method that combines the discovery of spatial neighborhoods with the mining process. Our technique is an extension of a spatial join algorithm that operates on multiple inputs and counts long pattern instances. As demonstrated by experimentation, it yields significant performance improvements compared to previous approaches.
KDD	On the discovery of significant statistical quantitative rules.	Hong Zhang,Balaji Padmanabhan,Alexander Tuzhilin	2004	"In this paper we study market share rules, rules that have a certain market share statistic associated with them. Such rules are particularly relevant for decision making from a business perspective. Motivated by market share rules, in this paper we consider statistical quantitative rules (SQ rules) that are quantitative rules in which the RHS can be any statistic that is computed for the segment satisfying the LHS of the rule. Building on prior work, we present a statistical approach for learning all significant SQ rules, i.e., SQ rules for which a desired statistic lies outside a confidence interval computed for this rule. In particular we show how resampling techniques can be effectively used to learn significant rules. Since our method considers the significance of a large number of rules in parallel, it is susceptible to learning a certain number of ""false"" rules. To address this, we present a technique that can determine the number of significant SQ rules that can be expected by chance alone, and suggest that this number can be used to determine a ""false discovery rate"" for the learning procedure. We apply our methods to online consumer purchase data and report the results."
KDD	Predicting prostate cancer recurrence via maximizing the concordance index.	Lian Yan,David Verbel,Olivier Saidi	2004	In order to effectively use machine learning algorithms, e.g., neural networks, for the analysis of survival data, the correct treatment of censored data is crucial. The concordance index (CI) is a typical metric for quantifying the predictive ability of a survival model. We propose a new algorithm that directly uses the CI as the objective function to train a model, which predicts whether an event will eventually occur or not. Directly optimizing the CI allows the model to make complete use of the information from both censored and non-censored observations. In particular, we approximate the CI via a differentiable function so that gradient-based methods can be used to train the model. We applied the new algorithm to predict the eventual recurrence of prostate cancer following radical prostatectomy. Compared with the traditional Cox proportional hazards model and several other algorithms based on neural networks and support vector machines, our algorithm achieves a significant improvement in being able to identify high-risk and low-risk groups of patients.
KDD	IMMC: incremental maximum margin criterion.	Jun Yan,Benyu Zhang,Shuicheng Yan,Qiang Yang,Hua Li,Zheng Chen,Wensi Xi,Weiguo Fan,Wei-Ying Ma,QianSheng Cheng	2004	Subspace learning approaches have attracted much attention in academia recently. However, the classical batch algorithms no longer satisfy the applications on streaming data or large-scale data. To meet this desirability, Incremental Principal Component Analysis (IPCA) algorithm has been well established, but it is an unsupervised subspace learning approach and is not optimal for general classification tasks, such as face recognition and Web document categorization. In this paper, we propose an incremental supervised subspace learning algorithm, called Incremental Maximum Margin Criterion (IMMC), to infer an adaptive subspace by optimizing the Maximum Margin Criterion. We also present the proof for convergence of the proposed algorithm. Experimental results on both synthetic dataset and real world datasets show that IMMC converges to the similar subspace as that of batch approach.
KDD	A data mining approach to modeling relationships among categories in image collection.	Ruofei Zhang,Zhongfei (Mark) Zhang,Sandeep Khanzode	2004	This paper proposes a data mining approach to modeling relationships among categories in image collection. In our approach, with image feature grouping, a visual dictionary is created for color, texture, and shape feature attributes respectively. Labeling each training image with the keywords in the visual dictionary, a classification tree is built. Based on the statistical properties of the feature space we define a structure, called α-Semantics Graph, to discover the hidden semantic relationships among the semantic categories embodied in the image collection. With the α-Semantics Graph, each semantic category is modeled as a unique fuzzy set to explicitly address the semantic uncertainty and semantic overlap among the categories in the feature space. The model is utilized in the semantics-intensive image retrieval application. An algorithm using the classification accuracy measures is developed to combine the built classification tree with the fuzzy set modeling method to deliver semantically relevant image retrieval for a given query image. The experimental evaluations have demonstrated that the proposed approach models the semantic relationships effectively and the image retrieval prototype system utilizing the derived model is promising both in effectiveness and efficiency.
KDD	The complexity of mining maximal frequent itemsets and maximal frequent patterns.	Guizhen Yang	2004	"Mining maximal frequent itemsets is one of the most fundamental problems in data mining. In this paper we study the complexity-theoretic aspects of maximal frequent itemset mining, from the perspective of counting the number of solutions. We present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transactions, given an arbitrary support threshold, is #P-complete, thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is NP-hard. This result is of particular interest since the associated decision problem of checking the existence of a maximal frequent itemset is in P.We also extend our complexity analysis to other similar data mining problems dealing with complex data structures, such as sequences, trees, and graphs, which have attracted intensive research interests in recent years. Normally, in these problems a partial order among frequent patterns can be defined in such a way as to preserve the downward closure property, with maximal frequent patterns being those without any successor with respect to this partial order. We investigate several variants of these mining problems in which the patterns of interest are subsequences, subtrees, or subgraphs, and show that the associated problems of counting the number of maximal frequent patterns are all either #P-complete or #P-hard."
KDD	V-Miner: using enhanced parallel coordinates to mine product design and test data.	Kaidi Zhao,Bing Liu,Thomas M. Tirpak,Andreas Schaller	2004	Analyzing data to find trends, correlations, and stable patterns is an important task in many industrial applications. This paper proposes a new technique based on parallel coordinate visualization. Previous work on parallel coordinate methods has shown that they are effective only when variables that are correlated and/or show similar patterns are displayed adjacently. Although current parallel coordinate tools allow the user to manually rearrange the order of variables, this process is very time-consuming when the number of variables is large. Automated assistance is required. This paper introduces an edit-distance based technique to rearrange variables so that interesting change patterns can be easily detected visually. The Visual Miner (V-Miner) software includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined or displayed by the system. In addition, the system can filter data according to rules sets imported from other data mining tools. This feature was found very helpful in practice, because it enables decision makers to visually identify interesting rules and data segments for further analysis or data mining. This paper begins with an introduction to the proposed techniques and the V-Miner system. Next, a case study illustrates how V-Miner has been used at Motorola to guide product design and test decisions.
KDD	2PXMiner: an efficient two pass mining of frequent XML query patterns.	Liang Huai Yang,Mong-Li Lee,Wynne Hsu,Xinyu Guo	2004	Caching the results of frequent query patterns can improve the performance of query evaluation. This paper describes a 2-pass mining algorithm called 2PXMiner to discover frequent XML query patterns. We design 3 data structures to expedite the mining process. Experiments results indicate that 2PXMiner is both efficient and scalable.
KDD	A DEA approach for model combination.	Zhiqiang Zheng,Balaji Padmanabhan,Haoqiang Zheng	2004	This paper proposes a novel Data Envelopment Analysis (DEA) based approach for model combination. We first prove that for the 2-class classification problems DEA models identify the same convex hull as the popular ROC analysis used for model combination. For general k-class classifiers, we then develop a DEA-based method to combine multiple classifiers. Experiments show that the method outperforms other benchmark methods and suggest that DEA can be a promising tool for model combination.
KDD	Optimal randomization for privacy preserving data mining.	Michael Yu Zhu,Lei Liu	2004	Optimal randomization for privacy preserving data mining.
KDD	GPCA: an efficient dimension reduction scheme for image compression and retrieval.	Jieping Ye,Ravi Janardan,Qi Li	2004	Recent years have witnessed a dramatic increase in the quantity of image data collected, due to advances in fields such as medical imaging, reconnaissance, surveillance, astronomy, multimedia etc. With this increase has come the need to be able to store, transmit, and query large volumes of image data efficiently. A common operation on image databases is the retrieval of all images that are similar to a query image. For this, the images in the database are often represented as vectors in a high-dimensional space and a query is answered by retrieving all image vectors that are proximal to the query image in this space, under a suitable similarity metric. To overcome problems associated with high dimensionality, such as high storage and retrieval times, a dimension reduction step is usually applied to the vectors to concentrate relevant information in a small number of dimensions. Principal Component Analysis (PCA) is a well-known dimension reduction scheme. However, since it works with vectorized representations of images, PCA does not take into account the spatial locality of pixels in images. In this paper, a new dimension reduction scheme, called Generalized Principal Component Analysis (GPCA), is presented. This scheme works directly with images in their native state, as two-dimensional matrices, by projecting the images to a vector space that is the tensor product of two lower-dimensional vector spaces. Experiments on databases of face images show that, for the same amount of storage, GPCA is superior to PCA in terms of quality of the compressed images, query precision, and computational cost.
KDD	IDR/QR: an incremental dimension reduction algorithm via QR decomposition.	Jieping Ye,Qi Li,Hui Xiong,Haesun Park,Ravi Janardan,Vipin Kumar	2004	Dimension reduction is a critical data preprocessing step for many database and data mining applications, such as efficient storage and retrieval of high-dimensional data. In the literature, a well-known dimension reduction algorithm is Linear Discriminant Analysis (LDA). The common aspect of previously proposed LDA-based algorithms is the use of Singular Value Decomposition (SVD). Due to the difficulty of designing an incremental solution for the eigenvalue problem on the product of scatter matrices in LDA, there has been little work on designing incremental LDA algorithms that can efficiently incorporate new data items as they become available. In this paper, we propose an LDA-based incremental dimension reduction algorithm, called IDR/QR, which applies QR Decomposition rather than SVD. Unlike other LDA-based algorithms, this algorithm does not require the whole data matrix in main memory. This is desirable for large data sets. More importantly, with the insertion of new data items, the IDR/QR algorithm can constrain the computational cost by applying efficient QR-updating techniques. Finally, we evaluate the effectiveness of the IDR/QR algorithm in terms of classification error rate on the reduced dimensional space. Our experiments on several real-world data sets reveal that the classification error rate achieved by the IDR/QR algorithm is very close to the best possible one achieved by other LDA-based algorithms. However, the IDR/QR algorithm has much less computational cost, especially when new data items are inserted dynamically.
KDD	Analytical view of business data.	Adam Yeh,Jonathan Tang,Youxuan Jin,Sam Skrivan	2004	"This paper describes a logical extension to Microsoft Business Framework (MBF) called Analytical View (AV). AV consists of three components: Model Service for design time, Business Intelligence Entity (BIE) for programming model, and IntellDrill for runtime navigation between OLTP and OLAP data sources. AV feature-set fulfills enterprise application requirements for Analysis and Decision Support, complementing the transactional feature-set currently provided by MBF. Model Service automatically transforms an ""object oriented model (transactional view)"" to a ""multi-dimensional model (analytical view)"" without the traditional Extraction/Transformation/Loading (ETL) overhead and complexity. It infers dimensionality from the object layer where richer metadata is stored, eliminating the ""guesswork"" that a traditional data warehousing process requires when going through physical database schema. BI Entities are classes code-generated by Model Service. As an intrinsic part of the framework, BI Entities enable a consistent object oriented way of programming model with strong types and rich semantics for OLAP, similar to what MBF object persistence technology does for OLTP data. More importantly, data contained in BI Entities have a higher degree of ""application awareness,"" such as the integrated application level security and customizability. IntelliDrill links together all the information islands in MBF using metadata. Because of the automatic transformation from transactional view to analytical view enabled by Model Service, we have the ability to understand natively what kind of drill-ability an object would have, thus making information navigation in MBF fully discover-able with built-in ontology."
KDD	Density-based spam detector.	Kenichi Yoshida,Fuminori Adachi,Takashi Washio,Hiroshi Motoda,Teruaki Homma,Akihiro Nakashima,Hiromitsu Fujikawa,Katsuyuki Yamazaki	2004	The volume of mass unsolicited electronic mail, often known as spam, has recently increased enormously and has become a serious threat to not only the Internet but also to society. This paper proposes a new spam detection method which uses document space density information. Although it requires extensive e-mail traffic to acquire the necessary information, an unsupervised learning engine with a short white list can achieve a 98% recall rate and 100% precision. A direct-mapped cache method contributes handling of over 13,000 e-mails per second. Experimental results, which were conducted using over 50 million actual e-mails of traffic, are also reported in this paper.
KDD	Redundancy based feature selection for microarray data.	Lei Yu,Huan Liu	2004	In gene expression microarray data analysis, selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes. The problem becomes particularly challenging due to the large number of features (genes) and small sample size. Traditional gene selection methods often select the top-ranked genes according to their individual discriminative power without handling the high degree of redundancy among the genes. Latest research shows that removing redundant genes among selected ones can achieve a better representation of the characteristics of the targeted phenotypes and lead to improved classification accuracy. Hence, we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant genes. The efficiency and effectiveness of our method in comparison with representative methods has been demonstrated through an empirical study using public microarray data sets.
KDD	Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Seattle, Washington, USA, August 22-25, 2004	Won Kim,Ron Kohavi,Johannes Gehrke,William DuMouchel	2004	Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Seattle, Washington, USA, August 22-25, 2004
PKDD	Real-World Learning with Markov Logic Networks.	Pedro Domingos	2004	Machine learning and data mining systems have achieved many impressive successes, but to become truly widespread they must be able to work with less help from people. This requires automating the data cleaning and integration process, handling multiple types of objects and relations at once, and easily incorporating domain knowledge. In this talk, I describe how we are pursuing these aims using Markov logic networks, a representation that combines first-order logic and probabilistic graphical models. Data from multiple sources is integrated by automatically learning mappings between the objects and terms in them. Rich relational structure is learned using a combination of ILP and statistical techniques. Knowledge is incorporated by viewing logic statements as soft constraints on the models to be learned. Application to a real-world university domain shows our approach to be accurate, efficient, and less labor-intensive than traditional ones.
PKDD	Reducing Data Stream Sliding Windows by Cyclic Tree-Like Histograms.	Francesco Buccafurri,Gianluca Lax	2004	Data reduction is a basic step in a KDD process useful for delivering to successive stages more concise and meaningful data. When mining is applied to data streams, that are continuous data flows, the issue of suitably reducing them is highly interesting, in order to arrange effective approaches requiring multiple scans on data, that, in such a way, may be performed over one or more reduced sliding windows. A class of queries, whose importance in the context of KDD is widely accepted, corresponds to sum range queries. In this paper we propose a histogram-based technique for reducing sliding windows supporting approximate arbitrary (i.e., non biased) sum range queries. The histogram, based on a hierarchical structure (opposed to the flat structure of traditional ones), results suitable for directly supporting hierarchical queries, and, thus, drill-down and roll-up operations. In addition, the structure well supports sliding window shifting and quick query answering (both these operations are loarithmic in the sliding window size). Experimental analysis shows the superiority of our method in terms of accuracy w.r.t. the state-of-the-art approaches in the context of histogram-based sliding window reduction techniques.
PKDD	Random Matrices in Data Analysis.	Dimitris Achlioptas	2004	We show how carefully crafted random matrices can achieve distance-preserving dimensionality reduction, accelerate spectral computations, and reduce the sample complexity of certain kernel methods.
PKDD	Data Privacy.	Rakesh Agrawal	2004	There is increasing need to build information systems that protect the privacy and ownership of data without impeding the flow of information. We will present some of our current work to demonstrate the technical feasibility of building such systems.
PKDD	Shape and Size Regularization in Expectation Maximization and Fuzzy Clustering.	Christian Borgelt,Rudolf Kruse	2004	The more sophisticated fuzzy clustering algorithms, like the Gustafson-Kessel algorithm [11] and the fuzzy maximum likelihood estimation (FMLE) algorithm [10] offer the possibility of inducing clusters of ellipsoidal shape and different sizes. The same holds for the EM algorithm for a mixture of Gaussians. However, these additional degrees of freedom often reduce the robustness of the algorithm, thus sometimes rendering their application problematic. In this paper we suggest shape and size regularization methods that handle this problem effectively.
PKDD	Visual Mining of Spatial Time Series Data.	Gennady L. Andrienko,Natalia V. Andrienko,Peter Gatalsky	2004	CommonGIS is a system comprising a number of tools for visual data analysis. In this paper we demonstrate our recent developments for analysis of spatial time series data.
PKDD	Mining Positive and Negative Association Rules: An Approach for Confined Rules.	Maria-Luiza Antonie,Osmar R. Zaïane	2004	Typical association rules consider only items enumerated in transactions. Such rules are referred to as positive association rules. Negative association rules also consider the same items, but in addition consider negated items (i.e. absent from transactions). Negative association rules are useful in market-basket analysis to identify products that conflict with each other or products that complement each other. They are also very convenient for associative classifiers, classifiers that build their classification model based on association rules. Many other applications would benefit from negative association rules if it was not for the expensive process to discover them. Indeed, mining for such rules necessitates the examination of an exponentially large search space. Despite their usefulness, and while they were referred to in many publications, very few algorithms to mine them have been proposed to date. In this paper we propose an algorithm that extends the support-confidence framework with sliding correlation coefficient threshold. In addition to finding confident positive rules that have a strong correlation, the algorithm discovers negative association rules with strong negative correlation between the antecedents and consequents.
PKDD	Detecting Driving Awareness.	Bruno Apolloni,Andrea Brega,Dario Malchiodi,Cristian Mesiano	2004	We consider the task of monitoring the awareness state of a driver engaged in attention demanding manoeuvres. A Boolean normal form launches a flag when the driver is paying special attention to his guiding. The contrasting analysis of these flags with the physical parameters of the car may alert a decision system whenever the driver awareness is judged unsatisfactory. The paper presents preliminary results showing the feasibility of the task.
PKDD	A Framework for Data Mining Pattern Management.	Barbara Catania,Anna Maddalena,Maurizio Mazza,Elisa Bertino,Stefano Rizzi	2004	To represent and manage data mining patterns, several aspects have to be taken into account: (i) patterns are heterogeneous in nature; (ii) patterns can be extracted from raw data by using data mining tools (a-posteriori patterns) but also defined by the users and used for example to check how well they represent some input data source (a-priori patterns); (iii) since source data change frequently, issues concerning pattern validity and synchronization are very important; (iv) patterns have to be manipulated and queried according to specific languages. Several approaches have been proposed so far to deal with patterns, however all of them lack some of the previous characteristics. The aim of this paper is to present an overall framework to cope with all these features.
PKDD	Spatial Associative Classification at Different Levels of Granularity: A Probabilistic Approach.	Michelangelo Ceci,Annalisa Appice,Donato Malerba	2004	In this paper we propose a novel spatial associative classifier method based on a multi-relational approach that takes spatial relations into account. Classification is driven by spatial association rules discovered at multiple granularity levels. Classification is probabilistic and is based on an extension of naive Bayes classifiers to multi- relational data. The method is implemented in a Data Mining system tightly integrated with an object relational spatial database. It performs the classification at different granularity levels and takes advantage from domain specific knowledge in form of rules that support qualitative spatial reasoning. An application to real-world spatial data is reported. Results show that the use of different levels of granularity is beneficial.
PKDD	An Effective Recommender System for Highly Dynamic and Large Web Sites.	Ranieri Baraglia,Francesco Merlo,Fabrizio Silvestri	2004	In this demo we show a recommender system, called SUGGEST, that dynamically generates links to pages that have not yet been visited by a user and might be of his potential interest. Usually other recommender systems exploit a kind of two-phase architecture composed by an off-line component that analyzes Web server access logs and generates information used by a successive online component that generates recommendations. SUGGEST collapse the two-phase into a single online Apache module. The component is able to manage very large Web sites made up of dinamically generated pages by means of an efficient LRU-based database management strategy. The demo will show the way SUGGEST is able to anticipate users requests that will be made farther in the future, introducing a limited overhead on the Web server activity.
PKDD	Breaking Through the Syntax Barrier: Searching with Entities and Relations.	Soumen Chakrabarti	2004	The next wave in search technology will be driven by the identification, extraction, and exploitation of real-world entities represented in unstructured textual sources. Search systems will either let users express information needs naturally and analyze them more intelligently, or allow simple enhancements that add more user control on the search process. The data model will exploit graph structure where available, but not impose structure by fiat. First generation Web search, which uses graph information at the macroscopic level of inter-page hyperlinks, will be enhanced to use fine-grained graph models involving page regions, tables, sentences, phrases, and real-world-entities. New algorithms will combine probabilistic evidence from diverse features to produce responses that are not URLs or pages, but entities and their relationships, or explanations of how multiple entities are related.
PKDD	AutoPart: Parameter-Free Graph Partitioning and Outlier Detection.	Deepayan Chakrabarti	2004	Graphs arise in numerous applications, such as the analysis of the Web, router networks, social networks, co-citation graphs, etc. Virtually all the popular methods for analyzing such graphs, for example, k-means clustering, METIS graph partitioning and SVD/PCA, require the user to specify various parameters such as the number of clusters, number of partitions and number of principal components. We propose a novel way to group nodes, using information-theoretic principles to choose both the number of such groups and the mapping from nodes to groups. Our algorithm is completely parameter-free, and also scales practically linearly with the problem size. Further, we propose novel algorithms which use this node group structure to get further insights into the data, by finding outliers and computing distances between groups. Finally, we present experiments on multiple synthetic and real-life datasets, where our methods give excellent, intuitive results.
PKDD	A Unified and Flexible Framework for Comparing Simple and Complex Patterns.	Ilaria Bartolini,Paolo Ciaccia,Irene Ntoutsi,Marco Patella,Yannis Theodoridis	2004	One of the most important operations involving Data Mining patterns is computing their similarity. In this paper we present a general framework for comparing both simple and complex patterns, i.e., patterns built up from other patterns. Major features of our framework include the notion of structure and measure similarity, the possibility of managing multiple coupling types and aggregation logics, and the recursive definition of similarity for complex patterns.
PKDD	Constructing (Almost) Phylogenetic Trees from Developmental Sequences Data.	Ronnie Bathoorn,Arno Siebes	2004	In this paper we present a new way of constructing almost phylogenetic trees. Almost since we reconstruct the tree, but without the timestamps. Rather than basing the tree on genetic sequence data ours is based on developmental sequence data. Using frequent episode discovery and clustering we reconstruct the consensus tree from the literature almost completely.
PKDD	An Experiment on Knowledge Discovery in Chemical Databases.	Sandra Berasaluce,Claude Laurenço,Amedeo Napoli,Gilles Niel	2004	In this paper, we present an experiment on knowledge discovery in chemical reaction databases. Chemical reactions are the main elements on which relies synthesis in organic chemistry, and this is why chemical reactions databases are of first importance. From a problem-solving process point of view, synthesis in organic chemistry must be considered at several levels of abstraction: mainly a strategic level where general synthesis methods are involved, and a tactic level where actual chemical reactions are applied. The research work presented in this paper is aimed at discovering general synthesis methods from chemical reaction databases in order to design generic and reusable synthesis plans. The knowledge discovery process relies on frequent levelwise itemset search and association rule extraction, but also on chemical knowledge involved within every step of the knowledge discovery process. Moreover, the overall process is supervised by an expert of the domain. The principles of this original periment on mining chemical reaction databases and its results are detailed and discussed.
PKDD	SemanticTalk: Software for Visualizing Brainstorming Sessions and Thematic Concept Trails on Document Collections.	Christian Biemann,Karsten Böhm,Gerhard Heyer,Ronny Melz	2004	"In this demonstration we introduce a technology to support knowledge structuring processes already at the time of their creation by building up concept structures in real time. Our focus was set on the design of a minimal invasive system, which ideally requires no human interaction and thus gives the maximum freedom to the participants of a knowledge creation or exchange processes. The system captures and displays spoken dialogs as well as text documents for further use in knowledge engineer's tools."
PKDD	Properties and Benefits of Calibrated Classifiers.	Ira Cohen,Moisés Goldszmidt	2004	A calibrated classifier provides reliable estimates of the true probability that each test sample is a member of the class of interest. This is crucial in decision making tasks. Procedures for calibration have already been studied in weather forecasting, game theory, and more recently in machine learning, with the latter showing empirically that calibration of classifiers helps not only in decision making, but also improves classification accuracy. In this paper we extend the theoretical foundation of these empirical observations. We prove that (1) a well calibrated classifier provides bounds on the Bayes error (2) calibrating a classifier is guaranteed not to decrease classification accuracy, and (3) the procedure of calibration provides the threshold or thresholds on the decision rule that minimize the classification error. We also draw the parallels and differences between methods that use receiver operating characteristic (ROC) curves and calibration based procedures that are aimed at findig a threshold of minimum error. In particular, calibration leads to improved performance when multiple thresholds exist.
PKDD	A Tree-Based Approach to Clustering XML Documents by Structure.	Gianni Costa,Giuseppe Manco,Riccardo Ortale,Andrea Tagarelli	2004	We propose a novel methodology for clustering XML documents on the basis of their structural similarities. The idea is to equip each cluster with an XML cluster representative, i.e. an XML document subsuming the most typical structural specifics of a set of XML documents. Clustering is essentially accomplished by comparing cluster representatives, and updating the representatives as soon as new clusters are detected. We present an algorithm for the computation of an XML representative based on suitable techniques for identifying significant node matchings and for reliably merging and pruning XML trees. Experimental evaluation performed on both synthetic and real data shows the effectiveness of our approach.
PKDD	Combining Multiple Clustering Systems.	Constantinos Boulis,Mari Ostendorf	2004	Three methods for combining multiple clustering systems are presented and evaluated, focusing on the problem of finding the correspondence between clusters of different systems. In this work, the clusters of individual systems are represented in a common space and their correspondence estimated by either clustering clusters or with Singular Value Decomposition. The approaches are evaluated for the task of topic discovery on three major corpora and eight different clustering algorithms and it is shown experimentally that combination schemes almost always offer gains compared to single systems, but gains from using a combination scheme depend on the underlying clustering systems.
PKDD	Orange: From Experimental Machine Learning to Interactive Data Mining.	Janez Demsar,Blaz Zupan,Gregor Leban,Tomaz Curk	2004	Orange (www.ailab.si/orange) is a suite for machine learning and data mining. For researchers in machine learning, Orange offers scripting to easily prototype new algorithms and experimental procedures. For explorative data analysis, it provides a visual programming framework with emphasis on interactions and creative combinations of visual components.
PKDD	Discovery of Regulatory Connections in Microarray Data.	Michael Egmont-Petersen,Wim de Jonge,Arno Siebes	2004	In this paper, we introduce a new approach for mining regulatory interactions between genes in microarray time series studies. A number of preprocessing steps transform the original continuous measurements into a discrete representation that captures salient regulatory events in the time series. The discrete representation is used to discover interactions between the genes. In particular, we introduce a new across-model sampling scheme for performing Markov Chain Monte Carlo sampling of probabilistic network classifiers. The results obtained from the microarray data are promising. Our approach can detect interactions caused both by co-regulation and by control-regulation.
PKDD	Terrorist Detection System.	Yuval Elovici,Abraham Kandel,Mark Last,Bracha Shapira,Omer Zaafrany,Moti Schneider,Menahem Friedman	2004	"Terrorist Detection System (TDS) is aimed at detecting suspicious users on the Internet by the content of information they access. TDS consists of two main modules: a training module activated in batch mode, and an on-line detection module. The training module is provided with web pages that include terror related content and learns the typical interests of terrorists by applying data mining algorithms to the training data. The detection module performs real-time monitoring on users traffic and analyzes the content of the pages they access. An alarm is issued upon detection of a user whose content of accessed pages is ""too"" similar to typical terrorist content. TDS feasibility was tested in a network environment. Its detection rate was better than the rate of a state of the art Intrusion Detection System based on anomaly detection."
PKDD	The Anatomy of SnakeT: A Hierarchical Clustering Engine for Web-Page Snippets.	Paolo Ferragina,Antonio Gulli	2004	The purpose of a search engine is to retrieve from a given textual collection the documents deemed relevant for a user query. Typically a user query is modeled as a set of keywords, and a document is a Web page, a pdf file or whichever file can be parsed into a set of tokens (words). Documents are ranked in a flat list according to some measure of relevance to the user query. That list contains hyperlinks to the relevant documents, their titles, and also the so called (page or web) snippets, namely document excerpts allowing the user to understand if a document is indeed relevant without accessing it.
PKDD	Experimenting SnakeT: A Hierarchical Clustering Engine for Web-Page Snippets.	Paolo Ferragina,Antonio Gulli	2004	Current search engines return a ranked list of web pages represented by page excerpts called the web snippets. The ranking is computed according to some relevance criterium that takes into account textual and hyperlink information about the web pages (see e.g. [1]). This approach is very well-known and a lot of research is pushing towards the design of better and faster ranking criteria. However, it is nowadays equally known that a flat list of results limits the retrieval of precise answers because of many factors. First, the relevance of the query results is a subjective and time-varying concept that strictly depends on the context in which the user is formulating the query. Second, the ever growing web is enlarging the number and heterogeneity of candidate query answers. Third, the web users have limited patience so that they usually just look at the top ten results. The net outcome of this scenario is that the retrieval of the correct answer by a standard user is getting more and more diffcult, if not impossible.
PKDD	Finding Interesting Pass Patterns from Soccer Game Records.	Shoji Hirano,Shusaku Tsumoto	2004	"This paper presents a novel method for finding interesting pass patterns from soccer game records. Taking two features of the pass sequence ""temporal irregularity and requirements for multiscale observation"" into account, we have developed a comparison method of the sequences based on multiscale matching. The method can be used with hierarchical clustering, that brings us a new style of data mining in sports data. Experimental results on 64 game records of FIFA world cup 2002 demonstrated that the method could discover some interesting pass patterns that may be associated with successful goals."
PKDD	Learning from Little: Comparison of Classifiers Given Little Training.	George Forman,Ira Cohen	2004	Many real-world machine learning tasks are faced with the problem of small training sets. Additionally, the class distribution of the training set often does not match the target distribution. In this paper we compare the performance of many learning models on a substantial benchmark of binary text classification tasks having small training sets. We vary the training size and class distribution to examine the learning surface, as opposed to the traditional learning curve. The models tested include various feature selection methods each coupled with four learning algorithms: Support Vector Machines (SVM), Logistic Regression, Naive Bayes, and Multinomial Naive Bayes. Different models excel in different regions of the learning surface, leading to meta-knowledge about which to apply in different situations. This helps guide the researcher and practitioner when facing choices of model and feature selection methods in, for example, information retrieval settings and others.
PKDD	COCOA: Compressed Continuity Analysis for Temporal Databases.	Kuo-Yu Huang,Chia-Hui Chang,Kuo-Zui Lin	2004	A continuity is a kind of inter-transaction association which describes the relationships among different transactions. Since it breaks the boundaries of transactions, the number of potential itemsets and the number of rules will increase drastically. In this paper we consider the problem of discovering frequent compressed continuity patterns, which have the same power as mining the complete set of frequent continuity patterns. We devised a three-phase algorithm, COCOA, for frequent compressed continuity mining.
PKDD	Learning from Multi-source Data.	Élisa Fromont,Marie-Odile Cordier,Rene Quiniou	2004	This paper proposes an efficient method to learn from multi source data with an Inductive Logic Programming method. The method is based on two steps. The first one consists in learning rules independently from each source. In the second step the learned rules are used to bias a new learning process from the aggregated data. We validate this method on cardiac data obtained from electrocardiograms or arterial blood pressure measures. Our method is compared to a single step learning on aggregated data.
PKDD	Discovering Unexpected Information for Technology Watch.	François Jacquenet,Christine Largeron	2004	The purpose of technology watch is to gather, process and integrate the scientific and technical information that is useful to economic players. In this article, we propose to use text mining techniques to automate processing of data found in scientific text databases. The watch activity introduces an unusual difficulty compared with conventional areas of application for text mining techniques since, instead of searching for frequent knowledge hidden in the texts, the target is unexpected knowledge. As a result, the usual measures used for knowledge discovery have to be revised. For that purpose, we have developed the UnexpectedMiner system using new measures for to estimate the unexpectedness of a document. Our system is evaluated using a base that contains articles relating to the field of machine learning.
PKDD	Scalable Density-Based Distributed Clustering.	Eshref Januzaj,Hans-Peter Kriegel,Martin Pfeifle	2004	Clustering has become an increasingly important task in analysing huge amounts of data. Traditional applications require that all data has to be located at the site where it is scrutinized. Nowadays, large amounts of heterogeneous, complex data reside on different, independently working computers which are connected to each other via local or wide area networks. In this paper, we propose a scalable density-based distributed clustering algorithm which allows a user-defined trade-off between clustering quality and the number of transmitted objects from the different local sites to a global server site. Our approach consists of the following steps: First, we order all objects located at a local site according to a quality criterion reflecting their suitability to serve as local representatives. Then we send the best of these representatives to a server site where they are clustered with a slightly enhanced density-based clustering algorithm. This approach is very efficient, because the local detemination of suitable representatives can be carried out quickly and independently from each other. Furthermore, based on the scalable number of the most suitable local representatives, the global clustering can be done very effectively and efficiently. In our experimental evaluation, we will show that our new scalable density-based distributed clustering approach results in high quality clusterings with scalable transmission cost.
PKDD	Summarization of Dynamic Content in Web Collections.	Adam Jatowt,Mitsuru Ishizuka	2004	This paper describes a new research proposal of multi-document summarization of dynamic content in web pages. Much information is lost in the Web due to the temporal character of web documents. Therefore adapting summarization techniques to the web genre is a promising task. The aim of our research is to provide methods for summarizing volatile content retrieved from collections of topically related web pages over defined time periods. The resulting summary ideally would reflect the most popular topics and concepts found in retrospective web collections. Because of the content and time diversities of web changes, it is necessary to apply different techniques than standard methods used for static documents. In this paper we propose an initial solution to this summarization problem. Our approach exploits temporal similarities between web pages by utilizing sliding window concept over dynamic parts of the collection.
PKDD	Geometric and Combinatorial Tiles in 0-1 Data.	Aristides Gionis,Heikki Mannila,Jouni K. Seppänen	2004	In this paper we introduce a simple probabilistic model, hierarchical tiles, for 0-1 data. A basic tile (X,Y,p) specifies a subset X of the rows and a subset Y of the columns of the data, i.e., a rectangle, and gives a probability p for the occurrence of 1s in the cells of X × Y. A hierarchical tile has additionally a set of exception tiles that specify the probabilities for subrectangles of the original rectangle. If the rows and columns are ordered and X and Y consist of consecutive elements in those orderings, then the tile is geometric; otherwise it is combinatorial. We give a simple randomized algorithm for finding good geometric tiles. Our main result shows that using spectral ordering techniques one can find good orderings that turn combinatorial tiles into geometric tiles. We give empirical results on the performance of the methods.
PKDD	Document Classification Through Interactive Supervision of Document and Term Labels.	Shantanu Godbole,Abhay Harpale,Sunita Sarawagi,Soumen Chakrabarti	2004	Effective incorporation of human expertise, while exerting a low cognitive load, is a critical aspect of real-life text classification applications that is not adequately addressed by batch-supervised high-accuracy learners. Standard text classifiers are supervised in only one way: assigning labels to whole documents. They are thus deprived of the enormous wisdom that humans carry about the significance of words and phrases in context. We present HIClass, an interactive and exploratory labeling package that actively collects user opinion on feature representations and choices, as well as whole-document labels, while minimizing redundancy in the input sought. Preliminary experience suggests that, starting with essentially an unlabeled corpus, very little cognitive labor suffices to set up a labeled collection on which standard classifiers perform well.
PKDD	HIClass: Hyper-interactive Text Classification by Interactive Supervision of Document and Term Labels.	Shantanu Godbole,Abhay Harpale,Sunita Sarawagi,Soumen Chakrabarti	2004	We present the HIClass (Hyper Interactive text Classification) system, an interactive text classification system which combines the cognitive power of humans with the power of automated learners to make statistically sound classification decisions. HIClass is based on active learning principles and has aids for detailed analysis and fine tuning of text classifiers while exerting a low cognitive load on the user.
PKDD	Mining Thick Skylines over Large Databases.	Wen Jin,Jiawei Han,Martin Ester	2004	People recently are interested in a new operator, called skyline [3], which returns the objects that are not dominated by any other objects with regard to certain measures in a multi-dimensional space. Recent work on the skyline operator [3,15,8,13,2] focuses on efficient computation of skylines in large databases. However, such work gives users only thin skylines, i.e., single objects, which may not be desirable in some real applications. In this paper, we propose a novel concept, called thick skyline, which recommends not only skyline objects but also their nearby neighbors within -distance. Efficient computation methods are developed including (1) two efficient algorithms, Sampling-and-Pruning and Indexing-and-Estimating, to find such thick skyline with the help of statistics or indexes in large databases, and (2) a highly efficient Microcluster-based algorithm for mining thick skyline. The Microcluster-based method not only leads to substantial savings in computation but also provides a cocise representation of the thick skyline in the case of high cardinalities. Our experimental performance study shows that the proposed methods are both efficient and effective.
PKDD	Ensemble Feature Ranking.	Kees Jong,Jérémie Mary,Antoine Cornuéjols,Elena Marchiori,Michèle Sebag	2004	A crucial issue for Machine Learning and Data Mining is Feature Selection, selecting the relevant features in order to focus the learning search. A relaxed setting for Feature Selection is known as Feature Ranking, ranking the features with respect to their relevance. This paper proposes an ensemble approach for Feature Ranking, aggregating feature rankings extracted along independent runs of an evolutionary learning algorithm named ROGER. The convergence of ensemble feature ranking is studied in a theoretical perspective, and a statistical model is devised for the empirical validation, inspired from the complexity framework proposed in the Constraint Satisfaction domain. Comparative experiments demonstrate the robustness of the approach for learning (a limited kind of) non-linear concepts, specifically when the features significantly outnumber the examples.
PKDD	Privately Computing a Distributed k-nn Classifier.	Murat Kantarcioglu,Chris Clifton	2004	The ability of databases to organize and share data often raises privacy concerns. Data warehousing combined with data mining, bringing data from multiple sources under a single authority, increases the risk of privacy violations. Privacy preserving data mining provides a means of addressing this issue, particularly if data mining is done in a way that doesnt disclose information beyond the result. This paper presents a method for privately computing k-nn classification from distributed sources without revealing any information about the sources or their data, other than that revealed by the final classification result.
PKDD	Strength in Diversity: The Advance of Data Analysis.	David J. Hand	2004	The scientific analysis of data is only around a century old. For most of that century, data analysis was the realm of only one discipline - statistics. As a consequence of the development of the computer, things have changed dramatically and now there are several such disciplines, including machine learning, pattern recognition, and data mining. This paper looks at some of the similarities and some of the differences between these disciplines, noting where they intersect and, perhaps of more interest, where they do not. Particular issues examined include the nature of the data with which they are concerned, the role of mathematics, differences in the objectives, how the different areas of application have led to different aims, and how the different disciplines have led sometimes to the same analytic tools being developed, but also sometimes to different tools being developed. Some conjectures about likely future developments are given.
PKDD	Balios - The Engine for Bayesian Logic Programs.	Kristian Kersting,Uwe Dick	2004	Inductive Logic Programming (ILP) [4] combines techniques from machine learning with the representation of logic programming. It aims at inducing logical clauses, i.e, general rules from specific observations and background knowledge. Because of focusing on logical clauses, traditional ILP systems do not model uncertainty explicitly. On the other hand, state-of-the-art probabilistic models such as Bayesian networks (BN) [5], hidden Markov models, and stochastic context-free grammars have a rigid structure and therefore have problems representing a variable number of objects and relations among these objects. Recently, various relational extensions of traditional probabilistic models have been proposed, see [1] for an overview. The newly emerging field of stochastic relational learning (SRL) studies learning such rich probabilistic models.
PKDD	Classifying Protein Fingerprints.	Melanie Hilario,Alex Mitchell,Jee-Hyub Kim,Paul Bradley,Terri K. Attwood	2004	Protein fingerprints are groups of conserved motifs which can be used as diagnostic signatures to identify and characterize collections of protein sequences. These fingerprints are stored in the prints database after time-consuming annotation by domain experts who must first of all determine the fingerprint type, i.e., whether a fingerprint depicts a protein family, superfamily or domain. To alleviate the annotation bottleneck, a system called PRECIS has been developed which automatically generates prints records, provisionally stored in a supplement called preprints. One limitation of PRECIS is that its classification heuristics, handcoded by proteomics experts, often misclassify fingerprint type; their error rate has been estimated at 40%. This paper reports on an attempt to build more accurate classifiers based on information drawn from the fingerprints themselves and from the SWISS-PROT database. Extensive experimentation using 10-fold cross-validation led to the selection of a model combiing the ReliefF feature selector with an SVM-RBF learner. The final models error rate was estimated at 14.1% on a blind test set, representing a 26% accuracy gain over PRECIS handcrafted rules.
PKDD	Incremental Nonlinear PCA for Classification.	Byung-Joo Kim,Il Kon Kim	2004	The purpose of this study is to propose a new online and nonlinear PCA(OL-NPCA) method for feature extraction from the incremental data. Kernel PCA(KPCA) is widely used for nonlinear feature extraction, however, it has been pointed out that KPCA has the following problems. First, applying KPCA to patterns requires storing and finding the eigenvectors of a kernel matrix, which is infeasible for a large number of data N. Second problem is that in order to update the eigenvectors with an another data, the whole eigenspace should be recomputed. OL-NPCA overcomes these problems by incremental eigenspace update method with a feature mapping function. According to the experimental results, which comes from applying OL-NPCA to a toy and a large data problem, OL-NPCA shows following advantages. First, OL-NPCA is more efficient in memory requirement than KPCA. Second advantage is that OL-NPCA is comparable in performance to KPCA. Furthermore, performance of OL-NPCA can be easily improved by re-learning the data. For classification extracted features are used as input for least squares support vector machine. In our experiments we show that proposed feature extraction method is comparable in performance to a Kernel PCA and proposed classification system shows a high classification performance on UCI benchmarking data and NIST handwritten data set.
PKDD	Constraint-Based Mining of Episode Rules and Optimal Window Sizes.	Nicolas Méger,Christophe Rigotti	2004	Episode rules are patterns that can be extracted from a large event sequence, to suggest to experts possible dependencies among occurrences of event types. The corresponding mining approaches have been designed to find rules under a temporal constraint that specifies the maximum elapsed time between the first and the last event of the occurrences of the patterns (i.e., a window size constraint). In some applications the appropriate window size is not known, and furthermore, this size is not the same for different rules. To cope with this class of applications, it has been recently proposed in [2] to specifying the maximal elapsed time between two events (i.e., a maximum gap constraint) instead of a window size constraint. Unfortunately, we show that the algorithm proposed to handle the maximum gap constraint is not complete. In this paper we present a sound and complete algorithm to mine episode rules under the maximum gap constraint, and propose to find, for each rule, the window size corresponding to a local maximum of confidence. We show that the extraction can be efficiently performed in practice on real and synthetic datasets. Finally the experiments show that the notion of local maximum of confidence is significant in practice, since no local maximum are found in random datasets, while they can be found in real ones.
PKDD	Discovering Interpretable Muscle Activation Patterns with the Temporal Data Mining Method.	Fabian Mörchen,Alfred Ultsch,Olaf Hoos	2004	The understanding of complex muscle coordination is an important goal in human movement science. There are numerous applications in medicine, sports, and robotics. The coordination process can be studied by observing complex, often cyclic movements, which are dynamically repeated in an almost identical manner. In this paper we demonstrate how interpretable temporal patterns can be discovered within raw EMG measurements collected from tests in professional In-Line Speed Skating. We show how the Temporal Data Mining Method, a general framework to discover knowledge in multivariate time series, can be used to extract such temporal patterns. This representation of complex muscle coordination opens up new possibilities to optimize, manipulate, or imitate the movements.
PKDD	Analysing Customer Churn in Insurance Data - A Case Study.	Katharina Morik,Hanna Köpcke	2004	Designing a new application of knowledge discovery is a very tedious task. The success is determined to a great extent by an adequate example representation. The transformation of given data to the example representation is a matter of feature generation and selection. The search for an appropriate approach is difficult. In particular, if time data are involved, there exist a large variety of how to handle them. Reports on successful cases can provide case designers with a guideline for the design of new, similar cases. In this paper we present a complete knowledge discovery process applied to insurance data. We use the TF/IDF representation from information retrieval for compiling time-related features of the data set. Experimental reasults show that these new features lead to superior results in terms of accuracy, precision and recall. A heuristic is given which calculates how much the feature space is enlarged or shrinked by the transformation to TF/IDF.
PKDD	Nomograms for Visualization of Naive Bayesian Classifier.	Martin Mozina,Janez Demsar,Michael W. Kattan,Blaz Zupan	2004	Besides good predictive performance, the naive Bayesian classifier can also offer a valuable insight into the structure of the training data and effects of the attributes on the class probabilities. This structure may be effectively revealed through visualization of the classifier. We propose a new way to visualize the naive Bayesian model in the form of a nomogram. The advantages of the proposed method are simplicity of presentation, clear display of the effects of individual attribute values, and visualization of confidence intervals. Nomograms are intuitive and when used for decision support can provide a visual explanation of predicted probabilities. And finally, with a nomogram, a naive Bayesian model can be printed out and used for probability prediction without the use of computer or calculator.
PKDD	A Tolerance Rough Set Approach to Clustering Web Search Results.	Chi Lang Ngo,Hung Son Nguyen	2004	"Two most popular approaches to facilitate searching for information on the web are represented by web search engine and web directories. Although the performance of search engines is improving every day, searching on the web can be a tedious and time-consuming task due to the huge size and highly dynamic nature of the web. Moreover, the user's ""intention behind the search"" is not clearly expressed which results in too general, short queries. Results returned by search engine can count from hundreds to hundreds of thousands of documents. One approach to manage the large number of results is clustering. Search results clustering can be defined as a process of automatical grouping search results into to thematic groups. However, in contrast to traditional document clustering, clustering of search results are done on-the-fly (per user query request) and locally on a limited set of results return from the search engine. Clustering of search results can help user navigate through large set of documents more efficiently. By providing concise, accurate description of clusters, it lets user localizes interesting document faster.In this paper, we proposed an approach to search results clustering based on Tolerance Rough Set following the work on document clustering [4,3]. Tolerance classes are used to approximate concepts existed in documents. The application of Tolerance Rough Set model in document clustering was proposed as a way to enrich document and cluster representation with the hope of increasing clustering performance."
PKDD	Using a Hash-Based Method for Apriori-Based Graph Mining.	Phu Chien Nguyen,Takashi Washio,Kouzou Ohara,Hiroshi Motoda	2004	The problem of discovering frequent subgraphs of graph data can be solved by constructing a candidate set of subgraphs first, and then, identifying within this candidate set those subgraphs that meet the frequent subgraph requirement. In Apriori-based graph mining, to determine candidate subgraphs from a huge number of generated adjacency matrices is usually the dominating factor for the overall graph mining performance since it requires to perform many graph isomorphism tests. To address this issue, we develop an effective algorithm for the candidate set generation. It is a hash-based algorithm and was confirmed effective through experiments on both real-world and synthetic graph data.
PKDD	A Spectroscopy of Texts for Effective Clustering.	Wenyuan Li,Wee Keong Ng,Kok-Leong Ong,Ee-Peng Lim	2004	"For many clustering algorithms, such as k-means, EM, and CLOPE, there is usually a requirement to set some parameters. Often, these parameters directly or indirectly control the number of clusters to return. In the presence of different data characteristics and analysis contexts, it is often difficult for the user to estimate the number of clusters in the data set. This is especially true in text collections such as Web documents, images or biological data. The fundamental question this paper addresses is: ""How can we effectively estimate the natural number of clusters in a given text collection?"". We propose to use spectral analysis, which analyzes the eigenvalues (not eigenvectors) of the collection, as the solution to the above. We first present the relationship between a text collection and its underlying spectra. We then show how the answer to this question enhances the clustering process. Finally, we conclude with empirical results and related work."
PKDD	Evaluation of Rule Interestingness Measures with a Clinical Dataset on Hepatitis.	Miho Ohsaki,Shinya Kitaguchi,Kazuya Okamoto,Hideto Yokoi,Takahira Yamaguchi	2004	This research empirically investigates the performance of conventional rule interestingness measures and discusses their practicality for supporting KDD through human-system interaction in medical domain. We compared the evaluation results by a medical expert and those by selected measures for the rules discovered from a dataset on hepatitis. Recall, Jaccard, Kappa, CST, X2-M, and Peculiarity demonstrated the highest performance, and many measures showed a complementary trend under our experimental conditions. These results indicate that some measures can predict really interesting rules at a certain level and that their combinational use will be useful.
PKDD	SEWeP: A Web Mining System Supporting Semantic Personalization.	Stratos Paulakis,Charalampos Lampos,Magdalini Eirinaki,Michalis Vazirgiannis	2004	We present SEWeP, a Web Personalization prototype system that integrates usage data with content semantics, expressed in taxonomy terms, in order to produce a broader yet semantically focused set of recommendations.
PKDD	Improving the Performance of the RISE Algorithm.	Aloísio Carlos de Pina,Gerson Zaverucha	2004	RISE is a well-known multi-strategy learning algorithm that combines rule induction and instance-based learning. It achieves higher accuracy than some state-of-the-art learning algorithms, but for large data sets it has a very high average running time. This work presents the analysis and experimental evaluation of SUNRISE, a new multi-strategy learning algorithm based on RISE, developed to be faster than RISE with similar accuracy.
PKDD	Classification in Geographical Information Systems.	Salvatore Rinzivillo,Franco Turini	2004	The paper deals with the problem of knowledge discovery in spatial databases. In particular, we explore the application of decision tree learning methods to the classification of spatial datasets. Spatial datasets, according to the Geographic Information System approach, are represented as stack of layers, where each layer is associated with an attribute. We propose an ID3-like algorithm based on an entropy measure, weighted on a specific spatial relation (i.e. overlap). We describe an application of the algorithm to the classification of geographical areas for agricultural purposes.
PKDD	Digging into Acceptor Splice Site Prediction: An Iterative Feature Selection Approach.	Yvan Saeys,Sven Degroeve,Yves Van de Peer	2004	Feature selection techniques are often used to reduce data dimensionality, increase classification performance, and gain insight into the processes that generated the data. In this paper, we describe an iterative procedure of feature selection and feature construction steps, improving the classification of acceptor splice sites, an important subtask of gene prediction. We show that acceptor prediction can benefit from feature selection, and describe how feature selection techniques can be used to gain new insights in the classification of acceptor sites. This is illustrated by the identification of a new, biologically motivated feature: the AG-scanning feature.The results described in this paper contribute both to the domain of gene prediction, and to research in feature selection techniques, describing a new wrapper based feature weighting method that aids in knowledge discovery when dealing with complex datasets.
PKDD	SPIN! Data Mining System Based on Component Architecture.	Alexandr A. Savinov	2004	The SPIN! data mining system has a component-based architecture, where each component encapsulates some specific functionality such as a data source, an analysis algorithm or visualization. Individual components can be visually linked within one workspace for solving different data mining tasks. The SPIN! friendly user interface and flexible underlying component architecture provide a powerful integrated environment for executing main tasks constituting a typical data mining cycle: data preparation, analysis, and visualization.
PKDD	Asynchronous and Anticipatory Filter-Stream Based Parallel Algorithm for Frequent Itemset Mining.	Adriano Veloso,Wagner Meira Jr.,Renato Ferreira,Dorgival Olavo Guedes Neto,Srinivasan Parthasarathy	2004	In this paper we propose a novel parallel algorithm for frequent itemset mining. The algorithm is based on the filter-stream programming model, in which the frequent itemset mining process is represented as a data flow controlled by a series of producer and consumer components (called filters), and the data flow (communication) between such filters is made via streams. When production rate matches consumption rate, and communication overhead between producer and consumer filters is minimized, a high degree of asynchrony is achieved. Following this strategy, our algorithm employs an asynchronous candidate generation, and minimizes communication between filters by transferring only the necessary aggregated information. Another nice feature of our algorithm is a look forward approach which accelerates frequent itemset determination. Extensive evaluation shows the parallel performance and scalability of our algorithm.
PKDD	A Quantification of Cluster Novelty with an Application to Martian Topography.	Ricardo Vilalta,Tomasz F. Stepinski,Murali-Krishna Achari,Francisco Ocegueda-Hernandez	2004	Automated tools for knowledge discovery are frequently invoked in databases where objects already group into some known classification scheme. In the context of unsupervised learning or clustering, such tools delve inside large databases looking for alternative classification schemes that are both meaningful and novel. A quantification of cluster novelty can be looked upon as the degree of separation between each new cluster and its most similar class. Our approach models each cluster and class as a Gaussian distribution and estimates the degree of overlap between both distributions by measuring their intersecting area. Unlike other metrics, our method quantifies the novelty of each cluster individually, and enables us to rank classes according to its similarity to each new cluster. We test our algorithm on Martian landscapes using a set of known classes called geological units; experimental results show a new interpretation for the characterization of Martian landscapes.
PKDD	Itemset Classified Clustering.	Jun Sese,Shinichi Morishita	2004	Clustering results could be comprehensible and usable if individual groups are associated with characteristic descriptions. However, characterization of clusters followed by clustering may not always produce clusters associated with special features, because the first clustering process and the second classification step are done independently, demanding an elegant way that combines clustering and classification and executes both simultaneously.In this paper, we focus on itemsets as the feature for characterizing groups, and present a technique called itemset classified clustering, which divides data into groups given the restriction that only divisions expressed using a common itemset are allowed and computes the optimal itemset maximizing the interclass variance between the groups. Although this optimization problem is generally intractable, we develop techniques that effectively prune the search space and efficiently compute optimal solutions in practice. We remark that itemset classified clusters are likely to be overlooked by traditional clustering algorithms such as two-clustering or k-means, and demonstrate the scalability of our algorithm with respect to the amount of data by the application of our method to real biological datasets.
PKDD	Combining Winnow and Orthogonal Sparse Bigrams for Incremental Spam Filtering.	Christian Siefkes,Fidelis Assis,Shalendra Chhabra,William S. Yerazunis	2004	Spam filtering is a text categorization task that has attracted significant attention due to the increasingly huge amounts of junk email on the Internet. While current best-practice systems use Naive Bayes filtering and other probabilistic methods, we propose using a statistical, but non-probabilistic classifier based on the Winnow algorithm. The feature space considered by most current methods is either limited in expressivity or imposes a large computational cost. We introduce orthogonal sparse bigrams (OSB) as a feature combination technique that overcomes both these weaknesses. By combining Winnow and OSB with refined preprocessing and tokenization techniques we are able to reach an accuracy of 99.68% on a difficult test corpus, compared to 98.88% previously reported by the CRM114 classifier on the same test corpus.
PKDD	Density-Based Spatial Clustering in the Presence of Obstacles and Facilitators.	Xin Wang,Camilo Rostoker,Howard J. Hamilton	2004	In this paper, we propose a new spatial clustering method, called DBRS+, which aims to cluster spatial data in the presence of both obstacles and facilitators. It can handle datasets with intersected obstacles and facilitators. Without preprocessing, DBRS+ processes constraints during clustering. It can find clusters with arbitrary shapes and varying densities. DBRS+ has been empirically evaluated using synthetic and real data sets and its performance has been compared to DBRS, AUTOCLUST+, and DBCLuC*.
PKDD	Text Mining for Finding Functional Community of Related Genes Using TCM Knowledge.	Zhaohui Wu,Xuezhong Zhou,Baoyan Liu,Junli Chen	2004	We present a novel text mining approach to uncover the functional gene relationships, maybe, temporal and spatial functional modular interaction networks, from MEDLINE in large scale. Other than the regular approaches, which only consider the reductionistic molecular biological knowledge in MEDLINE, we use TCM knowledge(e.g. Symptom Complex) and the 50,000 TCM bibliographic records to automatically congregate the related genes. A simple but efficient bootstrapping technique is used to extract the clinical disease names from TCM literature, and term co-occurrence is used to identify the disease-gene relationships in MEDLINE abstracts and titles. The underlying hypothesis is that the relevant genes of the same Symptom Complex will have some biological interactions. It is also a probing research to study the connection of TCM with modern biomedical and post-genomics studies by text mining. The preliminary results show that Symptom Complex gives a novel top-down view of functional genomics research, and it is a promising research field while connecting TCM with modern life science using text mining.
PKDD	Dealing with Predictive-but-Unpredictable Attributes in Noisy Data Sources.	Ying Yang,Xindong Wu,Xingquan Zhu	2004	Attribute noise can affect classification learning. Previous work in handling attribute noise has focused on those predictable attributes that can be predicted by the class and other attributes. However, attributes can often be predictive but unpredictable. Being predictive, they are essential to classification learning and it is important to handle their noise. Being unpredictable, they require strategies different from those of predictable attributes. This paper presents a study on identifying, cleansing and measuring noise for predictive-but-unpredictable attributes. New strategies are accordingly proposed. Both theoretical analysis and empirical evidence suggest that these strategies are more effective and more efficient than previous alternatives.
PKDD	A New Scheme on Privacy Preserving Association Rule Mining.	Nan Zhang,Shengquan Wang,Wei Zhao	2004	We address the privacy preserving association rule mining problem in a system with one data miner and multiple data providers, each holds one transaction. The literature has tacitly assumed that randomization is the only effective approach to preserve privacy in such circumstances. We challenge this assumption by introducing an algebraic techniques based scheme. Compared to previous approaches, our new scheme can identify association rules more accurately but disclose less private information. Furthermore, our new scheme can be readily integrated as a middleware with existing systems.
PKDD	Mining History of Changes to Web Access Patterns.	Qiankun Zhao,Sourav S. Bhowmick	2004	Recently, a lot of work has been done in web usage mining [2]. Among them, mining of frequent Web Access Pattern (WAP) is the most well researched issue [1]. The idea is to transform web logs into sequences of events with user identifications and timestamps, and then extract association and sequential patterns from the events data with certain metrics. The frequent WAPs have been applied to a wide range of applications such as personalization, system improvement, site modification, business intelligence, and usage characterization [2]. However, most of the existing techniques focus only on mining frequent WAP from snapshot web usage data, while web usage data is dynamic in real life. While the frequent WAPs are useful in many applications, knowledge hidden behind the historical changes of web usage data, which reflects how WAPs change, is also critical to many applications such as adaptive web, web site maintenance, business intelligence, etc.In this paper, we propose a novel approach to discover hidden knowledge from historical changes to WAPs. Rather than focusing on the occurrence of the WAPs, we focus on the frequently changing web access patterns. We define a novel type of knowledge, Frequent Mutating WAP (FM-WAP), based on the historical changes of WAPs. The FM-WAP mining process consists of three phases. Firstly, web usage data is represented as a set of WAP trees and partitioned into a sequence of WAP groups ( subsets of the WAP trees) according to a user-defined calendar pattern, where each WAP group is represented as a WAP forest. Consequently, the log data is represented by a sequence of WAP forests called WAP history. Then, changes among the WAP history are detected and stored in the global forest. Finally, the FM-WAP is extracted by a traversal of the global forest. Extensive experiments show that our proposed approach can produce novel knowledge of web access patterns efficiently with good scalability.
PKDD	Knowledge Discovery in Databases: PKDD 2004, 8th European Conference on Principles and Practice of Knowledge Discovery in Databases, Pisa, Italy, September 20-24, 2004, Proceedings	Jean-François Boulicaut,Floriana Esposito,Fosca Giannotti,Dino Pedreschi	2004	Knowledge Discovery in Databases: PKDD 2004, 8th European Conference on Principles and Practice of Knowledge Discovery in Databases, Pisa, Italy, September 20-24, 2004, Proceedings
SDM	Subspace Clustering of High Dimensional Data.	Carlotta Domeniconi,Dimitris Papadopoulos,Dimitrios Gunopulos,Sheng Ma	2004	Subspace Clustering of High Dimensional Data.
SDM	Privacy-Preserving Multivariate Statistical Analysis: Linear Regression and Classification.	Wenliang Du,Yunghsiang S. Han,Shigang Chen	2004	Privacy-Preserving Multivariate Statistical Analysis: Linear Regression and Classification.
SDM	A Top-Down Method for Mining Most-Specific Frequent Patterns in Biological Sequences.	Martin Ester,Xiang Zhang	2004	A Top-Down Method for Mining Most-Specific Frequent Patterns in Biological Sequences.
SDM	Active Mining of Data Streams.	Wei Fan,Yi-an Huang,Haixun Wang,Philip S. Yu	2004	Active Mining of Data Streams.
SDM	Enhancing Communities of Interest Using Bayesian Stochastic Blockmodels.	Deepak Agrawal,Daryl Pregibon	2004	Enhancing Communities of Interest Using Bayesian Stochastic Blockmodels.
SDM	GRM: A New Model for Clustering Linear Sequences.	Hansheng Lei	2004	GRM: A New Model for Clustering Linear Sequences.
SDM	The Discovery of Generalized Causal Models with Mixed Variables Using MML Criterion.	Gang Li,Honghua Dai	2004	The Discovery of Generalized Causal Models with Mixed Variables Using MML Criterion.
SDM	Clustering with Bregman Divergences.	Arindam Banerjee,Srujana Merugu,Inderjit S. Dhillon,Joydeep Ghosh	2004	A wide variety of distortion functions, such as squared Euclidean distance, Mahalanobis distance, Itakura-Saito distance and relative entropy, have been used for clustering. In this paper, we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences. The proposed algorithms unify centroid-based parametric clustering approaches, such as classical <tt>kmeans</tt>, the Linde-Buzo-Gray (LBG) algorithm and information-theoretic clustering, which arise by special choices of the Bregman divergence. The algorithms maintain the simplicity and scalability of the classical <tt>kmeans</tt> algorithm, while generalizing the method to a large class of clustering loss functions. This is achieved by first posing the hard clustering problem in terms of minimizing the loss in Bregman information, a quantity motivated by rate distortion theory, and then deriving an iterative algorithm that monotonically decreases this loss. In addition, we show that there is a bijection between regular exponential families and a large class of Bregman divergences, that we call regular Bregman divergences. This result enables the development of an alternative interpretation of an efficient EM scheme for learning mixtures of exponential family distributions, and leads to a simple soft clustering algorithm for regular Bregman divergences. Finally, we discuss the connection between rate distortion theory and Bregman clustering and present an information theoretic analysis of Bregman clustering algorithms in terms of a trade-off between compression and loss in Bregman information.
SDM	Classifying Documents Without Labels.	Daniel Barbará,Carlotta Domeniconi,Ning Kang	2004	Classifying Documents Without Labels.
SDM	Active Semi-Supervision for Pairwise Constrained Clustering.	Sugato Basu,Arindam Banerjee,Raymond J. Mooney	2004	Active Semi-Supervision for Pairwise Constrained Clustering.
SDM	A Kernel-Based Semi-Naïve Bayesian Classifier Using P-Trees.	Anne Denton,William Perrizo	2004	A Kernel-Based Semi-Naïve Bayesian Classifier Using P-Trees.
SDM	Basic Association Rules.	Guichong Li,Howard J. Hamilton	2004	Basic Association Rules.
SDM	IFD: Iterative Feature and Data Clustering.	Tao Li,Sheng Ma	2004	IFD: Iterative Feature and Data Clustering.
SDM	Principal Component Analysis and Effective K-Means Clustering.	Chris H. Q. Ding,Xiaofeng He	2004	Principal Component Analysis and Effective K-Means Clustering.
SDM	Training Support Vector Machines Using Adaptive Clustering.	Daniel Boley,Dongwei Cao	2004	Training Support Vector Machines Using Adaptive Clustering.
SDM	Adaptive Filtering for Efficient Record Linkage.	Lifang Gu,Rohan A. Baxter	2004	Adaptive Filtering for Efficient Record Linkage.
SDM	GenIc: A Single-Pass Generalized Incremental Algorithm for Clustering.	Chetan Gupta,Robert L. Grossman	2004	GenIc: A Single-Pass Generalized Incremental Algorithm for Clustering.
SDM	Visually Mining through Cluster Hierarchies.	Stefan Brecheisen,Hans-Peter Kriegel,Peer Kröger,Martin Pfeifle	2004	Visually Mining through Cluster Hierarchies.
SDM	Mining Patters of Activity from Video Data.	Michael C. Burl	2004	Mining Patters of Activity from Video Data.
SDM	Lazy Learning by Scanning Memory Image Lattice.	Yiqiu Han,Wai Lam	2004	Lazy Learning by Scanning Memory Image Lattice.
SDM	Exploiting Hierarchical Domain Values in Classification Learning.	Yiqiu Han,Wai Lam	2004	Exploiting Hierarchical Domain Values in Classification Learning.
SDM	Equivalence of Several Two-Stage Methods for Linear Discriminant Analysis.	Peg Howland,Haesun Park	2004	Equivalence of Several Two-Stage Methods for Linear Discriminant Analysis.
SDM	R-MAT: A Recursive Model for Graph Mining.	Deepayan Chakrabarti,Yiping Zhan,Christos Faloutsos	2004	R-MAT: A Recursive Model for Graph Mining.
SDM	Clustering Categorical Data Using the Correlated-Force Ensemble.	Ming-Syan Chen,Kun-Ta Chuang	2004	Clustering Categorical Data Using the Correlated-Force Ensemble.
SDM	Conquest: A Distributed Tool for Constructing Summaries of High-Dimensional Discrete Attribute Data Sets.	Jie Chi,Mehmet Koyutürk,Ananth Grama	2004	Conquest: A Distributed Tool for Constructing Summaries of High-Dimensional Discrete Attribute Data Sets.
SDM	Constructing Time Decompositions for Analyzing Time-Stamped Documents.	Parvathi Chundi,Daniel J. Rosenkrantz	2004	Constructing Time Decompositions for Analyzing Time-Stamped Documents.
SDM	IREP++, A Faster Rule Learning Algorithm.	Oliver Dain,Robert Cunningham,Stephen Boyer	2004	IREP++, A Faster Rule Learning Algorithm.
SDM	Hierarchical Clustering for Thematic Browsing and Summarization of Large Sets of Association Rules.	Alípio Jorge	2004	Hierarchical Clustering for Thematic Browsing and Summarization of Large Sets of Association Rules.
SDM	CREDOS: Classification Using Ripple Down Structure (A Case for Rare Classes).	Mahesh V. Joshi,Vipin Kumar	2004	CREDOS: Classification Using Ripple Down Structure (A Case for Rare Classes).
SDM	Learning to Read Between the Lines: The Aspect Bernoulli Model.	Ata Kabán,Ella Bingham,T. Hirsimäki	2004	Learning to Read Between the Lines: The Aspect Bernoulli Model.
SDM	DOMISA: DOM-Based Information Space Adsorption of Web Information Hierarchy Mining.	Hung-Yu Kao,Jan-Ming Ho,Ming-Syan Chen	2004	DOMISA: DOM-Based Information Space Adsorption of Web Information Hierarchy Mining.
SDM	Tesselation and Clustering by Mixture Models and Their Parallel Implementations.	Qiang Du,Xiaoqiang Wang	2004	Tesselation and Clustering by Mixture Models and Their Parallel Implementations.
SDM	Mining Text for Word Senses Using Independent Component Analysis.	Reinhard Rapp	2004	Mining Text for Word Senses Using Independent Component Analysis.
SDM	VEDAS: A Mobile and Distributed Data Stream Mining System for Real-Time Vehicle Monitoring.	Hillol Kargupta,Ruchita Bhargava,Kun Liu,Michael Powers,Patrick Blair,Samuel Bushra,James Dull,Kakali Sarkar,Martin Klein,Mitesh Vasa,David Handy	2004	VEDAS: A Mobile and Distributed Data Stream Mining System for Real-Time Vehicle Monitoring.
SDM	Data Reduction in Support Vector Machines by a Kernelized Ionic Interaction Model.	Hyunsoo Kim,Haesun Park	2004	Data Reduction in Support Vector Machines by a Kernelized Ionic Interaction Model.
SDM	Visualizing RFM Segmentation.	Ron Kohavi,Rajesh Parekh	2004	Visualizing RFM Segmentation.
SDM	Using Support Vector Machines for Classifying Large Sets of Multi-Represented Objects.	Hans-Peter Kriegel,Peer Kröger,Alexey Pryakhin,Matthias Schubert	2004	Using Support Vector Machines for Classifying Large Sets of Multi-Represented Objects.
SDM	Continuous-Time Bayesian Modeling of Clinical Data.	Sathyakama Sandilya,R. Bharat Rao	2004	Continuous-Time Bayesian Modeling of Clinical Data.
SDM	Density-Connected Subspace Clustering for High-Dimensional Data.	Peer Kröger,Hans-Peter Kriegel,Karin Kailing	2004	Density-Connected Subspace Clustering for High-Dimensional Data.
SDM	Finding Frequent Patterns in a Large Sparse Graph.	Michihiro Kuramochi,George Karypis	2004	Graph-based modeling has emerged as a powerful abstraction capable of capturing in a single and unified framework many of the relational, spatial, topological, and other characteristics that are present in a variety of datasets and application areas. Computationally efficient algorithms that find patterns corresponding to frequently occurring subgraphs play an important role in developing data mining-driven methodologies for analyzing the graphs resulting from such datasets. This paper presents two algorithms, based on the horizontal and vertical pattern discovery paradigms, that find the connected subgraphs that have a sufficient number of edge-disjoint embeddings in a single large undirected labeled sparse graph. These algorithms use three different methods for determining the number of edge-disjoint embeddings of a subgraph and employ novel algorithms for candidate generation and frequency counting, which allow them to operate on datasets with different characteristics and to quickly prune unpromising subgraphs. Experimental evaluation on real datasets from various domains show that both algorithms achieve good performance, scale well to sparse input graphs with more than 120,000 vertices or 110,000 edges, and significantly outperform previously developed algorithms.
SDM	Text Mining from Site Invariant and Dependent Features for Information Extraction Knowledge Adaptation.	Wai Lam,Tak-Lam Wong	2004	Text Mining from Site Invariant and Dependent Features for Information Extraction Knowledge Adaptation.
SDM	Nonlinear Manifold Learning for Data Stream.	Martin H. C. Law,Nan Zhang,Anil K. Jain	2004	Nonlinear Manifold Learning for Data Stream.
SDM	Minimum Sum-Squared Residue Co-Clustering of Gene Expression Data.	Hyuk Cho,Inderjit S. Dhillon,Yuqiang Guan,Suvrit Sra	2004	Minimum Sum-Squared Residue Co-Clustering of Gene Expression Data.
SDM	Mixture Density Mercer Kernels: A Method to Learn Kernels Directly from Data.	Ashok N. Srivastava	2004	Mixture Density Mercer Kernels: A Method to Learn Kernels Directly from Data.
SDM	Resource-Aware Mining with Variable Granularities in Data Streams.	Wei-Guang Teng,Ming-Syan Chen,Philip S. Yu	2004	Resource-Aware Mining with Variable Granularities in Data Streams.
SDM	A General Framework for Adaptive Anomaly Detection with Evolving Connectionist Systems.	Yihua Liao,V. Rao Vemuri,Alejandro Pasos	2004	A General Framework for Adaptive Anomaly Detection with Evolving Connectionist Systems.
SDM	A Mixture Model for Clustering Ensembles.	Alexander P. Topchy,Anil K. Jain,William F. Punch	2004	A Mixture Model for Clustering Ensembles.
SDM	A General Probabilistic Framework for Mining Labeled Ordered Trees.	Nobuhisa Ueda,Kiyoko F. Aoki,Hiroshi Mamitsuka	2004	A General Probabilistic Framework for Mining Labeled Ordered Trees.
SDM	Privacy Preserving Naïve Bayes Classifier for Vertically Partitioned Data.	Jaideep Vaidya,Chris Clifton	2004	Privacy Preserving Naïve Bayes Classifier for Vertically Partitioned Data.
SDM	BAMBOO: Accelerating Closed Itemset Mining by Deeply Pushing the Length-Decreasing Support Constraint.	Jianyong Wang,George Karypis	2004	BAMBOO: Accelerating Closed Itemset Mining by Deeply Pushing the Length-Decreasing Support Constraint.
SDM	Class-Specific Ensembles for Active Learning.	Amit Mandvikar,Huan Liu	2004	Class-Specific Ensembles for Active Learning.
SDM	Making Time-Series Classification More Accurate Using Learned Constraints.	Chotirat (Ann) Ratanamahatana,Eamonn J. Keogh	2004	Making Time-Series Classification More Accurate Using Learned Constraints.
SDM	Mining Relationships Between Interacting Episodes.	Carl Mooney,John F. Roddick	2004	Mining Relationships Between Interacting Episodes.
SDM	A Framework for Discovering Co-Location Patterns in Data Sets with Extended Spatial Objects.	Hui Xiong,Shashi Shekhar,Yan Huang,Vipin Kumar,Xiaobin Ma,Jin Soung Yoo	2004	A Framework for Discovering Co-Location Patterns in Data Sets with Extended Spatial Objects.
SDM	An Abstract Weighting Framework for Clustering Algorithms.	Richard Nock,Frank Nielsen	2004	An Abstract Weighting Framework for Clustering Algorithms.
SDM	HICAP: Hierarchical Clustering with Pattern Preservation.	Hui Xiong,Michael Steinbach,Pang-Ning Tan,Vipin Kumar	2004	HICAP: Hierarchical Clustering with Pattern Preservation.
SDM	RBA: An Integrated Framework for Regression based on Association Rules.	Aysel Ozgur,Pang-Ning Tan,Vipin Kumar	2004	RBA: An Integrated Framework for Regression based on Association Rules.
SDM	Reservoir-Based Random Sampling with Replacement from Data Stream.	Byung-Hoon Park,George Ostrouchov,Nagiza F. Samatova,Al Geist	2004	Reservoir-Based Random Sampling with Replacement from Data Stream.
SDM	A Foundational Approach to Mining Itemset Utilities from Databases.	Hong Yao,Howard J. Hamilton,Cory J. Butz	2004	A Foundational Approach to Mining Itemset Utilities from Databases.
SDM	Text Mining Using Non-Negative Matrix Factorizations.	V. Paul Pauca,Farial Shahnaz,Michael W. Berry,Robert J. Plemmons	2004	Text Mining Using Non-Negative Matrix Factorizations.
SDM	Analytical Evaluation of Clustering Results Using Computational Negative Controls.	Ronald K. Pearson,Tom Zylkin,James S. Schwaber,Gregory E. Gonye	2004	Analytical Evaluation of Clustering Results Using Computational Negative Controls.
SDM	Proceedings of the Fourth SIAM International Conference on Data Mining, Lake Buena Vista, Florida, USA, April 22-24, 2004	Michael W. Berry,Umeshwar Dayal,Chandrika Kamath,David B. Skillicorn	2004	Proceedings of the Fourth SIAM International Conference on Data Mining, Lake Buena Vista, Florida, USA, April 22-24, 2004
ICDM	A Thorough Experimental Study of Datasets for Frequent Itemsets.	Frédéric Flouvat,Fabien De Marchi,Jean-Marc Petit	2005	The discovery of frequent patterns is a famous problem in data mining. While plenty of algorithms have been proposed during the last decade, only a few contributions have tried to understand the influence of datasets on the algorithms behavior. Being able to explain why certain algorithms are likely to perform very well or very poorly on some datasets is still an open question. In this setting, we describe a thorough experimental study of datasets with respect to frequent itemsets. We study the distribution of frequent itemsets with respect to itemsets size together with the distribution of three concise representations: frequent closed, frequent free and frequent essential itemsets. For each of them, we also study the distribution of their positive and negative borders whenever possible. From this analysis, we exhibit a new characterization of datasets and some invariants allowing to better predict the behavior of well known algorithms. The main perspective of this work is to devise adaptive algorithms with respect to dataset characteristics.
ICDM	Modeling Multiple Time Series for Anomaly Detection.	Philip K. Chan,Matthew V. Mahoney	2005	Our goal is to generate comprehensible and accurate models from multiple time series for anomaly detection. The models need to produce anomaly scores in an online manner for real-life monitoring tasks. We introduce three algorithms that work in a constructed feature space and evaluate them with a real data set from the NASA shuttle program. Our offline and online evaluations indicate that our algorithms can be more accurate than two existing algorithms.
ICDM	A Rule Evaluation Support Method with Learning Models Based on Objective Rule Evaluation Indexes.	Hidenao Abe,Shusaku Tsumoto,Miho Ohsaki,Takahira Yamaguchi	2005	In this paper, we present a novel rule evaluation support method for post-processing of mined results with rule evaluation models based on objective indexes. Post-processing of mined results is one of the key issues to make a data mining process successfully. However, it is difficult for human experts to evaluate many thousands of rules from a large dataset with noises completely. To reduce the costs of rule evaluation procedures, we have developed the rule evaluation support method with rule evaluation models, which are obtained with objective rule evaluation indexes and evaluations of a human expert for each rule. Since the method is needed more accurate rule evaluation models, we have compared learning algorithms to construct rule evaluation models with the actual meningitis data mining result and actual rule sets from UCI datasets. Then we show the availability of our adaptive rule evaluation support method.
ICDM	Handling Generalized Cost Functions in the Partitioning Optimization Problem through Sequential Binary Programming.	Alan S. Abrahams,Adrian Becker,Daniel Fleder,Ian C. MacMillan	2005	This paper proposes a framework for cost-sensitive classification under a generalized cost function. By combining decision trees with sequential binary programming, we can handle unequal misclassification costs, constrained classification, and complex objective functions that other methods cannot. Our approach has two main contributions. First, it provides a new method for cost-sensitive classification that outperforms a traditional, accuracy-based method and some current cost-sensitive approaches. Second, and more important, our approach can handle a generalized cost function, instead of the simpler misclassification cost matrix to which other approaches are limited.
ICDM	Online Hierarchical Clustering in a Data Warehouse Environment.	Elke Achtert,Christian Böhm,Hans-Peter Kriegel,Peer Kröger	2005	Many important industrial applications rely on data mining methods to uncover patterns and trends in large data warehouse environments. Since a data warehouse is typically updated periodically in a batch mode, the mined patterns have to be updated as well. This requires not only accuracy from data mining methods but also fast availability of up-to-date knowledge, particularly in the presence of a heavy update load. To cope with this problem, we propose the use of online data mining algorithms which permanently store the discovered knowledge in suitable data structures and enable an efficient adaptation of these structures after insertions and deletions on the raw data. In this paper, we demonstrate how hierarchical clustering methods can be reformulated as online algorithms based on the hierarchical clustering method OPTICS, using a density estimator for data grouping. We also discuss how this algorithmic schema can be specialized for efficient online single-link clustering. A broad experimental evaluation demonstrates that the efficiency is superior with significant speed-up factors even for large bulk insertions and deletions.
ICDM	Sequential Pattern Mining in Multiple Streams.	Gong Chen,Xindong Wu,Xingquan Zhu	2005	"In this paper, we deal with mining sequential patterns in multiple data streams. Building on a state-of-the-art sequential pattern mining algorithm PrefixSpan for mining transaction databases, we propose MILE¹, an efficient algorithm to facilitate the mining process. MILE recursively utilizes the knowledge of existing patterns to avoid redundant data scanning, and can therefore effectively speed up the new patterns' discovery process. Another unique feature of MILE is that it can incorporate some prior knowledge of the data distribution in data streams into the mining process to further improve the performance. Extensive empirical results show thatMILE is significantly faster than PrefixSpan. As MILE consumes more memory than PrefixSpan, we also present a solution to balance the memory usage and time efficiency in memory constrained environments."
ICDM	eMailSift: Email Classification Based on Structure and Content.	Manu Aery,Sharma Chakravarthy	2005	In this paper we propose a novel approach that uses structure as well as the content of emails in a folder for email classification. Our approach is based on the premise that representative — common and recurring — structures/patterns can be extracted from a pre-classified email folder and the same can be used effectively for classifying incoming emails. A number of factors that influence representative structure extraction and the classification are analyzed conceptually and validated experimentally. In our approach, the notion of inexact graph match is leveraged for deriving structures that provide coverage for characterizing folder contents. Extensive experimentation validate the selection of parameters and the effectiveness of our approach for email classification.
ICDM	Mining Chains of Relations.	Foto N. Afrati,Gautam Das,Aristides Gionis,Heikki Mannila,Taneli Mielikäinen,Panayiotis Tsaparas	2005	Traditional data mining applications consider the problem of mining a single relation between two attributes. For example, in a scientific bibliography database, authors are related to papers, and we may be interested in discovering association rules between authors. However, in real life, we often have multiple attributes related though chains of relations. For example, authors write papers, and papers concern one or more topics. Mining such relational chains poses additional challenges. In this paper we consider the following problem: given a chain of two relationsR₁(A, P) and R₂(P, T) we want to find selectors for the objects in T such that the projected relation between A and P satisfies a specific property. The motivation for our approach is that a given property might not hold on the whole dataset, but it might hold when projecting the data on a selector set. We discuss various algorithms and we examine the conditions under which the apriori technique can be used. We experimentally demonstrate the effectiveness of our methods.
ICDM	An Empirical Bayes Approach to Detect Anomalies in Dynamic Multidimensional Arrays.	Deepak K. Agarwal	2005	"We consider the problem of detecting anomalies in data that arise as multidimensional arrays with each dimension corresponding to the levels of a categorical variable. In typical data mining applications, the number of cells in such arrays are usually large. Our primary focus is detecting anomalies by comparing information at the current time to historical data. Naive approaches advocated in the process control literature do not work well in this scenario due to the multiple testing problem - performing multiple statistical tests on the same data produce excessive number of false positives. We use an Empirical Bayes method which works by fitting a two component gaussian mixture to deviations at current time. The approach is scalable to problems that involve monitoring massive number of cells and fast enough to be potentially useful in many streaming scenarios. We show the superiority of the method relative to a naive ""per component error rate"" procedure through simulation. A novel feature of our technique is the ability to suppress deviations that are merely the consequence of sharp changes in the marginal distributions. This research was motivated by the need to extract critical application information and business intelligence from the daily logs that accompany large-scale spoken dialog systems deployed by AT&T. We illustrate our method on one such system."
ICDM	A Preference Model for Structured Supervised Learning Tasks.	Fabio Aiolli	2005	The preference model introduced in this paper gives a natural framework and a principled solution for a broad class of supervised learning problems with structured predictions, such as predicting orders (label and instance ranking), and predicting rates (classification and ordinal regression). We show how all these problems can be cast as linear problems in an augmented space, and we propose an on-line method to efficiently solve them. Experiments on an ordinal regression task confirm the effectiveness of the approach.
ICDM	Obtaining Best Parameter Values for Accurate Classification.	Frans Coenen,Paul H. Leng	2005	In this paper we examine the effect that the choice of support and confidence thresholds has on the accuracy of classifiers obtained by Classification Association Rule Mining. We show that accuracy can almost always be improved by a suitable choice of threshold values, and we describe a method for finding the best values. We present results that demonstrate this approach can obtain higher accuracy without the need for coverage analysis of the training data. Keywords: Classification, Association Rule Mining.
ICDM	Blocking Anonymity Threats Raised by Frequent Itemset Mining.	Maurizio Atzori,Francesco Bonchi,Fosca Giannotti,Dino Pedreschi	2005	In this paper we study when the disclosure of datamining results represents, per se, a threat to the anonymity of the individuals recorded in the analyzed database. The novelty of our approach is that we focus on an objective definition of privacy compliance of patterns without any reference to a preconceived knowledge of what is sensitive and what is not, on the basis of the rather intuitive and realistic constraint that the anonymity of individuals should be guaranteed. In particular, the problem addressed here arises from the possibility of inferring from the output of frequent itemset mining (i.e., a set of itemsets with support larger than a threshold ó), the existence of patterns with very low support (smaller than an anonymity threshold k)[3]. In the following we develop a simple methodology to block such inference opportunities by introducing distortion on the dangerous patterns.
ICDM	On Feature Selection through Clustering.	Richard Butterworth,Gregory Piatetsky-Shapiro,Dan A. Simovici	2005	We study an algorithm for feature selection that clusters attributes using a special metric and then makes use of the dendrogram of the resulting cluster hierarchy to choose the most relevant attributes. The main interest of our technique resides in the improved understanding of the structure of the analyzed data and of the relative importance of the attributes for the selection process.
ICDM	Adaptive Clustering: Obtaining Better Clusters Using Feedback and Past Experience.	Abraham Bagherjeiran,Christoph F. Eick,Chun-Sheng Chen,Ricardo Vilalta	2005	Adaptive clustering uses external feedback to improve cluster quality; past experience serves to speed up execution time. An adaptive clustering environment is proposed that uses Q-learning to learn the reward values of successive data clusterings. Adaptive clustering supports the reuse of clusterings by memorizing what worked well in the past. It has the capability of exploring multiple paths in parallel when searching for good clusters. In a case study, we apply adaptive clustering to instance-based learning relying on a distance function modification approach. A distance function adaptation scheme that uses external feedback is proposed and compared with other distance function learning approaches. Experimental results indicate that the use of adaptive clustering leads to significant improvements of instance-based learning techniques, such as k-nearest neighbor classifiers. Moreover, as a by-product a new instance-based learning technique is introduced that classifies examples by solely using cluster representatives; this technique shows high promise in our experimental evaluation.
ICDM	Classifier Fusion Using Shared Sampling Distribution for Boosting.	Costin Barbu,Raja Tanveer Iqbal,Jing Peng	2005	We present a new framework for classifier fusion that uses a shared sampling distribution for obtaining a weighted classifier ensemble. The weight update process is self regularizing as subsequent classifiers trained on the disjoint views rectify the bias introduced by any classifier in preceding iterations. We provide theoretical guarantees that our approach indeed provides results which are better than the case when boosting is performed separately on different views. The results are shown to outperform other classifier fusion strategies on a well known texture image database.
ICDM	Mining Frequent Spatio-Temporal Sequential Patterns.	Huiping Cao,Nikos Mamoulis,David W. Cheung	2005	Many applications track the movement of mobile objects, which can be represented as sequences of timestamped locations. Given such a spatio-temporal series, we study the problem of discovering sequential patterns, which are routes frequently followed by the object. Sequential pattern mining algorithms for transaction data are not directly applicable for this setting. The challenges to address are (i) the fuzziness of locations in patterns, and (ii) the identification of non-explicit pattern instances. In this paper, we define pattern elements as spatial regions around frequent line segments. Our method first transforms the original sequence into a list of sequence segments, and detects frequent regions in a heuristic way. Then, we propose algorithms to find patterns by employing a newly proposed substring tree structure and improving Apriori technique. A performance evaluation demonstrates the effectiveness and efficiency of our approach.
ICDM	Improving Automatic Query Classification via Semi-Supervised Learning.	Steven M. Beitzel,Eric C. Jensen,Ophir Frieder,David D. Lewis,Abdur Chowdhury,Aleksander Kolcz	2005	Accurate topical classification of user queries allows for increased effectiveness and efficiency in general-purpose web search systems. Such classification becomes critical if the system is to return results not just from a general web collection but from topic-specific back-end databases as well. Maintaining sufficient classification recall is very difficult as web queries are typically short, yielding few features per query. This feature sparseness coupled with the high query volumes typical for a large-scale search service makes manual and supervised learning approaches alone insufficient. We use an application of computational linguistics to develop an approach for mining the vast amount of unlabeled data in web query logs to improve automatic topical web query classification. We show that our approach in combination with manual matching and supervised learning allows us to classify a substantially larger proportion of queries than any single technique. We examine the performance of each approach on a real web query stream and show that our combined method accurately classifies 46% of queries, outperforming the recall of best single approach by nearly 20%, with a 7% improvement in overall effectiveness.
ICDM	Pairwise Symmetry Decomposition Method for Generalized Covariance Analysis.	Tsuyoshi Idé	2005	We propose a new theoretical framework for generalizing the traditional notion of covariance. First, we discuss the role of pairwise cross-cumulants by introducing a cluster expansion technique for the cumulant generating function. Next, we introduce a novel concept of symmetry decomposition of probability density functions according to the C_4v group. By utilizing the irreducible representations, generalized covariances are explicitly defined, and their utility is demonstrated using an analytically solvable model.
ICDM	ViVo: Visual Vocabulary Construction for Mining Biomedical Images.	Arnab Bhattacharya,Vebjorn Ljosa,Jia-Yu Pan,Mark R. Verardo,Hyung-Jeong Yang,Christos Faloutsos,Ambuj K. Singh	2005	"Given a large collection of medical images of several conditions and treatments, how can we succinctly describe the characteristics of each setting? For example, given a large collection of retinal images from several different experimental conditions (normal, detached, reattached, etc.), how can data mining help biologists focus on important regions in the images or on the differences between different experimental conditions? If the images were text documents, we could find the main terms and concepts for each condition by existing IR methods (e.g., tf/idf and LSI). We propose something analogous, but for the much more challenging case of an image collection: We propose to automatically develop a visual vocabulary by breaking images into n × n tiles and deriving key tiles (""ViVos"") for each image and condition. We experiment with numerous domain-independent ways of extracting features from tiles (color histograms, textures, etc.), and several ways of choosing characteristic tiles (PCA, ICA). We perform experiments on two disparate biomedical datasets. The quantitative measure of success is classification accuracy: Our ""ViVos"" achieve high classification accuracy (up to 83% for a nine-class problem on feline retinal images). More importantly, qualitatively, our ""ViVos"" do an excellent job as ""visual vocabulary terms"": they have biological meaning, as corroborated by domain experts; they help spot characteristic regions of images, exactly like text vocabulary terms do for documents; and they highlight the differences between pairs of images."
ICDM	Semi-Supervised Mixture of Kernels via LPBoost Methods.	Jinbo Bi,Glenn Fung,Murat Dundar,R. Bharat Rao	2005	We propose an algorithmto construct classification models with a mixture of kernels from labeled and unlabeled data. The derived classifier is a mixture of models, each based on one kernel choice from a library of kernels. The sparse-favoring 1-norm regularization method is employed to restrict the complexity of mixture models and to achieve the sparsity of solutions. By modifying the column generation boosting algorithm LPBoost to a more general linear programming formulation, we are able to efficiently solve mixture-of-kernel problems and automatically select kernel basis functions centered at labeled data as well as unlabeled data. The effectiveness of the proposed approach is proved by experimental results on benchmark datasets.
ICDM	Kernel-Density-Based Clustering of Time Series Subsequences Using a Continuous Random-Walk Noise Model.	Anne Denton	2005	Noise levels in time series subsequence data are typically very high, and properties of the noise differ from those of white noise. The proposed algorithm incorporates a continuous random-walk noise model into kernel-density-based clustering. Evaluation is done by testing to what extent the resulting clusters are predictive of the process that generated the time series. It is shown that the new algorithm not only outperforms partitioning techniques that lead to trivial and unsatisfactory results under the given quality measure, but also improves upon other density-based algorithms. The results suggest that the noise elimination properties of kernel-density-based clustering algorithms can be of significant value for the use of clustering in preprocessing of data.
ICDM	A Levelwise Search Algorithm for Interesting Subspace Clusters.	Haiyun Bian,Raj Bhatnagar	2005	We present a levelwise search algorithm for finding subspace clusters in high dimensional data satisfying various properties besides the commonly used minimum density property. A set of such properties are summarized and a user can choose any of these properties. A lattice is built with all the discovered clusters which enables further analysis and discovery of useful knowledge about the clusters and their inter-relationships.
ICDM	Adaptive Product Normalization: Using Online Learning for Record Linkage in Comparison Shopping.	Mikhail Bilenko,Sugato Basu,Mehran Sahami	2005	The problem of record linkage focuses on determining whether two object descriptions refer to the same underlying entity. Addressing this problem effectively has many practical applications, e.g., elimination of duplicate records in databases and citation matching for scholarly articles. In this paper, we consider a new domain where the record linkage problem is manifested: Internet comparison shopping. We address the resulting linkage setting that requires learning a similarity function between record pairs from streaming data. The learned similarity function is subsequently used in clustering to determine which records are co-referent and should be linked. We present an online machine learning method for addressing this problem, where a composite similarity function based on a linear combination of basis functions is learned incrementally. We illustrate the efficacy of this approach on several real-world datasets from an Internet comparison shopping site, and show that our method is able to effectively learn various distance functions for product data with differing characteristics. We also provide experimental results that show the importance of considering multiple performance measures in record linkage evaluation.
ICDM	Using Information-Theoretic Measures to Assess Association Rule Interestingness.	Julien Blanchard,Fabrice Guillet,Régis Gras,Henri Briand	2005	Assessing rules with interestingness measures is the cornerstone of successful applications of association rule discovery. However, there exists no information-theoretic measure which is adapted to the semantics of association rules. In this article, we present the Directed Information Ratio (DIR), a new rule interestingness measure which is based on information theory. DIR is specially designed for association rules, and in particular it differentiates two opposite rules a → b and a → \mathop b\limits^ - . Moreover, to our knowledge, DIR is the only rule interestingness measure which rejects both independence and (what we call) equilibrium, i.e. it discards both the rules whose antecedent and consequent are negatively correlated, and the rules which have more counter-examples than examples. Experimental studies show that DIR is a very filtering measure, which is useful for association rule post-processing.
ICDM	Usage-Based PageRank for Web Personalization.	Magdalini Eirinaki,Michalis Vazirgiannis	2005	"Recommendation algorithms aim at proposing ""next"" pages to a user based on her current visit and the past users' navigational patterns. In the vast majority of related algorithms, only the usage data are used to produce recommendations, whereas the structural properties of the Web graph are ignored. We claim that taking also into account the web structure and using link analysis algorithms ameliorates the quality of recommendations. In this paper we present UPR, a novel personalization algorithm which combines usage data and link analysis techniques for ranking and recommending web pages to the end user. Using the web site's structure and its usage data we produce personalized navigational graph synopses (prNG) to be used for applying UPR and produce personalized recommendations. Experimental results show that the accuracy of the recommendations is superior to pure usage-based approaches."
ICDM	Bifold Constraint-Based Mining by Simultaneous Monotone and Anti-Monotone Checking.	Mohammad El-Hajj,Osmar R. Zaïane,Paul Nalos	2005	Mining for frequent itemsets can generate an overwhelming number of patterns, often exceeding the size of the original transactional database. One way to deal with this issue is to set filters and interestingness measures. Others advocate the use of constraints to apply to the patterns, either on the form of the patterns or on descriptors of the items in the patterns. However, typically the filtering of patterns based on these constraints is done as a post-processing phase. Filtering the patterns post-mining adds a significant overhead, still suffers from the sheer size of the pattern set and loses the opportunity to exploit those constraints. In this paper we propose an approach that allows the efficientmining of frequent itemsets patterns, while pushing simultaneously both monotone and anti-monotone constraints during and at different strategic stages of the mining process. Our implementation shows a significant improvement when considering the constraints early and a better performance over Dualminer which also considers both types of constraints.
ICDM	Shortest-Path Kernels on Graphs.	Karsten M. Borgwardt,Hans-Peter Kriegel	2005	Data mining algorithms are facing the challenge to deal with an increasing number of complex objects. For graph data, a whole toolbox of data mining algorithms becomes available by defining a kernel function on instances of graphs. Graph kernels based on walks, subtrees and cycles in graphs have been proposed so far. As a general problem, these kernels are either computationally expensive or limited in their expressiveness. We try to overcome this problem by defining expressive graph kernels which are based on paths. As the computation of all paths and longest paths in a graph is NP-hard, we propose graph kernels based on shortest paths. These kernels are computable in polynomial time, retain expressivity and are still positive definite. In experiments on classification of graph models of proteins, our shortest-path kernels show significantly higher classificationaccuracy than walk-based kernels.
ICDM	WARP: Time Warping for Periodicity Detection.	Mohamed G. Elfeky,Walid G. Aref,Ahmed K. Elmagarmid	2005	Periodicity mining is used for predicting trends in time series data. Periodicity detection is an essential process in periodicity mining to discover potential periodicity rates. Existing periodicity detection algorithms do not take into account the presence of noise, which is inevitable in almost every real-world time series data. In this paper, we tackle the problem of periodicity detection in the presence of noise. We propose a new periodicity detection algorithm that deals efficiently with all types of noise. Based on time warping, the proposed algorithm warps (extends or shrinks) the time axis at various locations to optimally remove the noise. Experimental results show that the proposed algorithm out-performs the existing periodicity detection algorithms in terms of noise resiliency.
ICDM	Segment-Based Injection Attacks against Collaborative Filtering Recommender Systems.	Robin D. Burke,Bamshad Mobasher,Runa Bhaumik,Chad Williams	2005	"Significant vulnerabilities have recently been identi- fied in collaborative filtering recommender systems. Researchers have shown that attackers can manipulate a system's recommendations by injecting biased profiles into it. In this paper, we examine attacks that concentrate on a targeted set of users with similar tastes, biasing the system's responses to these users. We show that such attacks are both pragmatically reasonable and also highly effective against both user-based and item-based algorithms. As a result, an attacker can mount such a ""segmented"" attack with little knowledge of the specific system being targeted and with strong likelihood of success."
ICDM	Summarization - Compressing Data into an Informative Representation.	Varun Chandola,Vipin Kumar	2005	In this paper, we formulate the problem of summarization of a dataset of transactions with categorical attributes as an optimization problem involving two objective functions - compaction gain and information loss. We propose metrics to characterize the output of any summarization algorithm. We investigate two approaches to address this problem. The first approach is an adaptation of clustering and the second approach makes use of frequent itemsets from the association analysis domain. We illustrate one application of summarization in the field of network data where we show how our technique can be effectively used to summarize network traffic into a compact but meaningful representation. Specifically, we evaluate our proposed algorithms on the 1998 DARPA Off-line Intrusion Detection Evaluation data and network data generated by SKAION Corp for the ARDA information assurance program.
ICDM	"An Improved Categorization of Classifier's Sensitivity on Sample Selection Bias."	Wei Fan,Ian Davidson,Bianca Zadrozny,Philip S. Yu	2005	"A recent paper categorizes classifier learning algorithms according to their sensitivity to a common type of sample selection bias where the chance of an example being selected into the training sample depends on its feature vector x but not (directly) on its class label y. A classifier learner is categorized as ""local"" if it is insensitive to this type of sample selection bias, otherwise, it is considered ""global"". In that paper, the true model is not clearly distinguished from the model that the algorithm outputs. In their discussion of Bayesian classifiers, logistic regression and hard-margin SVMs, the true model (or the model that generates the true class label for every example) is implicitly assumed to be contained in the model space of the learner, and the true class probabilities and model estimated class probabilities are assumed to asymptotically converge as the training data set size increases. However, in the discussion of naive Bayes, decision trees and soft-margin SVMs, the model space is assumed not to contain the true model, and these three algorithms are instead argued to be ""global learners"". We argue that most classifier learners may or may not be affected by sample selection bias; this depends on the dataset as well as the heuristics or inductive bias implied by the learning algorithm and their appropriateness to the particular dataset."
ICDM	Effective Estimation of Posterior Probabilities: Explaining the Accuracy of Randomized Decision Tree Approaches.	Wei Fan,Ed Greengrass,Joe McCloskey,Philip S. Yu,Kevin Drummey	2005	There has been increasing number of independently proposed randomization methods in different stages of decision tree construction to build multiple trees. Randomized decision tree methods have been reported to be significantly more accurate than widely-accepted single decision trees, although the training procedure of some methods incorporates a surprisingly random factor and therefore opposes the generally accepted idea of employing gain functions to choose optimum features at each node and compute a single tree that fits the data. One important question that is not well understood yet is the reason behind the high accuracy. We provide an insight based on posterior probability estimations. We first establish the relationship between effective posterior probability estimation and effective loss reduction. We argue that randomized decision tree methods effectively approximate the true probability distribution using the decision tree hypothesis space. We conduct experiments using both synthetic and real-world datasets under both 0-1 and cost-sensitive loss functions.
ICDM	Making Subsequence Time Series Clustering Meaningful.	Jason R. Chen	2005	"Recently, the startling claim was made that sequential time series clustering is meaningless. This has important consequences for a significant amount of work in the literature, since such a claim invalidates this work's contribution. In this paper, we show that sequential time series clustering is not meaningless, and that the problem highlighted in these works stem from their use of the Euclidean distance metric as the distance measure in the subsequence vector space. As a solution, we consider quite a general class of time series, and propose a regime based on two types of similarity that can exist between subsequence vectors, which give rise naturally to an alternative distance measure to Euclidean distance in the subsequence vector space. We show that, using this alternative distance measure, sequential time series clustering can indeed be meaningful. We repeat a key experiment in the work on which the ""meaningless"" claim was based, and show that our method leads to a successful clustering outcome."
ICDM	A Computational Framework for Taxonomic Research: Diagnosing Body Shape within Fish Species Complexes.	Yixin Chen,Henry L. Bart Jr.,Shuqing Huang,Huimin Chen	2005	"It is estimated that ninety percent of the world's species have yet to be discovered and described. The main reason for the slow pace of new species description is that the science of taxonomy, as traditionally practiced, can be very laborious. To formally describe a new species, taxonomists have to manually gather and analyze data from large numbers of specimens, often from broad geographic areas, and identify the smallest subset of external body characters that uniquely diagnoses the new species as distinct from all its known relatives. In this paper, we use an automated feature selection and classification approach to address the taxonomic impediment in new species discovery. The experiments on a taxonomic problem involving species of suckers in the genus Carpiodes demonstrate promising results."
ICDM	Labeling Unclustered Categorical Data into Clusters Based on the Important Attribute Values.	Hung-Leng Chen,Kun-Ta Chuang,Ming-Syan Chen	2005	Sampling has been recognized as an important technique to improve the efficiency of clustering. However, with sampling applied, those points which are not sampled will not have their labels. Although there is a straightforward approach in the numerical domain, the problem of how to allocate those unlabeled data points into proper clusters remains as a challenging issue in the categorical domain. In this paper, a mechanism named MAximal Resemblance Data Labeling (abbreviated as MARDL) is proposed to allocate each unlabeled data point into the corresponding appropriate cluster based on the novel categorical clustering representative, namely, Node Importance Representative(abbreviated as NIR), which represents clusters by the importance of attribute values. MARDL has two advantages: (1) MARDL exhibits high execution efficiency; (2) after each unlabeled data is allocated into the proper cluster, MARDL preserves clustering characteristics, i.e., high intra-cluster similarity and low inter-cluster similarity. MARDL is empirically validated via real and synthetic data sets, and is shown to be not only more efficient than prior methods but also attaining results of better quality.
ICDM	Privacy Preserving Data Classification with Rotation Perturbation.	Keke Chen,Ling Liu	2005	Data perturbation techniques are one of the most popular models for privacy preserving data mining [3, 1]. It is especially convenient for applications where the data owners need to export/publish the privacy-sensitive data. A data perturbation procedure can be simply described as follows. Before the data owner publishes the data, they randomly change the data in certain way to disguise the sensitive information while preserving the particular data property that is critical for building the data models. Several perturbation techniques have been proposed recently, among which the most typical ones are randomization approach [3] and condensation approach [1].
ICDM	Fast Frequent String Mining Using Suffix Arrays.	Johannes Fischer,Volker Heun,Stefan Kramer	2005	We present a method to mine strings that are frequent in one database and infrequent in another. The method uses suffix- and lcp-arrays that can be computed extremely fast and space efficiently, and further exhibit a good locality behavior. Experiments with several biologically relevant data sets show that our approach outperforms existing methods in terms of time and space.
ICDM	Privacy-Preserving Frequent Pattern Mining across Private Databases.	Ada Wai-Chee Fu,Raymond Chi-Wing Wong,Ke Wang	2005	Privacy consideration has much significance in the application of data mining. It is very important that the privacy of individual parties will not be exposed when data mining techniques are applied to a large collection of data about the parties. In many scenarios such as data warehousing or data integration, data from the different parties form a many-to-many schema. This paper addresses the problem of privacy-preserving frequent pattern mining in such a schema across two dimension sites. We assume that sites are not trusted and they are semi-honest. Our method is based on the concept of semi-join and does not involve data encryption which is used in most previous work. Experiments are conducted to study the efficiency of the proposed models.
ICDM	CoLe: A Cooperative Data Mining Approach and Its Application to Early Diabetes Detection.	Jie Gao,Jörg Denzinger,Robert C. James	2005	We present CoLe, a cooperative data mining approach for discovering hybrid knowledge. It employs multiple different data mining algorithms, and combines results from them to enhance the mined knowledge. For our medical application area, we analyse several focusing strategies that allowed us to gain medically significant results.
ICDM	Feature Selection for Building Cost-Effective Data Stream Classifiers.	Like Gao,Xiaoyang Sean Wang	2005	A stream classifier is a decision model that assigns a class label to a data stream, based on its arriving data. Various features of the stream can be used in the classifier, each of which may have different relevance to the classification task and different cost in obtaining its value. As time passes by, some less costly features may become more relevant, but the time needed for decision may be considered as a cost. A challenge is how to balance the different costs when building a cost-effective classifier. This paper proposes a new feature selection strategy that extends the traditional Relief algorithm in two aspects: (1) estimate the classification cost associated with each feature, and (2) order all the features with a score that combines both cost estimation and classification relevance. A classifier is then built with the selected features using a traditional classification method. Experimental results show that classifiers constructed with this strategy are indeed cost effective.
ICDM	A Scalable Collaborative Filtering Framework Based on Co-Clustering.	Thomas George,Srujana Merugu	2005	Collaborative filtering-based recommender systems have become extremely popular in recent years due to the increase in web-based activities such as e-commerce and online content distribution. Current collaborative filtering (CF) techniques such as correlation and SVD based methods provide good accuracy, but are computationally expensive and can be deployed only in static off-line settings. However, a number of practical scenarios require dynamic real-time collaborative filtering that can allow new users, items and ratings to enter the system at a rapid rate. In this paper, we consider a novel CF approach based on a recently proposed weighted co-clustering algorithm [1] that involves simultaneous clustering of users and items. We design incremental and parallel versions of the co-clustering algorithm and use it to build an efficient real-time CF framework. Empirical evaluation demonstrates that our approach provides an accuracy comparable to that of the correlation and matrix factorization based approaches at a much lower computational cost.
ICDM	An Algorithm for In-Core Frequent Itemset Mining on Streaming Data.	Ruoming Jin,Gagan Agrawal	2005	"Frequent itemset mining is a core data mining operation and has been extensively studied over the last decade. This paper takes a new approach for this problem and makes two major contributions. First, we present a one pass algorithm for frequent itemset mining, which has deterministic bounds on the accuracy, and does not require any out-of-core summary structure. Second, because our one pass algorithm does not produce any false negatives, it can be easily extended to a two pass accurate algorithm. Our two pass algorithm is very memory efficient, and allows mining of datasets with large number of distinct items and/or very low support levels. Our detailed experimental evaluation on synthetic and real datasets shows the following. First, our one pass algorithm is very accurate in practice. Second, our algorithm requires significantly lower memory than Manku and Motwani's one pass algorithm and the multi-pass Apriori algorithm. Our two pass algorithm outperforms Apriori and FP-tree when the number of distinct items is large and/or support levels are very low. In other cases, it is quite competitive, with possible exception of cases where the average length of frequent itemsets is quite high."
ICDM	Text Classification with Evolving Label-Sets.	Shantanu Godbole,Ganesh Ramakrishnan,Sunita Sarawagi	2005	We introduce the evolving label-set problem encountered in building real-world text classification systems. This problem arises when a text classification system trained on a label-set encounters documents of unseen classes at deployment time. We design a Class-Detector module that monitors unlabeled data, detects new classes, and suggests them to the administrator for inclusion in the label-set. We propose abstractions that group together tokens under human understandable concepts and provide a mechanism of assigning importance to unseen terms. We present generative algorithms leveraging the notion of support of documents in a model for (1) selecting documents of proposed new classes, and (2) automatically triggering detection of new classes. Experiments on three real world taxonomies show that our methods select new class documents with high precision, and trigger emergence of new classes with low false-positive and false-negative rates.
ICDM	Stability of Feature Selection Algorithms.	Alexandros Kalousis,Julien Prados,Melanie Hilario	2005	With the proliferation of extremely high-dimensional data, feature selection algorithms have become indispensable components of the learning process. Strangely, despite extensive work on the stability of learning algorithms, the stability of feature selection algorithms has been relatively neglected. This study is an attempt to fill that gap by quantifying the sensitivity of feature selection algorithms to variations in the training set. We assess the stability of feature selection algorithms based on the stability of the feature preferences that they express in the form of weights-scores, ranks, or a selected feature subset. We examine a number of measures to quantify the stability of feature preferences and propose an empirical way to estimate them. We perform a series of experiments with several feature selection algorithms on a set of proteomics datasets. The experiments allow us to explore the merits of each stability measure and create stability profiles of the feature selection algorithms. Finally we show how stability profiles can support the choice of a feature selection algorithm.
ICDM	Supervised Ordering - An Empirical Survey.	Toshihiro Kamishima,Hideto Kazawa,Shotaro Akaho	2005	Ordered lists of objects are widely used as representational forms. Such ordered objects include Web search results or bestseller lists. In spite of their importance, methods of processing orders have received little attention. However, research concerning orders has recently become common; in particular, researchers have developed various methods for the task of Supervised Ordering to acquire functions for object sorting from example orders. Here, we give a unified view of these methods and our new one, and empirically survey their merits and demerits.
ICDM	Categorization and Keyword Identification of Unlabeled Documents.	Ning Kang,Carlotta Domeniconi,Daniel Barbará	2005	In this paper we first propose a global unsupervised feature selection approach for text, based on frequent itemset mining. As a result, each document is represented as a set of words that co-occur frequently in the given corpus of documents. We then introduce a locally adaptive clustering algorithm, designed to estimate (local) word relevance and, simultaneously, to group the documents. We present experimental results to demonstrate the feasibility of our approach. Furthermore, the analysis of the weights credited to terms provides evidence that the identified keywords can guide the process of label assignment to clusters. We take into consideration both spam email filtering and general classification datasets. Our analysis of the distribution of weights in the two cases provides insights on how the spam problem distinguishes from the general classification case.
ICDM	A Framework for Semi-Supervised Learning Based on Subjective and Objective Clustering Criteria.	Maria Halkidi,Dimitrios Gunopulos,Nitin Kumar,Michalis Vazirgiannis,Carlotta Domeniconi	2005	In this paper, we propose a semi-supervised framework for learning a weighted Euclidean subspace, where the best clustering can be achieved. Our approach capitalizes on user-constraints and the quality of intermediate clustering results in terms of its structural properties. It uses the clustering algorithm and the validity measure as parameters.
ICDM	Gradual Model Generator for Single-Pass Clustering.	Ismo Kärkkäinen,Pasi Fränti	2005	We present an algorithm for generating a mixture model from a data set by converting the data into a model. The method is applicable when only part of the data fits in the main memory at the same time. The generated model is a Gaussian mixture model but the algorithm can be adapted to other types of models, too. The user cannot specify the size of the generated model. We also introduce a post-processing method, which can reduce the size of the model without using the original data. This will result in a more compact model with fewer components, but with approximately the same representation accuracy as the original model. Our comparisons show that the algorithm produces good results and is quite efficient. The whole process requires only 0.5-10% of the time spent by the expectation-maximization algorithm.
ICDM	HOT SAX: Efficiently Finding the Most Unusual Time Series Subsequence.	Eamonn J. Keogh,Jessica Lin,Ada Wai-Chee Fu	2005	In this work, we introduce the new problem of finding time series discords. Time series discords are subsequences of a longer time series that are maximally different to all the rest of the time series subsequences. They thus capture the sense of the most unusual subsequence within a time series. Time series discords have many uses for data mining, including improving the quality of clustering, data cleaning, summarization, and anomaly detection. As we will show, discords are particularly attractive as anomaly detectors because they only require one intuitive parameter (the length of the subsequence) unlike most anomaly detection algorithms that typically require many parameters. We evaluate our work with a comprehensive set of experiments. In particular, we demonstrate the utility of discords with objective experiments on domains as diverse as Space Shuttle telemetry monitoring, medicine, surveillance, and industry, and we demonstrate the effectiveness of our discord discovery algorithm with more than one million experiments, on 82 different datasets from diverse domains.
ICDM	AMIOT: Induced Ordered Tree Mining in Tree-Structured Databases.	Shohei Hido,Hiroyuki Kawano	2005	Frequent subtree mining has become increasingly important in recent years. In this paper, we present AMIOT algorithm to discover all frequent ordered subtrees in a tree-structured database. In order to avoid the generation of infrequent candidate trees, we propose the techniques such as right-and-left tree join and serial tree extension. Proposed methods enumerate only the candidate trees with high probability of being frequent without any duplications. The experiments on synthetic dataset and XML database show that AMIOT reduces redundant candidate trees and outperforms FREQT algorithm by up to five times in execution time.
ICDM	Orthogonal Neighborhood Preserving Projections.	Effrosini Kokiopoulou,Yousef Saad	2005	Orthogonal Neighborhood Preserving Projections (ONPP) is a linear dimensionality reduction technique which attempts to preserve both the intrinsic neighborhood geometry of the data samples and the global geometry. The proposed technique constructs a weighted data graph where the weights are constructed in a data-driven fashion, similarly to Locally Linear Embedding (LLE). A major difference with the standard LLE where the mapping between the input and the reduced spaces is implicit, is that ONPP employs an explicit linear mapping between the two. As a result, and in contrast with LLE, handling new data samples becomes straightforward, as this amounts to a simple linear transformation. ONPP shares some of the properties of Locality Preserving Projections (LPP). Both ONPP and LPP rely on a k-nearest neighbor graph in order to capture the data topology. However, our algorithm inherits the characteristics of LLE in preserving the structure of local neighborhoods, while LPP aims at preserving only locality without specifically aiming at preserving the geometric structure. This feature makes ONPP an effective method for data visualization. We provide ample experimental evidence to demonstrate the advantageous characteristics of ONPP, using well known synthetic test cases as well as real life data from computational biology and computer vision.
ICDM	Focused Community Discovery.	Kirsten Hildrum,Philip S. Yu	2005	We present a new approach to community discovery. Community discovery usually partitions the graph into communities or clusters. Focused community discovery allows the searcher to specify start points of interest, and find the community of those points. Focused search allows for a much more scalable algorithm in which the time depends only on the size of the community, and not on the number of nodes in the graph, and so is scalable to arbitrarily large graphs. Furthermore, our algorithm is robust to imperfect data, such as extra or missing edges in the graph. We show the effectiveness of our algorithm using both synthetic graphs and on the real-life Livejournal friends graph, a publicly-available social network consisting of over two million users and 13 million edges.
ICDM	Higher-Order Web Link Analysis Using Multilinear Algebra.	Tamara G. Kolda,Brett W. Bader,Joseph P. Kenny	2005	Linear algebra is a powerful and proven tool in web search. Techniques, such as the PageRank algorithm of Brin and Page and the HITS algorithm of Kleinberg, score web pages based on the principal eigenvector (or singular vector) of a particular non-negative matrix that captures the hyperlink structure of the web graph. We propose and test a new methodology that uses multilinear algebra to elicit more information from a higher-order representation of the hyperlink graph. We start by labeling the edges in our graph with the anchor text of the hyperlinks so that the associated linear algebra representation is a sparse, three-way tensor. The first two dimensions of the tensor represent the web pages while the third dimension adds the anchor text. We then use the rank-1 factors of a multilinear PARAFAC tensor decomposition, which are akin to singular vectors of the SVD, to automatically identify topics in the collection along with the associated authoritative web pages.
ICDM	Suppressing Data Sets to Prevent Discovery of Association Rules.	Ayça Azgin Hintoglu,Ali Inan,Yücel Saygin,Mehmet Keskinöz	2005	Enterprises have been collecting data for many reasons including better customer relationship management, and high-level decision making. Public safety was another motivation for large-scale data collection efforts initiated by government agencies. However, such widespread data collection efforts coupled with powerful data analysis tools raised concerns about privacy. This is due to the fact that collected data may contain confidential information. One method to ensure privacy is to selectively hide confidential information from the data sets to be disclosed. In this paper, we focus on hiding confidential correlations. We introduce a heuristic to reduce the information loss and propose a blocking method that prevents discovery of confidential correlations while preserving the usefulness of the data set.
ICDM	Making Logistic Regression a Core Data Mining Tool with TR-IRLS.	Paul Komarek,Andrew W. Moore	2005	Binary classification is a core data mining task. For large datasets or real-time applications, desirable classifiersare accurate, fast, and need no parameter tuning. We present a simple implementation of logistic regression that meets these requirements. A combination of regularization, truncated Newton methods, and iteratively re-weighted least squares make it faster and more accurate than modern SVM implementations, and relatively insensitive to parameters. It is robust to linear dependencies and some scaling problems, making most data preprocessing unnecessary.
ICDM	Effective and Efficient Distributed Model-Based Clustering.	Hans-Peter Kriegel,Peer Kröger,Alexey Pryakhin,Matthias Schubert	2005	In many companies data is distributed among several sites, i.e. each site generates its own data and manages its own data repository. Analyzing and mining these distributed sources requires distributed data mining techniques to find global patterns representing the complete information. The transmission of the entire local data set is often unacceptable because of performance considerations, privacy and security aspects, and bandwidth constraints. Traditional data mining algorithms, demanding access to complete data, are not appropriate for distributed applications. Thus, there is a need for distributed data mining algorithms in order to analyze and discover new knowledge in distributed environments. One of the most important data mining tasks is clustering which aims at detecting groups of similar data objects. In this paper, we propose a distributed model-based clustering algorithm that uses EM for detecting local models in terms of mixtures of Gaussian distributions. We propose an efficient and effective algorithm for deriving and merging these local Gaussian distributions to generate a meaningful global model. In a broad experimental evaluation we show that our framework is scalable in a highly distributed environment.
ICDM	A Generic Framework for Efficient Subspace Clustering of High-Dimensional Data.	Hans-Peter Kriegel,Peer Kröger,Matthias Renz,Sebastian H. R. Wurst	2005	Subspace clustering has been investigated extensively since traditional clustering algorithms often fail to detect meaningful clusters in high-dimensional data spaces. Many recently proposed subspace clustering methods suffer from two severe problems: First, the algorithms typically scale exponentially with the data dimensionality and/or the subspace dimensionality of the clusters. Second, for performance reasons, many algorithms use a global density threshold for clustering, which is quite questionable since clusters in subspaces of significantly different dimensionality will most likely exhibt significantly varying densities. In this paper, we propose a generic framework to overcome these limitations. Our framework is based on an efficient filter-refinement architecture that scales at most quadratic w.r.t. the data dimensionality and the dimensionality of the subspace clusters. It can be applied to any clustering notions including notions that are based on a local density threshold. A broad experimental evaluation on synthetic and real-world data empirically shows that our method achieves a significant gain of runtime and quality in comparison to state-of-the-art subspace clustering algorithms.
ICDM	Hierarchical Density-Based Clustering of Uncertain Data.	Hans-Peter Kriegel,Martin Pfeifle	2005	The hierarchical density-based clustering algorithm OPTICS has proven to help the user to get an overview over large data sets. When using OPTICS for analyzing uncertain data which naturally occur in many emerging application areas, e.g. location based services, or sensor databases, the similarity between uncertain objects has to be expressed by one numerical distance value. Based on such single-valued distance functions OPTICS, like other standard data mining algorithms, can work without any changes. In this paper, we propose to express the similarity between two fuzzy objects by distance probability functions which assign a probability value to each possible distance value. Contrary to the traditional approach, we do not extract aggregated values from the fuzzy distance functions but enhance OPTICS so that it can exploit the full information provided by these functions. The resulting algorithm FOPTICS helps the user to get an overview over a large set of fuzzy objects.
ICDM	Semi-Supervised Clustering with Metric Learning Using Relative Comparisons.	Nimit Kumar,Krishna Kummamuru,Deepa Paranjpe	2005	Semi-supervised clustering algorithms partition a given data set using limited supervision from the user. In this paper, we propose a clustering algorithmthat uses supervision in terms of relative comparisons, viz., is closer to than to . The success of a clustering algorithm also depends on the kind of dissimilarity measure. The proposed clustering algorithm learns the underlying dissimilarity measure while finding compact clusters in the given data set. Through our experimental studies on high-dimensional textual data sets, we demonstrate that the proposed algorithm achieves higher accuracy than the algorithms using pairwise constraints for supervision.
ICDM	On Learning Asymmetric Dissimilarity Measures.	Krishna Kummamuru,Raghu Krishnapuram,Rakesh Agrawal	2005	"Many practical applications require that distance measures to be asymmetric and context-sensitive. We introduce Context-sensitive Learnable Asymmetric Dissimilarity (CLAD) measures, which are defined to be a weighted sum of a fixed number of dissimilarity measures where the associated weights depend on the point from which the dissimilarity is measured. The parameters used in defining the measure capture the global relationships among the features. We provide an algorithm to learn the dissimilarity measure automatically from a set of user specified comparisons in the form ""x is closer to y than to z,"" and study its performance. The experimental results show that the proposed algorithm outperforms other approaches due to the context sensitive nature of the CLAD measures."
ICDM	Partial Ensemble Classifiers Selection for Better Ranking.	Jin Huang,Charles X. Ling	2005	Ranking is an important task in data mining and knowledge discovery. We propose a novel approach called PECS algorithm to improve the overall ranking performance of a given ensemble. We formally analyse the sufficient and necessary condition under whichPECS algorithm can effectively improve ensemble ranking performance. The experiments with real-world data sets show that this new approach achieves significant improvements in ranking over the original Bagging and Adaboost ensembles.
ICDM	Triple Jump Acceleration for the EM Algorithm.	Han-Shen Huang,Bou-Ho Yang,Chun-Nan Hsu	2005	This paper presents the triple jump framework for accelerating the EM algorithm and other bound optimization methods. The idea is to extrapolate the third search point based on the previous two search points found by regular EM. As the convergence rate of regular EM becomes slower, the distance of the triple jump will be longer, and thus provide higher speedup for data sets where EM converges slowly. Experimental results show that the triple jump framework significantly outperforms EM and other acceleration methods of EM for a variety of probabilistic models, especially when the data set is sparse. The results also show that the triple jump framework is particularly effective for Cluster Models.
ICDM	Hierarchy-Regularized Latent Semantic Indexing.	Yi Huang,Kai Yu,Matthias Schubert,Shipeng Yu,Volker Tresp,Hans-Peter Kriegel	2005	Organizing textual documents into a hierarchical taxonomy is a common practice in knowledge management. Beside textual features, the hierarchical structure of directories reflect additional and important knowledge annotated by experts. It is generally desired to incorporate this information into text mining processes. In this paper, we propose hierarchy-regularized latent semantic indexing, which encodes the hierarchy into a similarity graph of documents and then formulates an optimization problem mapping each document into a low dimensional vector space. The new feature space preserves the intrinsic structure of the original taxonomy and thus provides a meaningful basis for various learning tasks like visualization and classification. Our approach employs the information about class proximity and class specificity, and can naturally cope with multi-labeled documents. Our empirical studies show very encouraging results on two real-world data sets, the new Reuters (RCV1) benchmark and the Swissprot protein database.
ICDM	Partial Elastic Matching of Time Series.	Longin Jan Latecki,Vasileios Megalooikonomou,Qiang Wang,Rolf Lakämper,Chotirat (Ann) Ratanamahatana,Eamonn J. Keogh	2005	We consider the problem of elastic matching of time series. We propose an algorithm that determines a subsequence of a target time series that best matches a query series. In the proposed algorithm we map the problem of the best matching subsequence to the problem of a cheapest path in a DAG (directed acyclic graph). The proposed approach allows us to also compute the optimal scale and translation of time series values, which is a nontrivial problem in the case of subsequence matching.
ICDM	CLUGO: A Clustering Algorithm for Automated Functional Annotations Based on Gene Ontology.	In-Yee Lee,Jan-Ming Ho,Ming-Syan Chen	2005	We address the issue of providing highly informative and comprehensive annotations using information revealed by the structured vocabularies of Gene Ontology (GO). For a target, a set of candidate terms for inferring target properties is collected and form a unique distribution on the GO directed acyclic graph (DAG). We propose a novel ontology-based clustering algorithm — CLUGO, which considers GO hierarchical characteristics and the clustering of term distributions. By identifying significant groups in the distributions, CLUGO assigns comprehensive and correct annotations for a target. According to the results of experiments with automated sequence functional annotations, CLUGO represents a considerable improvement over our previous work — GOMIT in terms of recall while maintaining a similar level of precision. We conclude that given a GO candidate term distribution, CLUGO is an efficient ontology-based clustering algorithm for selecting comprehensive and correct annotations.
ICDM	Extracting Frequent Subsequences from a Single Long Data Sequence: A Novel Anti-Monotonic Measure and a Simple On-Line Algorithm.	Koji Iwanuma,Ryuichi Ishihara,Yo Takano,Hidetomo Nabeshima	2005	In this paper, we study frequent-subsequence extraction from a single very-long data-sequence. First we propose a novel frequency measure, called the total frequency, for counting multiple occurrences of a sequential pattern in a single data sequence. The total frequency is anti-monotonic, and makes it possible to count up pattern occurrences without duplication. Moreover the total frequency has a good property for implementation based on the dynamic programming strategy. Second we give a simple on-line algorithm for a specialized subsequence extraction problem, i.e., a problem with the infinite window-length. This specialized problem is considered to be a relaxation of the general-case problem, thus this fast on-line algorithm is important from the view of practical applications.
ICDM	Finding Maximal Frequent Itemsets over Online Data Streams Adaptively.	Daesu Lee,Wonsuk Lee	2005	Due to the characteristics of a data stream, it is very important to confine the memory usage of a data mining process regardless of the amount of information generated in the data stream. For this purpose, this paper proposes a CP-tree (Compressed-prefix tree)that can be effectively used in finding either frequent or maximal frequent itemsets over an online data stream. Unlike a prefix tree, a node of a CP-tree can maintain the information of several itemsets together. Based on this characteristic, the size of a CP-tree can be flexibly controlled by merging or splitting nodes. In this paper, a mining method employing a CP-tree is proposed and an adaptive memory utilization scheme is also presented in order to maximize the mining accuracy of the proposed method for confined memory space at all times. Finally, the performance of the proposed method is analyzed by a series of experiments to identify its various characteristics.
ICDM	FS: A Random Walk Based Free-Form Spatial Scan Statistic for Anomalous Window Detection.	Vandana Pursnani Janeja,Vijayalakshmi Atluri	2005	Often, it is required to identify anomalous windows over a spatial region that reflect unusual rate of occurrence of a specific event of interest. A spatial scan statistic essentially considers a scan window, and identifies anomalous windows by moving the scan window in the region. While spatial scan statistic has been successful, earlier proposals suffer from two limitations: (i) They resrict the scan window to be of a regular shape (e.g., circle, rectangle, cylinder). However, the region of anomaly, in general, is not necessarily of a regular shape. (ii) They take into account autocorrelation among spatial data, but not spatial heterogeneity. As a result, they often result in inaccurate anomalous windows. To address these limitations, we propose a random walk based Free-Form Spatial Scan Statistic (FS³). Application of FS³ on real datasets has shown that it can identify more refined anomalous windows with better likelihood ratio of it being an anomaly, than those identified by earlier spatial scan statistic approaches.
ICDM	Mining Minimal Distinguishing Subsequence Patterns with Gap Constraints.	Xiaonan Ji,James Bailey,Guozhu Dong	2005	Discovering contrasts between collections of data is an important task in data mining. In this paper, we introduce a new type of contrast pattern, called a Minimal Distinguishing Subsequence (MDS). An MDS is a minimal subsequence that occurs frequently in one class of sequences and infrequently in sequences of another class. It is a natural way of representing strong and succinct contrast information between two sequential datasets and can be useful in applicationssuch as protein comparison, document comparison and building sequential classification models. Mining MDS patterns is a challenging task and is significantly different from mining contrasts between relational/transactional data. One particularly important type of constraint that can be integrated into the mining process is the gap constraint. We present an efficient algorithm called ConSGapMiner (Contrast Sequences with Gap Miner), to mine all MDSs satisfying a minimum and maximum gap constraint, plus a maximum length constraint. It employs highly efficient bitset and boolean operations, for powerful gap-based pruning within a prefix growth framework. A performance evaluation with both sparse and dense datasets, demonstrates the scalability of ConSGapMiner and shows its ability to mine patterns from high dimensional datasets at low supports.
ICDM	An Optimal Linear Time Algorithm for Quasi-Monotonic Segmentation.	Daniel Lemire,Martin Brooks,Yuhong Yan	2005	Monotonicity is a simple yet significant qualitative characteristic. We consider the problem of segmenting an array in up to K segments. We want segments to be as monotonic as possible and to alternate signs. We propose a quality metric for this problem, present an optimal linear time algorithm based on novel formalism, and compare experimentally its performance to a linear time top-down regression algorithm. We show that our algorithm is faster and more accurate. Applications include pattern recognition and qualitative modeling.
ICDM	CanTree: A Tree Structure for Efficient Incremental Mining of Frequent Patterns.	Carson Kai-Sang Leung,Quamrul I. Khan,Tariqul Hoque	2005	Since its introduction, frequent-pattern mining has been the subject of numerous studies, including incremental updating. Many existing incremental mining algorithms are Apriori-based, which are not easily adoptable to FP-tree based frequent-pattern mining. In this paper, we propose a novel tree structure, called CanTree (Canonical-order Tree), that captures the content of the transaction database and orders tree nodes according to some canonical order. By exploiting its nice properties, the CanTree can be easily maintained when database transactions are inserted, deleted, and/or modified. For example, the CanTree does not require adjustment, merging, and/or splitting of tree nodes during maintenance. No rescan of the entire updated database or reconstruction of a new tree is needed for incremental updating. Experimental results show the effectiveness of our CanTree.
ICDM	Mining Ontological Knowledge from Domain-Specific Text Documents.	Xing Jiang,Ah-Hwee Tan	2005	Traditional text mining systems employ shallow parsing techniques and focus on concept extraction and taxonomic relation extraction. This paper presents a novel system called CRCTOL for mining rich semantic knowledge in the form of ontology from domain-specific text documents. By using a full text parsing technique and incorporating both statistical and lexico-syntactic methods, the knowledge extracted by our system is more concise and contains a richer semantics compared with alternative systems. We conduct a case study wherein CRCTOL extracts ontological knowledge, specifically key concepts and semantic relations, from a terrorism domain text collection. Quantitative evaluation, by comparing with a state-of-the-art ontology learning system known as Text-To-Onto, has shown that CRCTOL produces much better precision and recall for both concept and relation extraction, especially from sentences with complex structures.
ICDM	Average Number of Frequent (Closed) Patterns in Bernouilli and Markovian Databases.	Loïck Lhote,François Rioult,Arnaud Soulet	2005	Average Number of Frequent (Closed) Patterns in Bernouilli and Markovian Databases.
ICDM	Mining Patterns That Respond to Actions.	Yuelong Jiang,Ke Wang,Alexander Tuzhilin,Ada Wai-Chee Fu	2005	Data mining focuses on patterns that summarize the data. In this paper, we focus on mining patterns that could change the state by responding to opportunities of actions.
ICDM	Learning Instance Greedily Cloning Naive Bayes for Ranking.	Liangxiao Jiang,Harry Zhang	2005	"Naive Bayes (simply NB) [12] has been widely used in machine learning and data mining as a simple and effective classification algorithm. Since its conditional independence assumption is rarely true, researchers have made a substantial amount of effort to improve naive Bayes. The related research work can be broadly divided into two approaches: eager learning and lazy learning, depending on when the major computation occurs. Different from eager approach, the key idea for extending naive Bayes from the lazy approach is to learn a naive Bayes for each testing example. In recent years, some lazy extensions of naive Bayes have been proposed. For example, SNNB [18], LWNB [7], and LBR [19]. All are aiming at improving the classification accuracy of naive Bayes. In many real-world machine learning and data mining applications, however, an accurate ranking is more desirable than an accurate classification. Responding to this fact, we present a lazy learning algorithm called instance greedily cloning naive Bayes (simply IGCNB) in this paper. Our motivation is to improve naive Bayes' ranking performance measured by AUC [4, 14]. We experimentally tested our algorithm, using the whole 36 UCI datasets recommended by Weka [1], and compared it to C4.4 [16], NB [12], SNNB [18] and LWNB [7]. The experimental results show that our algorithm outperforms all the other algorithms used to compare significantly in yielding accurate ranking."
ICDM	Balancing Exploration and Exploitation: A New Algorithm for Active Machine Learning.	Thomas Takeo Osugi,Kun Deng,Stephen D. Scott	2005	Balancing Exploration and Exploitation: A New Algorithm for Active Machine Learning.
ICDM	Multi-Stage Classification.	Ted E. Senator	2005	While much research has focused on methods for evaluating and maximizing the accuracy of classifiers either individually or in ensembles, little effort has been devoted to analyzing how classifiers are typically deployed in practice. In many domains, classifiers are used as part of a multi-stage process that increases accuracy at the expense of more data collection and/or more processing resources as the likelihood of a positive class label increases. This paper systematically explores the tradeoffs inherent in constructing these multi-stage classifiers from a series of increasingly accurate and expensive individual classifiers, considering a variety of metrics such as accuracy, cost/benefit ratio, and lift. It suggests architectures appropriate for both independent instances and for highly linked data.
ICDM	Process Diagnosis via Electrical-Wafer-Sorting Maps Classification.	Federico Di Palma,Giuseppe De Nicolao,Guido Miraglia,Oliver M. Donzelli	2005	The commonality analysis is a proven tool for fault detection in semiconductor manufacturing. This methodology extracts subsets of production lots from all the available data. Then, data mining techniques are used only on the selected data. This approach loses part of the available information and does not discriminate among the lots. The new methodology performance the automatic classificationof the electrical wafer test maps in order to identify the classes of failure present in the production lots. Subsequently, the proposed procedure uses the process history of each wafer to create a list of the root cause candidates. This methodology is the core of the software tool ACID which is currently used for process diagnosis at the Agrate site of the ST Microelectronics. A real analysis is presented.
ICDM	Finding Representative Set from Massive Data.	Feng Pan,Wei Wang,Anthony K. H. Tung,Jiong Yang	2005	In the information age, data is pervasive. In some applications, data explosion is a significant phenomenon. The massive data volume poses challenges to both human users and computers. In this project, we propose a new model for identifying representative set from a large database. A representative set is a special subset of the original dataset, which has three main characteristics: It is significantly smaller in size compared to the original dataset. It captures the most information from the original dataset compared to other subsets of the same size. It has low redundancy among the representatives it contains. We use information-theoretic measures such as mutual information and relative entropy to measure the representativeness of the representative set. We first design a greedy algorithm and then present a heuristic algorithm that delivers much better performance. We run experiments on two real datasets and evaluate the effectiveness of our representative set in terms of coverage and accuracy. The experiments show that our representative set attains expected characteristics and captures information more efficiently.
ICDM	Parameter-Free Spatial Data Mining Using MDL.	Spiros Papadimitriou,Aristides Gionis,Panayiotis Tsaparas,Risto A. Väisänen,Heikki Mannila,Christos Faloutsos	2005	"Consider spatial data consisting of a set of binary features taking values over a collection of spatial extents (grid cells). We propose a method that simultaneously finds spatial correlation and feature co-occurrence patterns, without any parameters. In particular, we employ the Minimum Description Length (MDL) principle coupled with a natural way of compressing regions. This defines what ""good"" means: a feature co-occurrence pattern is good, if it helps us better compress the set of locations for these features. Conversely, a spatial correlation is good, if it helps us better compress the set of features in the corresponding region. Our approach is scalable for large datasets (both number of locations and of features). We evaluate our method on both real and synthetic datasets."
ICDM	Predicting Software Escalations with Maximum ROI.	Charles X. Ling,Shengli Sheng,Tilmann F. W. Bruckhaus,Nazim H. Madhavji	2005	Enterprise software venders often have to release software products before all reported defects are corrected, and a small number of these reported defects will be escalated by customers whose businesses are seriously impacted. Escalated defects must be quickly resolved at a high cost by the software vendors. The total costs can be even greater, including loss of reputation, satisfaction, loyalty, and repeat revenue. In this paper, we develop an Escalation Prediction (EP) system to mine historic defect report data and predict the escalation risk of current defect reports for maximum ROI (Return On Investment). More specifically, we first describe a simple and general framework to convert the maximum ROI problem to cost-sensitive learning. We then apply and compare several best-known cost-sensitive learning approaches for EP. The EP system has produced promising results, and has been deployed in the product group of an enterprise software vendor. Conclusions drawn from this study also provide guidelines for mining imbalanced datasets and cost-sensitive learning.
ICDM	Discovering Frequent Arrangements of Temporal Intervals.	Panagiotis Papapetrou,George Kollios,Stan Sclaroff,Dimitrios Gunopulos	2005	In this paper we study a new problem in temporal pattern mining: discovering frequent arrangements of temporal intervals. We assume that the database consists of sequences of events, where an event occurs during a time-interval. The goal is to mine arrangements of event intervals that appear frequently in the database. There are many applications where these type of patterns can be useful, including data network, scientific, and financial applications. Efficient methods to find frequent arrangements of temporal intervals using both breadth first and depth first search techniques are described. The performance of the proposed algorithms is evaluated and compared with other approaches on real datasets (American Sign Language streams and network data) and large synthetic datasets.
ICDM	Mining Approximate Frequent Itemsets from Noisy Data.	Jinze Liu,Susan Paulsen,Wei Wang,Andrew B. Nobel,Jan Prins	2005	"Frequent itemset mining is a popular and important first step in analyzing data sets across a broad range of applications. The traditional, ""exact"" approach for finding frequent itemsets requires that every item in the itemset occurs in each supporting transaction. However, real data is typically subject to noise, and in the presence of such noise, traditional itemset mining may fail to detect relevant itemsets, particularly those large itemsets that are more vulnerable to noise. In this paper we propose approximate frequent itemsets (AFI), as a noise-tolerant itemset model. In addition to the usual requirement for sufficiently many supporting transactions, the AFI model places constraints on the fraction of errors permitted in each item column and the fraction of errors permitted in a supporting transaction. Taken together, these constraints winnow out the approximate itemsets that exhibit systematic errors. In the context of a simple noise model, we demonstrate that AFI is better at recovering underlying data patterns, while identifying fewer spurious patterns than either the exact frequent itemset approach or the existing error tolerant itemset approach of Yang et al. [11]."
ICDM	Text Representation: From Vector to Tensor.	Ning Liu,Benyu Zhang,Jun Yan,Zheng Chen,Wenyin Liu,Fengshan Bai,Leefeng Chien	2005	In this paper, we propose a text representation model, Tensor Space Model (TSM), which models the text by multilinear algebraic high-order tensor instead of the traditional vector. Supported by techniques of multilinear algebra, TSM offers a potent mathematical framework for analyzing the multifactor structures. TSM is further supported by certain introduced particular operations and presented tools, such as the High-Order Singular Value Decomposition (HOSVD) for dimension reduction and other applications. Experimental results on the 20 Newsgroups dataset show that TSM is constantly better than VSM for text classification.
ICDM	Combining Multiple Clusterings by Soft Correspondence.	Bo Long,Zhongfei (Mark) Zhang,Philip S. Yu	2005	Combining multiple clusterings arises in various important data mining scenarios. However, finding a consensus clustering from multiple clusterings is a challenging task because there is no explicit correspondence between the classes from different clusterings. We present a new framework based on soft correspondence to directly address the correspondence problem in combining multiple clusterings. Under this framework, we propose a novel algorithm that iteratively computes the consensus clustering and correspondence matrices using multiplicative updating rules. This algorithm provides a final consensus clustering as well as correspondence matrices that gives intuitive interpretation of the relations between the consensus clustering and each clustering from clustering ensembles. Extensive experimental evaluations also demonstrate the effectiveness and potential of this framework as well as the algorithm for discovering a consensus clustering from multiple clusterings.
ICDM	Parallel Algorithms for Distance-Based and Density-Based Outliers.	Elio Lozano,Edgar Acuña	2005	An outlier is an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism. Outlier detection has many applications, such as data cleaning, fraud detection and network intrusion. The existence of outliers can indicate individuals or groups that exhibit a behavior that is very different from most of the individuals of the dataset. In this paper we design two parallel algorithms, the first one is for finding out distance-based outliers based on nested loops along with randomization and the use of a pruning rule. The second parallel algorithm is for detecting density-based local outliers. In both cases data parallelism is used. We show that both algorithms reach near linear speedup. Our algorithms are tested on four real-world datasets coming from the Machine Learning Database Repository at the UCI.
ICDM	Efficiently Mining Frequent Closed Partial Orders.	Jian Pei,Jian Liu,Haixun Wang,Ke Wang,Philip S. Yu,Jianyong Wang	2005	Mining ordering information from sequence data is an important data mining task. Sequential pattern mining [1] can be regarded as mining frequent segments of total orders from sequence data. However, sequential patterns are often insufficient to concisely capture the general ordering information.
ICDM	Bit Reduction Support Vector Machine.	Tong Luo,Lawrence O. Hall,Dmitry B. Goldgof,Andrew Remsen	2005	Support vector machines are very accurate classifiers and have been widely used in many applications. However, the training and to a lesser extent prediction time of support vector machines on very large data sets can be very long. This paper presents a fast compression method to scale up support vector machines to large data sets. A simple bit reduction method is applied to reduce the cardinality of the data by weighting representative examples. We then develop support vector machines which may be trained on weighted data. Experiments indicate that the bit reduction support vector machine produces a significant reduction in the time required for both training and prediction with minimum loss in accuracy. It is also shown to be more accurate than random sampling, when the data is not over-compressed.
ICDM	Spatial Clustering of Chimpanzee Locations for Neighborhood Identification.	Sandeep Mane,Carson Murray,Shashi Shekhar,Jaideep Srivastava,Anne Pusey	2005	"Since 1960, the chimpanzees (Pan troglodytes) of Gombe National Park, Tanzania, have been studied by behavioral ecologists, including Jane Goodall. Data have been collected for more than 40 years and are being analyzed by researchers in order to increase our understanding of the social structure of chimpanzees. In this paper, we consider the following question of interest to behavioral ecologists — ""Does clustering exist among female chimpanzees in terms of their spatial locations?"" The analysis of this question will help behavioral ecologists to learn about the space use and the social interactions between female chimpanzees. The data collected for this analysis are marked spatial point patterns over the park. Current spatial clustering methods lack the ability to handle such marked point patterns directly. This paper presents a novel application of spatial point pattern analysis and data mining techniques to the ecological problem of clustering female chimpanzees. We found that Ripley's K-function provides a powerful statistical tool for evaluating clustering behavior among spatial point patterns. We then proposed two clustering approaches for marked point patterns using the K-function. Experimental results using the proposed clustering methods provide significant insight into the dynamics of female chimpanzee space use and into the overall social stucture of the species. In addition, the proposed methods can be extended to also include temporal information."
ICDM	A New Algorithm for Finding Minimal Sample Uniques for Use in Statistical Disclosure Assessment.	Anna M. Manning,David J. Haglin	2005	We present SUDA2, a recursive algorithm for finding Minimal Sample Uniques (MSUs). SUDA2 uses a novel method for representing the search space forMSUs and new observations about the properties ofMSUs to prune and traverse this space. Experimental comparisons with previous work demonstrate that SUDA2 is not only several orders of magnitude faster but is also capable of identifying the boundaries of the search space, enabling datasets of larger numbers of columns than before to be addressed.
ICDM	Alternate Representation of Distance Matrices for Characterization of Protein Structure.	Keith Marsolo,Srinivasan Parthasarathy	2005	"The most suitable method for the automated classification of protein structures remains an open problem in computational biology. In order to classify a protein structure with any accuracy, an effective representation must be chosen. Here we present two methods of representing protein structure. One involves representing the distances between the Cá atoms of a protein as a two-dimensional matrix and creating a model of the resulting surface with Zernike polynomials. The second uses a wavelet-based approach. We convert the distances between a protein's Cα atoms into a one-dimensional signal which is then decomposed using a discrete wavelet transformation. Using the Zernike co-efficients and the approximation coefficients of the wavelet decomposition as feature vectors, we test the effectiveness of our representation with two different classifiers on a dataset of more than 600 proteins taken from the 27 most-populated SCOP folds. We find that the wavelet decomposition greatly outperforms the Zernike model.With the wavelet representation, we achieve an accuracy of approximately 56%, roughly 12% higher than results reported on a similar, but less-challenging dataset. In addition, we can couple our structure-based feature vectors with several sequence-based properties to increase accuracy another 5-7%. Finally, we use a multi-stage classification strategy on the combined features to increase performance to 78%, an improvement in accuracy of more than 15-20% and 34% over the highest reported sequence-based and structure-based classification results, respectively."
ICDM	CLUMP: A Scalable and Robust Framework for Structure Discovery.	Kunal Punera,Joydeep Ghosh	2005	We introduce a robust and efficient framework called CLUMP (CLustering Using Multiple Prototypes) for unsupervised discovery of structure in data. CLUMP relies on finding multiple prototypes that summarize the data. Clustering the prototypes enables our algorithm to scale up to extremely large and high-dimensional domains such as text data. Other desirable properties include robustness to noise and parameter choices. In this paper, we describe the approach in detail, characterize its performance on a variety of datasets, and compare it to some existing model selection approaches.
ICDM	"Training Support Vector Machines Using Gilbert's Algorithm."	Shawn Martin	2005	"Support Vector Machines are classifiers designed around the computation of an optimal separating hyperplane. This hyperplane is typically obtained by solving a constrained quadratic programming problem, but may also be located by solving a nearest point problem. Gilbert's Algorithm can be used to solve this nearest point problem but is unreasonably slow. In this paper we present a modified version of Gilbert's Algorithm for the fast computation of the Support Vector Machine hyperplane. We then compare our algorithm with the Nearest Point Algorithm and with Sequential Minimal Optimization."
ICDM	A Graph-Ranking Algorithm for Geo-Referencing Documents.	Bruno Martins,Mário J. Silva	2005	This paper presents an application of PageRank for assigning documents with a corresponding geographical scope. We describe the technique in detail, together with its theoretical formulation. Experimental results are promising, comparing favorably with previous proposals.
ICDM	An Expected Utility Approach to Active Feature-Value Acquisition.	Prem Melville,Foster J. Provost,Raymond J. Mooney	2005	An Expected Utility Approach to Active Feature-Value Acquisition.
ICDM	Ranking-Based Evaluation of Regression Models.	Saharon Rosset,Claudia Perlich,Bianca Zadrozny	2005	"We suggest the use of ranking-based evaluation measures for regression models, as a complement to the commonly used residual-based evaluation. We argue that in some cases, such as the case study we present, ranking can be the main underlying goal in building a regression model, and ranking performance is the correct evaluation metric. However, even when ranking is not the contextually correct performance metric, the measures we explore still have significant advantages: They are robust against extreme outliers in the evaluation set; and they are interpretable. The two measures we consider correspond closely to non-parametric correlation coefficients commonly used in data analysis (Spearman's ρ and Kendall's τ); and they both have interesting graphical representations, which, similarly to ROC curves, offer useful ""partial"" model performance views, in addition to a one-number summary in the area under the curve. We illustrate our methods on a case study of evaluating IT Wallet size estimation models for IBM's customers."
ICDM	A Heterogeneous Field Matching Method for Record Linkage.	Steven Minton,Claude Nanjo,Craig A. Knoblock,Martin Michalowski,Matthew Michelson	2005	"Record linkage is the process of determining that two records refer to the same entity. A key subprocess is evaluating how well the individual fields, or attributes, of the records match each other. One approach to matching fields is to use hand-written domain-specific rules. This ""expert systems"" approach may result in good performance for specific applications, but it is not scalable. This paper describes a new machine learning approach that creates expert-like rules for field matching. In our approach, the relationship between two field values is described by a set of heterogeneous transformations. Previous machine learning methods used simple models to evaluate the distance between two fields. However, our approach enables more sophisticated relationships to be modeled, which better capture the complex domain specific, common-sense phenomena that humans use to judge similarity. We compare our approach to methods that rely on simpler homogeneous models in several domains. By modeling more complex relationships we produce more accurate results."
ICDM	Automatically Mining Result Records from Search Engine Response Pages.	Dheerendranath Mundluru,Jayasimha Reddy Katukuri,Saygin Celebi	2005	Usually, Web applications such as deep Web crawlers, metasearch engines, and other Web mining systems need to extract information displayed in the form of result records on response pages returned by search engines in response to submitted queries. Extracting such records is challenging as search engines are heterogeneous in displaying their records. In addition, response pages returned by many search engines include other noisy content such as advertisements, suggestion links, etc., which make the extraction task even more complicated. In this paper, we propose a highly effective and efficient algorithm for automatically mining result records from search engine response pages.
ICDM	Leveraging Relational Autocorrelation with Latent Group Models.	Jennifer Neville,David Jensen	2005	The presence of autocorrelation provides a strong motivation for using relational learning and inference techniques. Autocorrelation is a statistical dependence between the values of the same variable on related entities and is a nearly ubiquitous characteristic of relational data sets. Recent research has explored the use of collective inference techniques to exploit this phenomenon. These techniques achieve significant performance gains by modeling observed correlations among class labels of related instances, but the models fail to capture a frequent cause of autocorrelation - the presence of underlying groups that influence the attributes on a set of entities. We propose a latent group model (LGM) for relational data, which discovers and exploits the hidden structures responsible for the observed autocorrelation among class labels. Modeling the latent group structure improves model performance, increases inference efficiency, and enhances our understanding of the datasets. We evaluate performance on three relational classification tasks and show that LGM outperforms models that ignore latent group structure, particularly when there is little information with which to seed inference.
ICDM	Compound Classification Models for Recommender Systems.	Lars Schmidt-Thieme	2005	Recommender systems recommend products to customers based on ratings or past customer behavior. Without any information about attributes of the products or customers involved, the problem has been tackled most successfully by a nearest neighbor method called collaborative filtering in the context, while additional efforts invested in building classification models did not pay off and did not increase the quality. Therefore, classification methods have mainly been used in conjunction with product or customer attributes. Starting from a view on the plain recommendation task without attributes as a multi-class classification problem, we investigate two particularities, its autocorrelation structure as well as the absence of re-occurring items (repeat buying). We adapt the standard generic reductions 1-vs-rest and 1-vs-1 of multi-class problems to a set of binary classification problems to these particularities and thereby provide a generic compound classifier for recommender systems. We evaluate a particular specialization thereof using linear support vector machines as member classifiers on MovieLens data and show that it outperforms state-of-the-artmethods, i.e., item-based collaborative filtering.
ICDM	On the Tractability of Rule Discovery from Distributed Data.	Martin Scholz	2005	This paper analyses the tractability of rule selection for supervised learning in distributed scenarios. The selection of rules is usually guided by a utility measure such as predictive accuracy or weighted relative accuracy. A common strategy to tackle rule selection from distributed data is to evaluate rules locally on each dataset. While this works well for homogeneously distributed data, this work proves limitations of this strategy if distributions are allowed to deviate. The identification of those subsets for which local and global distributions deviate, poses a learning task of its own, which is shown to be at least as complex as discovering the globally best rules from local data.
ICDM	Face Recognition Using Landmark-Based Bidimensional Regression.	Jiazheng Shi,Ashok Samal,David Marx	2005	This paper studies how biologically meaningful landmarks extracted from face images can be exploited for face recognition using the bidimensional regression. Incorporating the correlation statistics of landmarks, this paper also proposes a new approach called eigenvalue weighted bidimensional regression. Complex principal component analysis is used for computing eigenvalues and removing correlation among landmarks. We evaluate our approach using two standard face databases: the Purdue AR and the NIST FERET. Experimental results show that the bidimensional regression is an efficient method to exploit geometry information of face images.
ICDM	Learning Functional Dependency Networks Based on Genetic Programming.	Wing-Ho Shum,Kwong-Sak Leung,Man Leung Wong	2005	Bayesian Network (BN) is a powerful network model, which represents a set of variables in the domain and provides the probabilistic relationships among them. But BN can handle discrete values only; it cannot handle continuous, interval and ordinal ones, which must be converted to discrete values and the order information is lost. Thus, BN tends to have higher network complexity and lower understandability. In this paper, we present a novel dependency network which can handle discrete, continuous, interval and ordinal values through functions; it has lower network complexity and stronger expressive power; it can represent any kind of relationships; and it can incorporate a-priori knowledge though user-defined functions. We also propose a novel Genetic Programming (GP) to learn dependency networks. The novel GP does not use any knowledge-guided nor application-oriented operator, thus it is robust and easy to replicate. The experimental results demonstrate that the novel GP can successfully discover the target novel dependency networks, which have the highest accuracy and the lowest network complexity.
ICDM	Instability of Classifiers on Categorical Data.	Arno Siebes,Muhammad Subianto,A. J. Feelders	2005	In this paper we study the local behaviour of arbitrary classifiers using the instability of that classifier in a data point. Moreover, we introduce two algorithms. The first to find highly unstable points, the second to find islands of stability.
ICDM	Mining Patterns of Change in Remote Sensing Image Databases.	Marcelino Pereira dos Santos Silva,Gilberto Câmara,Ricardo Cartaxo Modesto de Souza,Dalton M. Valeriano,Maria Isabel Sobral Escada	2005	"Remote sensing image databases are the fastest growing archives of spatial information. However, we still have a limited capacity for extracting information from large remote sensing image databases. There are currently very few techniques for image data mining and information extraction in large image data sets, and thus we are failing to exploit our large remote sensing data archives. This paper proposes a methodology to provide guidance for mining remote sensing image databases. The basic idea is to use domain concepts to build generic description of patterns in remote sensing images, and then use structural approaches to identify such patterns in images. We illustrate our proposal with a case study for detecting land use patterns in Amazonia from INPE's remote sensing image database."
ICDM	Pruning Social Networks Using Structural Properties and Descriptive Attributes.	Lisa Singh,Lise Getoor,Louis Licamele	2005	Scale is often an issue with understanding and making sense of large social networks. Here we investigate methods for pruning social networks by determining the most relevant relationships. We measure importance in terms of predictive accuracy on a set of target attributes of the social network. Our goal is to create a pruned network that models only the most informative affiliations and relationships. We present methods for pruning networks based on both structural properties and descriptive attributes demonstrate it on a network of NASDAQ and NYSE businesses and on a bibliographic network.
ICDM	CloseMiner: Discovering Frequent Closed Itemsets Using Frequent Closed Tidsets.	Ningthoujam Gourakishwar Singh,Sanasam Ranbir Singh,Anjana K. Mahanta	2005	Complete set of itemsets can be grouped into non-overlapping clusters identified by closed tidsets. Each cluster has only one closed itemset and is the superset of all itemsets with the same support. Number of closed itemsets is identical to the number of clusters. Therefore, the problem of discovering closed itemsets can be considered as the problem of clustering the complete set of itemsets by closed tidsets. In this paper, we present CloseMiner, a new algorithm for discovering all frequent closed itemsets by grouping the complete set of itemsets into non-overlapping clusters identified by closed tidsets. An extensive experimental evaluation on a number of real and synthetic databases shows that CloseMiner outperforms Apriori and CHARM.
ICDM	Optimizing Constraint-Based Mining by Automatically Relaxing Constraints.	Arnaud Soulet,Bruno Crémilleux	2005	In constraint-based mining, the monotone and anti-monotone properties are exploited to reduce the search space. Even if a constraint has not such suitable properties, existing algorithms can be re-used thanks to an approximation, called relaxation. In this paper, we automatically compute monotone relaxations of primitive-based constraints. First, we show that the latter are a superclass of combinations of both kinds of monotone constraints. Second, we add two operators to detect the properties of monotonicity of such constraints. Finally, we define relaxing operators to obtain monotone relaxations of them.
ICDM	Generalizing the Notion of Confidence.	Michael Steinbach,Vipin Kumar	2005	In this paper, we explore extending association analysis to non-traditional types of patterns and non-binary data by generalizing the notion of confidence. The key idea is to regard confidence as a measure of the extent to which the strength of one association pattern provides information about the strength of another. This approach provides a framework that encompasses the traditional concept of confidence as a special case and can be used as the basis for designing a variety of new confidence measures. Besides discussing such confidence measures, we provide examples that illustrate the potential usefulness of a generalized notion of confidence. In particular, we describe an approach to defining confidence for error tolerant itemsets that preserves the interpretation of confidence as a conditional probability and derive a confidence measure for continuous data that agrees with the standard confidence measure when applied to binary transaction data.
ICDM	"SVM Feature Selection for Classification of SPECT Images of Alzheimer's Disease Using Spatial Information."	Jonathan Stoeckel,Glenn Fung	2005	"Alzheimer's disease is the most frequent type of dementia for elderly patients. Due to aging populations the occurrence of this disease will increase in the next years. Early diagnosis is crucial to be able to develop more powerful treatments. Brain perfusion changes can be a marker for Alzheimer's disease. In this article we study the use of SPECT perfusion imaging for the diagnosis of Alzheimer's disease differentiating between images from healthy subjects and images from Alzheimer's disease patients. Our classification approach is based on a linear programming formulation similar to the 1-norm support vector machines. In contrastwith other linear hyperplane-based methods that perform simultaneous feature selection and classification, our proposed formulation incorporates proximity information about the features and generates a classifier that does not just select the most relevant voxels but the most relevant ""areas"" for classification resulting in more robust classifiersthat are better suitable for interpretation. This approach is compared with the classical Fisher linear discriminant (FLD) classifier as well as with statistical parametric mapping (SPM). We tested our method on data from four European institutions. Our method achieved sensitivity of 84.4% at 90.9% specificity, this is considerable better the human experts. Our method also outperformed the FLD and SPM techniques. We conclude that our approach has the potential to be a useful help for clinicians."
ICDM	Neighborhood Formation and Anomaly Detection in Bipartite Graphs.	Jimeng Sun,Huiming Qu,Deepayan Chakrabarti,Christos Faloutsos	2005	Many real applications can be modeled using bipartite graphs, such as users vs. files in a P2P system, traders vs. stocks in a financial trading system, conferences vs. authors in a scientific publication network, and so on. We introduce two operations on bipartite graphs: 1) identifying similar nodes (Neighborhood formation), and 2) finding abnormal nodes (Anomaly detection). And we propose algorithms to compute the neighborhood for each node using random walk with restarts and graph partitioning; we also propose algorithms to identify abnormal nodes, using neighborhood information. We evaluate the quality of neighborhoods based on semantics of the datasets, and we also measure the performance of the anomaly detection algorithm with manually injected anomalies. Both effectiveness and efficiency of the methods are confirmed by experiments on several real datasets.
ICDM	A Border-Based Approach for Hiding Sensitive Frequent Itemsets.	Xingzhi Sun,Philip S. Yu	2005	Sharing data among organizations often leads to mutual benefit. Recent technology in data mining has enabled efficientextraction of knowledge from large databases. This, however, increases risks of disclosing the sensitive knowledge when the database is released to other parties. To address this privacy issue, one may sanitize the original database so that the sensitive knowledge is hidden. The challenge is to minimize the side effect on the quality of the sanitized database so that non-sensitive knowledge can still be mined. In this paper, we study such a problem in the context of hiding sensitive frequent itemsets by judiciously modifying the transactions in the database. To preserve the non-sensitive frequent itemsets, we propose a border-based approach to efficiently evaluate the impact of any modification to the database during the hiding process. The quality of database can be well maintained by greedily selecting the modifications with minimal side effect. Experiments results are also reported to show the effectiveness of the proposed approach.
ICDM	X-mHMM: An Efficient Algorithm for Training Mixtures of HMMs When the Number of Mixtures Is Unknown.	Zoltán Szamonek,Csaba Szepesvári	2005	In this paper we consider sequence clustering problems and propose an algorithm for the estimation of the number of clusters based on the X-means algorithm. The sequences are modeled using mixtures of Hidden Markov Models. By means of experiments with synthetic data we analyze the proposed algorithm. This algorithm proved to be both computationally efficient and capable of providing accurate estimates of the number of clusters. Some results of experiments with real-world web-log data are also given.
ICDM	A Random Walk through Human Associations.	Raz Tamir	2005	"Letting one's thoughts wander is not simply an arbitrary or rambling process. It can better be described as ""associative thinking"", where a complex chain of associative thoughts and ideas are linked. It is our contention that this seemingly chaotic process can be modeled by a random walk in a weighted directed graph. Furthermore, is it possible to predict mathematically the ""steady state"" of such a process, to determine where such wandering is leading. The random walk process uses rules of association, defined by the Local Confidence Gain (LCG) interestingness measure. Extracted concepts are used as nodes of a directed graph. The associative ""forces"" between any two concepts (measured by LCG) are used to weigh the edges connecting the nodes that create a graph of associations. It is common, yet not trivial, for people to look for data about a subject without knowing its exact nomenclature (for example, finding the name of a disease just by knowing its symptoms). Random walk in association graphs can discover highly informative phrases that can be used for query expansion in a way that better expresses the user's initial search goals. A different usage is to create a user profile representing his current interests. We used a modified version of the Turing Test to show that the random walk process discovers association rules that conform to a human associations generating process. By constructing the user associations we were able to build a profile representing the user's ""line of thoughts"". The suggested algorithm can be used in any database and can implement the ranking measures of other association rules."
ICDM	Bias Analysis in Text Classification for Highly Skewed Data.	Lei Tang,Huan Liu	2005	Feature selection is often applied to high-dimensional data as a preprocessing step in text classification. When dealing with highly skewed data, we observe that typical feature selection metrics like information gain or chi-squared are biased toward selecting features for the minor class, and the metric of bi-normal separation can select features for both minor and major classes. In this work, we investigate how these feature selection metrics impact on the performance of frequently used classifiers such as Decision Trees, Na¨ýve Bayes, and Support Vector Machines via bias analysis for highly skewed data. Three types of biases are metric bias, class bias, and classifier bias. Extensive experiments are designed to understand how these biases can be employed in concert and efficiently to achieve good classificationperformance. We report our findings and present recommended approaches to text classification based on bias analysis and the empirical study.
ICDM	Supervised Tensor Learning.	Dacheng Tao,Xuelong Li,Weiming Hu,Stephen J. Maybank,Xindong Wu	2005	This paper aims to take general tensors as inputs for supervised learning. A supervised tensor learning (STL) framework is established for convex optimization based learning techniques such as support vector machines (SVM) and minimax probability machines (MPM). Within the STL framework, many conventional learning machines can be generalized to take n^th-order tensors as inputs. We also study the applications of tensors to learning machine design and feature extraction by linear discriminant analysis (LDA). Our method for tensor based feature extraction is named the tenor rank-one discriminant analysis (TR1DA). These generalized algorithms have several advantages: 1) reduce the curse of dimension problem in machine learning and data mining; 2) avoid the failure to converge; and 3) achieve better separation between the different categories of samples. As an example, we generalize MPM to its STL version, which is named the tensor MPM (TMPM). TMPM learns a series of tensor projections iteratively. It is then evaluated against the original MPM. Our experiments on a binary classification problem show that TMPM significantly outperforms the original MPM.
ICDM	Efficient Mining of High Branching Factor Attribute Trees.	Alexandre Termier,Marie-Christine Rousset,Michèle Sebag,Kouzou Ohara,Takashi Washio,Hiroshi Motoda	2005	In this paper, we present a new tree mining algorithm, DRYADEPARENT, based on the hooking principle first introduced in DRYADE [9]. In the experiments, we demonstrate that the branching factor and depth of the frequent patterns to find are key factor of complexity for tree mining algorithms. We show that DRYADEPARENT outperforms the current fastest algorithm, CMTreeMiner, by orders of magnitude on datasets where the frequent patterns have a high branching factor.
ICDM	Anomaly Intrusion Detection Using Multi-Objective Genetic Fuzzy System and Agent-Based Evolutionary Computation Framework.	Chi-Ho Tsang,Sam Kwong,Hanli Wang	2005	In this paper, we present a multi-objective genetic fuzzy system for anomaly intrusion detection. The proposed system extracts accurate and interpretable fuzzy rule-based knowledge from network data using an agent-based evolutionary computation framework. The experimental results on KDD-Cup99 intrusion detection benchmark data demonstrate that our system can achieve high detection rate for intrusion attacks and low false positive rate for normal network traffic.
ICDM	Template-Based Privacy Preservation in Classification Problems.	Ke Wang,Benjamin C. M. Fung,Philip S. Yu	2005	"In this paper, we present a template-based privacy preservation to protect against the threats caused by data mining abilities. The problem has dual goals: preserve the information for a wanted classification analysis and limit the usefulness of unwanted sensitive inferences that may be derived from the data. Sensitive inferences are specified by a set of ""privacy templates"". Each template specifies the sensitive information to be protected, a set of identifying attributes, and the maximum association between the two. We show that suppressing the domain values is an effective way to eliminate sensitive inferences. For a large data set, finding an optimal suppression is hard, since it requires optimization over all suppressions. We present an approximate but scalable solution. We demonstrate the effectiveness of this approach on real life data sets."
ICDM	Approximate Inverse Frequent Itemset Mining: Privacy, Complexity, and Approximation.	Yongge Wang,Xintao Wu	2005	In order to generate synthetic basket datasets for better benchmark testing, it is important to integrate characteristics from real-life databases into the synthetic basket datasets. The characteristics that could be used for this purpose include the frequent itemsets and association rules. The problem of generating synthetic basket datasets from frequent itemsets is generally referred to as inverse frequent itemset mining. In this paper, we show that the problem of approximate inverse frequent itemset mining is NP-complete. Then we propose and analyze an approximate algorithm for approximate inverse frequent itemset mining, and discuss privacy issues related to the synthetic basket dataset. In particular, we propose an approximate algorithm to determine the privacy leakage in a synthetic basket dataset.
ICDM	On Reducing Classifier Granularity in Mining Concept-Drifting Data Streams.	Peng Wang,Haixun Wang,Xiaochen Wu,Wei Wang,Baile Shi	2005	"Many applications use classification models on streaming data to detect actionable alerts. Due to concept drifts in the underlying data, how to maintain a model's up-to-dateness has become one of the most challenging tasks in mining data streams. State of the art approaches, including both the incrementally updated classifiers and the ensemble classifiers, have proved that model update is a very costly process. In this paper, we introduce the concept of model granularity. We show that reducing model granularity will reduce model update cost. Indeed, models of fine granularity enable us to efficiently pinpoint local components in the model that are affected by the concept drift. It also enables us to derive new components that can easily integrate with the model to reflect the current data distribution, thus avoiding expensive updates on a global scale. Experiments on real and synthetic data show that our approach is able to maintain good prediction accuracy at a fraction of model updating cost of state of the art approaches."
ICDM	A Bernoulli Relational Model for Nonlinear Embedding.	Gang Wang,Hui Zhang,Zhihua Zhang,Frederick H. Lochovsky	2005	"The notion of relations is extremely important in mathematics. In this paper, we use relations to describe the embedding problem and propose a novel stochastic relational model for nonlinear embedding. Given some relation among points in a high-dimensional space, we start from preserving the same relation in a low embedded space and model the relation as probabilistic distributions over these two spaces, respectively. We illustrate that the stochastic neighbor embedding and the Gaussian process latent variable model can be derived from our relational model. Moreover we devise a new stochastic embedding model and refer to it as Bernoulli relational embedding (BRE). BRE's ability in nonlinear dimensionality reduction is illustrated on a set of synthetic data and collections of bitmaps of handwritten digits and face images."
ICDM	Mining Quantitative Frequent Itemsets Using Adaptive Density-Based Subspace Clustering.	Takashi Washio,Yuki Mitsunaga,Hiroshi Motoda	2005	A novel approach to subspace clustering is proposed to exhaustively and efficiently mine quantitative frequent itemsets (QFIs) from massive transaction data¹. For the computational tractability, our approach introduces adaptive density-based and Apriori-like algorithm. Its outstanding performance is shown through numerical experiments.
ICDM	Atomic Wedgie: Efficient Query Filtering for Streaming Times Series.	Li Wei,Eamonn J. Keogh,Helga Van Herle,Agenor Mafra-Neto	2005	In many applications it is desirable to monitor a streaming time series for predefined patterns. In domains as diverse as the monitoring of space telemetry, patient intensive care data, and insect populations, where data streams at a high rate and the number of predefined patterns is large, it may be impossible for the comparison algorithm to keep up. We propose a novel technique that exploits the commonality among the predefined patterns to allow monitoring at higher bandwidths, while maintaining a guarantee of no false dismissals. Our approach is based on the widely used envelope-based lower bounding technique. Extensive experiments demonstrate that our approach achieves tremendous improvements in performance in the offline case, and significant improvements in the fastest possible arrival rate of the data stream that can be processed with guaranteed no false dismissal.
ICDM	Hot Item Mining and Summarization from Multiple Auction Web Sites.	Tak-Lam Wong,Wai Lam	2005	Online auction Web sites are fast changing, highly dynamic, and complex as they involve tremendous sellers and potential buyers, as well as a huge amount of items listed for bidding. We develop a two-phase framework which aims at mining and summarizing hot items from multiple auctionWeb sites to assist decision making. The objective of the first phase is to automatically extract the product features and product feature values of the items from the descriptions provided by the sellers. We design a HMM-based learning method to train an extended HMM model which can adapt to the unseen Web page from which the information is extracted. The goal of the second phase is to discover and summarize the hot items based on the extracted information. We formulate the hot item mining task as a semi-supervised learning problem and employ the graph mincuts algorithm to accomplish this task. The summary of the hot items is then generated by considering the frequency and the position of the product features being mentioned in the descriptions. We have conducted extensive experiments from several real-world auction Web sites to demonstrate the effectiveness of our framework.
ICDM	Merging Interface Schemas on the Deep Web via Clustering Aggregation.	Wensheng Wu,AnHai Doan,Clement T. Yu	2005	We consider the problem of integrating a large number of interface schemas over the Deep Web, The scale of the problem and the diversity of the sources present serious challenges to the conventional manual or rule-based approaches to schema integration. To address these challenges, we propose a novel formulation of schema integration as an optimization problem, with the objective of maximally satisfying the constraints given by individual schemas. Since the optimization problem can be shown to be NP-complete, we develop a novel approximation algorithm LMax, which builds the unified schema via recursive applications of clustering aggregation. We further extend LMax to handle the irregularities frequently occurring among the interface schemas. Extensive evaluation on real-world data sets shows the effectiveness of our approach.
ICDM	Welcome Message from the Conference Chairs.		2005	Welcome Message from the Conference Chairs.
ICDM	Welcome to ICDM 2005.		2005	Welcome to ICDM 2005.
ICDM	Conference Organization.		2005	Conference Organization.
ICDM	Steering Committee.		2005	Steering Committee.
ICDM	Program Committee.		2005	Program Committee.
ICDM	Non-PC Reviewers.		2005	Non-PC Reviewers.
ICDM	Invited Talks.		2005	Invited Talks.
ICDM	Tutorials.		2005	Tutorials.
ICDM	Workshops.		2005	Workshops.
ICDM	Panel Session.		2005	Panel Session.
ICDM	Discriminatively Trained Markov Model for Sequence Classification.	Oksana Yakhnenko,Adrian Silvescu,Vasant Honavar	2005	In this paper, we propose a discriminative counterpart of the directed Markov Models of order k - 1, or MM(k-1) for sequence classification. MM(k-1) models capture dependencies among neighboring elements of a sequence. The parameters of the classifiers are initialized to based on the maximum likelihood estimates for their generative counterparts. We derive gradient based update equations for the parameters of the sequence classifiers in order to maximize the conditional likelihood function. Results of our experiments with data sets drawn from biological sequence classification (specifically protein function and subcellular localization) and text classification applications show that the discriminatively trained sequence classifiers outperform their generative counterparts, confirming the benefits of discriminative training when the primary objective is classification.Our experiments also show that the discriminatively trained MM(k - 1) sequence classifiers are competitive with the computationally much more expensive Support Vector Machines trained using k-gram representations of sequences.
ICDM	On the Stationarity of Multivariate Time Series for Correlation-Based Data Analysis.	Kiyoung Yang,Cyrus Shahabi	2005	Multivariate time series (MTS) data sets are common in various multimedia, medical and financial application domains. These applications perform several data-analysis operations on large number of MTS data sets such as similarity searches, feature-subset-selection, clustering and classifications. Correlation-based techniques, such as Principal Component Analysis (PCA), have proven to improve the efficiency of many of the above-mentioned data-analysis operations on MTS, which implies that the correlation coefficientsconcisely represent the original MTS data. However, if the statistical properties (e.g., variance) of MTS data change over time dimension, i.e., MTS data is non-stationary, the correlation coefficients are not stable. In this paper, we propose to utilize the stationarity of the MTS data sets, in order to represent the original MTS data more stably, as well as concisely with the correlation coefficients. That is, before performing any correlation-based data analysis, we first executes the stationarity test to decide whether the MTS data is stationary or not, i.e., whether the correlation is stable or not. Subsequently, for a non-stationary MTS data set, we difference it to render the data set stationary. Even though our approach is general, to focus the discussion we describe our approach within the context of our previously proposed technique for MTS similarity search. In order to show the validity of our approach, we performed several experiments on four real-world data sets. The results show that the performance of our similarity search technique have significantly improved in terms of precision/recall.
ICDM	Visualizing Global Manifold Based on Distributed Local Data Abstractions.	Xiaofeng Zhang,William K. Cheung	2005	Mining distributed data for global knowledge is getting more attention recently. The problem is especially challenging when data sharing is prohibited due to local constraints like limited bandwidth and data privacy. In this paper, we investigate how to derive the embedded manifold (as a 2-D map)for a horizontally partitioned data set, where data cannot be shared among the partitions directly. We propose a model-based approach which computes hierarchical local data abstractions, aggregates the abstractions, and finally learns a global generative model — generative topographic mapping (GTM) based on the aggregated data abstraction. We applied the proposed method to two benchmarking data sets and demonstrated that the accuracy of the derived manifold can effectively be controlled by adjusting the data granularity level of the adopted local abstraction.
ICDM	Speculative Markov Blanket Discovery for Optimal Feature Selection.	Sandeep Yaramakala,Dimitris Margaritis	2005	In this paper we address the problem of learning the Markov blanket of a quantity from data in an efficient manner. Markov blanket discovery can be used in the feature selection problem to find an optimal set of features for classificationtasks, and is a frequently-used preprocessing phase in data mining, especially for high-dimensional domains. Our contribution is a novel algorithm for the induction of Markov blankets from data, called Fast-IAMB, that employs a heuristic to quickly recover the Markov blanket. Empirical results show that Fast-IAMB performs in many cases faster and more reliably than existing algorithms without adversely affecting the accuracy of the recovered Markov blankets.
ICDM	Discriminant Analysis: A Unified Approach.	Peng Zhang,Norbert Riedel	2005	Linear discriminant analysis (LDA) as a dimension reduction method is widely used in data mining and machine learning. It however suffers from the small sample size (SSS) problem when data dimensionality is greater than the sample size. Many modified methods have been proposed to address some aspect of this difficulty from a particular viewpoint. A comprehensive framework that provides a complete solution to the SSS problem is still missing. In this paper, we provide a unified approach to LDA, and investigate the SSS problem in the framework of statistical learning theory. In such a unified approach, our analysis results in a deeper understanding of LDA. We demonstrate that LDA (and its nonlinear extension) belongs to the same framework where powerful classifiers such as support vector machines (SVMs) are formulated. In addition, this approach allows us to establish an error bound for LDA. Finally our experiments validate our theoretical analysis results.
ICDM	Bagging with Adaptive Costs.	Yi Zhang,W. Nick Street	2005	Ensemble methods have proved to be highly effective in improving the performance of base learners under most circumstances. In this paper, we propose a new algorithm that combines the merits of some existing techniques, namely bagging, arcing and stacking. The basic structure of the algorithm resembles bagging. However, the misclassification cost of each training point is repeatedly adjusted according to its observed out-of-bag vote margin. In this way, the method gains the advantage of arcing - building the classifier the ensemble needs - without fixating on potentially noisy points. Computational experiments show that this algorithm performs consistently better than bagging and arcing with linear and nonlinear base classifiers. In view of the characteristics of bacing, a hybrid ensemble learning strategy, which combines bagging and different versions of bacing, is proposed and studied empirically.
ICDM	Sharing Classifiers among Ensembles from Related Problem Domains.	Yi Zhang,W. Nick Street,Samuel Burer	2005	A classification ensemble is a group of classifiers that all solve the same prediction problem in different ways. It is well-known that combining the predictions of classifiers within the same problem domain using techniques like bagging or boosting often improves the performance. This research shows that sharing classifiers among different but closely related problem domains can also be helpful. In addition, a semi-definite programming based ensemble pruning method is implemented in order to optimize the selection of a subset of classifiers for each problem domain. Computational results on a catalog dataset indicate that the ensembles resulting from sharing classifiers among different product categories generally have larger AUCs than those ensembles trained only on their own categories. The pruning algorithm not only prevents the occasional decrease of effectiveness caused by conflicting concepts among the problem domains, but also provides a better understanding of the problem domains and their relationships.
ICDM	Learning through Changes: An Empirical Study of Dynamic Behaviors of Probability Estimation Trees.	Kun Zhang,Zujia Xu,Jing Peng,Bill P. Buckles	2005	In practice, learning from data is often hampered by the limited training examples. In this paper, as the size of training data varies, we empirically investigate several probability estimation tree algorithms over eighteen binary classification problems. Nine metrics are used to evaluate their performances. Our aggregated results show that ensemble trees consistently outperform single trees. Confusion factor trees(CFT) register poor calibration even as training size increases, which shows that CFTs are potentially biased if data sets have small noise. We also provide analysis on the observed performance of the tree algorithms.
ICDM	Integrating Hidden Markov Models and Spectral Analysis for Sensory Time Series Clustering.	Jie Yin,Qiang Yang	2005	We present a novel approach for clustering sequences of multi-dimensional trajectory data obtained from a sensor network. The sensory time-series data present new challenges to data mining, including uneven sequence lengths, multi-dimensionality and high levels of noise. We adopt a principled approach, by first transforming all the data into an equal-length vector form while keeping as much temporal information as we can, and then applying dimensionality and noise reduction techniques such as spectral clustering to the transformed data. Experimental evaluation on synthetic and real data shows that our proposed approach outperforms standard model-based clustering algorithms for time series data.
ICDM	A Visual Data Mining Framework for Convenient Identification of Useful Knowledge.	Kaidi Zhao,Bing Liu,Thomas M. Tirpak,Weimin Xiao	2005	Data mining algorithms usually generate a large number of rules, which may not always be useful to human users. In this project, we propose a novel visual data-mining framework, called Opportunity Map, to identify useful and actionable knowledge quickly and easily from the discovered rules. The framework is inspired by the House of Quality from Quality Function Deployment (QFD) in Quality Engineering. It associates discovered rules, related summarized data and data distributions with the application objective using an interactive matrix. Combined with drill down visualization, integrated visualization of data distribution bars and rules, visualization of trend behaviors, and comparative analysis, the Opportunity Map allows users to analyze rules and data at different levels of detail and quickly identify the actionable knowledge and opportunities. The proposed framework represents a systematic and flexible approach to rule analysis. Applications of the system to large-scale data sets from our industrial partner have yielded promising results.
ICDM	A Join-Less Approach for Co-Location Pattern Mining: A Summary of Results.	Jin Soung Yoo,Shashi Shekhar,Mete Celik	2005	Spatial co-location patterns represent the subsets of features whose instances are frequently located together in geographic space. Co-location pattern discovery presents challenges since the instances of spatial features are embedded in a continuous space and share a variety of spatial relationships. A large fraction of the computation time is devoted to identifying the instances of co-location patterns. We propose a novel join-less approach for co-location pattern mining, which materializes spatial neighbor relationships with no loss of co-location instances and reduces the computational cost of identifying the instances. The join-less co-location mining algorithm is efficient since it uses an instance-lookup scheme instead of an expensive spatial or instance join operation for identifying co-location instances. The experimental evaluations show the join-less algorithm performs more efficiently than a current join-based algorithm and is scalable in dense spatial datasets.
ICDM	Example-Based Robust Outlier Detection in High Dimensional Datasets.	Cui Zhu,Hiroyuki Kitagawa,Christos Faloutsos	2005	Detecting outliers is an important problem. Most of its applications typically possess high dimensional datasets. In high dimensional space, the data becomes sparse which implies that every object can be regarded as an outlier from the point of view of similarity. Furthermore, a fundamental issue is that the notion of which objects are outliers typically varies between users, problem domains or, even, datasets. In this paper, we present a novel robust solution which detects high dimensional outliers based on user examples and tolerates incorrect inputs. It studies the behavior of projections of such a few examples, to discover further objects that are outstanding in the projection where many examples are outlying. Our experiments on both real and synthetic datasets demonstrate the ability of the proposed method to detect outliers corresponding to the user examples.
ICDM	Efficient Text Classification by Weighted Proximal SVM.	Dong Zhuang,Benyu Zhang,Qiang Yang,Jun Yan,Zheng Chen,Ying Chen	2005	In this paper, we present an algorithm that can classify large-scale text data with high classification quality and fast training speed. Our method is based on a novel extension of the proximal SVM mode [3]. Previous studies on proximal SVM have focused on classification for low dimensional data and did not consider the unbalanced data cases. Such methods will meet difficulties when classifying unbalanced and high dimensional data sets such as text documents. In this work, we extend the original proximal SVM by learning a weight for each training error. We show that the classification algorithm based on this model is capable of handling high dimensional and unbalanced data. In the experiments, we compare our method with the original proximal SVM (as a special case of our algorithm) and the standard SVM (such as SVM light) on the recently published RCV1-v2 dataset. The results show that our proposed method had comparable classification quality with the standard SVM. At the same time, both the time and memory consumption of our method are less than that of the standard SVM.
ICDM	CTC - Correlating Tree Patterns for Classification.	Albrecht Zimmermann,Björn Bringmann	2005	We present CTC, a new approach to structural classification. It uses the predictive power of tree patterns correlating with the class values, combining state-of-the-art tree mining with sophisticated pruning techniques to find the k most discriminative pattern in a dataset. In contrast to existing methods, CTC uses no heuristics and the only parameters to be chosen by the user are the maximum size of the rule set and a single, statistically well founded cut-off value. The experiments show that CTC classifiers achieve good accuracies while the induced models are smaller than those of existing approaches, facilitating comprehensibility.
ICDM	Proceedings of the 5th IEEE International Conference on Data Mining (ICDM 2005), 27-30 November 2005, Houston, Texas, USA		2005	Proceedings of the 5th IEEE International Conference on Data Mining (ICDM 2005), 27-30 November 2005, Houston, Texas, USA
KDD	The architecture of complexity: the structure and the dynamics of networks, from the web to the cell.	Albert-László Barabási	2005	"Networks with complex topology describe systems as diverse as the cell, the World Wide Web or the society. The emergence of most networks is driven by self-organizing processes that are governed by simple but generic laws. The analysis of the cellular network of various organisms shows that cells and complex man-made networks, such as the Internet or the world wide web, and many social and collaboration networks share the same large-scale topology. I will show that the scale-free topology of these complex webs have important consequences on their robustness against failures and attacks, with implications on drug design, the Internet's ability to survive attacks and failures, and the ability of ideas and innovations to spread on the network."
KDD	Model-based overlapping clustering.	Arindam Banerjee,Chase Krumpelman,Joydeep Ghosh,Sugato Basu,Raymond J. Mooney	2005	While the vast majority of clustering algorithms are partitional, many real world datasets have inherently overlapping clusters. Several approaches to finding overlapping clusters have come from work on analysis of biological datasets. In this paper, we interpret an overlapping clustering model proposed by Segal et al. [23] as a generalization of Gaussian mixture models, and we extend it to an overlapping clustering model based on mixtures of any regular exponential family distribution and the corresponding Bregman divergence. We provide the necessary algorithm modifications for this extension, and present results on synthetic data as well as subsets of 20-Newsgroups and EachMovie datasets.
KDD	Towards exploratory test instance specific algorithms for high dimensional classification.	Charu C. Aggarwal	2005	In an interactive classification application, a user may find it more valuable to develop a diagnostic decision support method which can reveal significant classification behavior of exemplar records. Such an approach has the additional advantage of being able to optimize the decision process for the individual record in order to design more effective classification methods. In this paper, we propose the Subspace Decision Path method which provides the user with the ability to interactively explore a small number of nodes of a hierarchical decision process so that the most significant classification characteristics for a given test instance are revealed. In addition, the SD-Path method can provide enormous interpretability by constructing views of the data in which the different classes are clearly separated out. Even in cases where the classification behavior of the test instance is ambiguous, the SD-Path method provides a diagnostic understanding of the characteristics which result in this ambiguity. Therefore, this method combines the abilities of the human and the computer in creating an effective diagnostic tool for instance-centered high dimensional classification.
KDD	Integration of profile hidden Markov model output into association rule mining.	Christopher Besemann,Anne Denton	2005	Scientific models typically depend on parameters. Preserving the parameter dependence of models in the pattern mining context opens up several applications. Within association rule mining (ARM), the choice of parameters can be studied with more flexibly then in traditional model building. Studying support, confidence, and other rule metrics as a function of model parameters allows conclusions on assumptions underlying the models. We present efficient techniques to handle multiple model output data sets at little more than the cost of one. We integrate output from hidden Markov models into the association rule mining framework, demonstrating the potential for frequent pattern mining in the field of scientific modeling and experimentation.
KDD	Fast window correlations over uncooperative time series.	Richard Cole,Dennis Shasha,Xiaojian Zhao	2005	"Data arriving in time order (a data stream) arises in fields including physics, finance, medicine, and music, to name a few. Often the data comes from sensors (in physics and medicine for example) whose data rates continue to improve dramatically as sensor technology improves. Further, the number of sensors is increasing, so correlating data between sensors becomes ever more critical in order to distill knowlege from the data. In many applications such as finance, recent correlations are of far more interest than long-term correlation, so correlation over sliding windows (windowed correlation) is the desired operation. Fast response is desirable in many applications (e.g., to aim a telescope at an activity of interest or to perform a stock trade). These three factors -- data size, windowed correlation, and fast response -- motivate this work.Previous work [10, 14] showed how to compute Pearson correlation using Fast Fourier Transforms and Wavelet transforms, but such techniques don't work for time series in which the energy is spread over many frequency components, thus resembling white noise. For such ""uncooperative"" time series, this paper shows how to combine several simple techniques -- sketches (random projections), convolution, structured random vectors, grid structures, and combinatorial design -- to achieve high performance windowed Pearson correlation over a variety of data sets."
KDD	Parallel mining of closed sequential patterns.	Shengnan Cong,Jiawei Han,David A. Padua	2005	Discovery of sequential patterns is an essential data mining task with broad applications. Among several variations of sequential patterns, closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it. Unfortunately, there is no parallel closed sequential pattern mining method proposed yet. In this paper we develop an algorithm, called Par-CSP (Parallel Closed Sequential Pattern mining), to conduct parallel mining of closed sequential patterns on a distributed memory system. Par-CSP partitions the work among the processors by exploiting the divide-and-conquer property so that the overhead of interprocessor communication is minimized. Par-CSP applies dynamic scheduling to avoid processor idling. Moreover, it employs a technique, called selective sampling to address the load imbalance problem. We implement Par-CSP using MPI on a 64-node Linux cluster. Our experimental results show that Par-CSP attains good parallelization efficiencies on various input datasets.
KDD	Scalable discovery of hidden emails from large folders.	Giuseppe Carenini,Raymond T. Ng,Xiaodong Zhou	2005	The popularity of email has triggered researchers to look for ways to help users better organize the enormous amount of information stored in their email folders. One challenge that has not been studied extensively in text mining is the identification and reconstruction of hidden emails. A hidden email is an original email that has been quoted in at least one email in a folder, but does not present itself in the same folder. It may have been (un)intentionally deleted or may never have been received. The discovery and reconstruction of hidden emails is critical for many applications including email classification, summarization and forensics. This paper proposes a framework for reconstructing hidden emails using the embedded quotations found in messages further down the thread hierarchy. We evaluate the robustness and scalability of our framework by using the Enron public email corpus. Our experiments show that hidden emails exist widely in that corpus and also that our optimization techniques are effective in processing large email folders.
KDD	Variable latent semantic indexing.	Anirban Dasgupta,Ravi Kumar,Prabhakar Raghavan,Andrew Tomkins	2005	"Latent Semantic Indexing is a classical method to produce optimal low-rank approximations of a term-document matrix. However, in the context of a particular query distribution, the approximation thus produced need not be optimal. We propose VLSI, a new query-dependent (or ""variable"") low-rank approximation that minimizes approximation error for any specified query distribution. With this tool, it is possible to tailor the LSI technique to particular settings, often resulting in vastly improved approximations at much lower dimensionality. We validate this method via a series of experiments on classical corpora, showing that VLSI typically performs similarly to LSI with an order of magnitude fewer dimensions."
KDD	Making holistic schema matching robust: an ensemble approach.	Bin He,Kevin Chen-Chuan Chang	2005	"The Web has been rapidly ""deepened"" by myriad searchable databases online, where data are hidden behind query interfaces. As an essential task toward integrating these massive ""deep Web"" sources, large scale schema matching (i.e., discovering semantic correspondences of attributes across many query interfaces) has been actively studied recently. In particular, many works have emerged to address this problem by ""holistically"" matching many schemas at the same time and thus pursuing ""mining"" approaches in nature. However, while holistic schema matching has built its promise upon the large quantity of input schemas, it also suffers the robustness problem caused by noisy data quality. Such noises often inevitably arise in the automatic extraction of schema data, which is mandatory in large scale integration. For holistic matching to be viable, it is thus essential to make it robust against noisy schemas. To tackle this challenge, we propose a data-ensemble framework with sampling and voting techniques, which is inspired by bagging predictors. Specifically, our approach creates an ensemble of matchers, by randomizing input schema data into many independently downsampled trials, executing the same matcher on each trial and then aggregating their ranked results by taking majority voting. As a principled basis, we provide analytic justification of the effectiveness of this data-ensemble framework. Further, empirically, our experiments on real Web data show that the ""ensemblization"" indeed significantly boosts the matching accuracy under noisy schema input, and thus maintains the desired robustness of a holistic matcher."
KDD	A general model for clustering binary data.	Tao Li	2005	"Clustering is the problem of identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning the data points into similarity classes. This paper studies the problem of clustering binary data. This is the case for market basket datasets where the transactions contain items and for document datasets where the documents contain ""bag of words"". The contribution of the paper is three-fold. First a general binary data clustering model is presented. The model treats the data and features equally, based on their symmetric association relations, and explicitly describes the data assignments as well as feature assignments. We characterize several variations with different optimization procedures for the general model. Second, we also establish the connections between our clustering model with other existing clustering methods. Third, we also discuss the problem for determining the number of clusters for binary clustering. Experimental results show the effectiveness of the proposed clustering model."
KDD	LIPED: HMM-based life profiles for adaptive event detection.	Chien Chin Chen,Meng Chang Chen,Ming-Syan Chen	2005	In this paper, the proposed LIPED (LIfe Profile based Event Detection) employs the concept of life profiles to predict the activeness of event for effective event detection. A group of events with similar activeness patterns shares a life profile, modeled by a hidden Markov model. Considering the burst-and-diverse property of events, LIPED identifies the activeness status of event. As a result, LIPED balances the clustering precision and recall to achieve better F1 scores than other well known approaches evaluated on the official TDT1 corpus.
KDD	A fast kernel-based multilevel algorithm for graph clustering.	Inderjit S. Dhillon,Yuqiang Guan,Brian Kulis	2005	Graph clustering (also called graph partitioning) --- clustering the nodes of a graph --- is an important problem in diverse data mining applications. Traditional approaches involve optimization of graph clustering objectives such as normalized cut or ratio association; spectral methods are widely used for these objectives, but they require eigenvector computation which can be slow. Recently, graph clustering with a general cut objective has been shown to be mathematically equivalent to an appropriate weighted kernel k-means objective function. In this paper, we exploit this equivalence to develop a very fast multilevel algorithm for graph clustering. Multilevel approaches involve coarsening, initial partitioning and refinement phases, all of which may be specialized to different graph clustering objectives. Unlike existing multilevel clustering approaches, such as METIS, our algorithm does not constrain the cluster sizes to be nearly equal. Our approach gives a theoretical guarantee that the refinement step decreases the graph cut objective under consideration. Experiments show that we achieve better final objective function values as compared to a state-of-the-art spectral clustering algorithm: on a series of benchmark test graphs with up to thirty thousand nodes and one million edges, our algorithm achieves lower normalized cut values in 67% of our experiments and higher ratio association values in 100% of our experiments. Furthermore, on large graphs, our algorithm is significantly faster than spectral methods. Finally, our algorithm requires far less memory than spectral methods; we cluster a 1.2 million node movie network into 5000 clusters, which due to memory requirements cannot be done directly with spectral methods.
KDD	A Bayesian network classifier with inverse tree structure for voxelwise magnetic resonance image analysis.	Rong Chen,Edward Herskovits	2005	We propose a Bayesian-network classifier with inverse-tree structure (BNCIT) for joint classification and variable selection. The problem domain of voxelwise magnetic-resonance image analysis often involves millions of variables but only dozens of samples. Judicious variable selection may render classification tractable, avoid over-fitting, and improve classifier performance. BNCIT embeds the variable-selection process within the classifier-training process, which makes this algorithm scalable. BNCIT is based on a Bayesian-network model with inverse-tree structure, i.e., the class variable C is a leaf node, and predictive variables are parents of C; thus, the classifier-training process returns a parent set for C, which is a subset of the Markov blanket of C. BNCIT uses voxels in the parent set, and voxels that are probabilistically equivalent to them, as variables for classification of new image data. Since the data set has a limited number of samples, we use the jackknife method to determine whether the classifier generated by BNCIT is a statistical artifact. In order to enhance stability and improve classification accuracy, we model the state of the probabilistically equivalent voxels with a latent variable. We employ an efficient method for determining states of hidden variables, thus reducing dramatically the computational cost of model generation. Experimental results confirm the accuracy and efficiency of BNCIT.
KDD	Failure detection and localization in component based systems by online tracking.	Haifeng Chen,Guofei Jiang,Cristian Ungureanu,Kenji Yoshihira	2005	"The increasing complexity of today's systems makes fast and accurate failure detection essential for their use in mission-critical applications. Various monitoring methods provide a large amount of data about system's behavior. Analyzing this data with advanced statistical methods holds the promise of not only detecting the errors faster, but also detecting errors which are difficult to catch with current monitoring tools. Two challenges to building such detection tools are: the high dimensionality of observation data, which makes the models expensive to apply, and frequent system changes, which make the models expensive to update. In this paper, we present algorithms to reduce the dimensionality of data in a way that makes it easy to adapt to system changes. We decompose the observation data into signal and noise subspaces. Two statistics, the Hotelling T2 score and squared prediction error (SPE) are calculated to represent the data characteristics in signal and noise subspaces respectively. Instead of tracking the original data, we use a sequentially discounting expectation maximization (SDEM) algorithm to learn the distribution of the two extracted statistics. A failure event can then be detected based on the abnormal change of the distribution. Applying our technique to component interaction data in a simple e-commerce application shows better accuracy than building independent profiles for each component. Additionally, experiments on synthetic data show that the detection accuracy is high even for changing systems."
KDD	"Web mining from competitors' websites."	Xin Chen,Yi-fang Brook Wu	2005	"This paper presents a framework for user-oriented text mining. It is then illustrated with an example of discovering knowledge from competitors' websites. The knowledge to be discovered is in the form of association rules. A user's background knowledge is represented as a concept hierarchy developed from documents on his/her own website. The concept hierarchy captures the semantic usage of words and relationships among words in background documents. Association rules are identified among the noun phrases extracted from documents on competitors' websites. The interestingness measure, i.e. novelty, which measures the semantic distance between the antecedent and the consequent of a rule in the background knowledge, is computed from the co-occurrence frequency of words and the connection lengths among words in the concept hierarchy. A user evaluation of the novelty of discovered rules demonstrates that the correlation between the algorithm and the human judges is comparable to that between human judges."
KDD	An approach to spacecraft anomaly detection problem using kernel feature space.	Ryohei Fujimaki,Takehisa Yairi,Kazuo Machida	2005	"Development of advanced anomaly detection and failure diagnosis technologies for spacecraft is a quite significant issue in the space industry, because the space environment is harsh, distant and uncertain. While several modern approaches based on qualitative reasoning, expert systems, and probabilistic reasoning have been developed recently for this purpose, any of them has a common difficulty in obtaining accurate and complete a priori knowledge on the space systems from human experts. A reasonable alternative to this conventional anomaly detection method is to reuse a vast amount of telemetry data which is multi-dimensional time-series continuously produced from a number of system components in the spacecraft.This paper proposes a novel ""knowledge-free"" anomaly detection method for spacecraft based on Kernel Feature Space and directional distribution, which constructs a system behavior model from the past normal telemetry data from a set of telemetry data in normal operation and monitors the current system status by checking incoming data with the model.In this method, we regard anomaly phenomena as unexpected changes of causal associations in the spacecraft system, and hypothesize that the significant causal associations inside the system will appear in the form of principal component directions in a high-dimensional non-linear feature space which is constructed by a kernel function and a set of data.We have confirmed the effectiveness of the proposed anomaly detection method by applying it to the telemetry data obtained from a simulator of an orbital transfer vehicle designed to make a rendezvous maneuver with the International Space Station."
KDD	Mining the internet: the eighth wonder of the world.	Gian Fulgoni	2005	"The Internet takes behavioral consumer research to a new level by providing the ability to passively and continuously monitor the complete online behavior of millions of consumers in an opt-in, privacy protected manner. Imagine the analytical possibilities if every site visited, every page viewed, content seen, transaction conducted ..... all of this granularity in behavior --- was continuously captured with explicit consumer permission for millions of consumers and privacy was protected. What unique insights could one gain into consumers' behavior, their interests, passions and lifestyles? What behavior could be predicted? What commercial applications would be possible. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player"
KDD	Rule extraction from linear support vector machines.	Glenn Fung,Sathyakama Sandilya,R. Bharat Rao	2005	"We describe an algorithm for converting linear support vector machines and any other arbitrary hyperplane-based linear classifiers into a set of non-overlapping rules that, unlike the original classifier, can be easily interpreted by humans. Each iteration of the rule extraction algorithm is formulated as a constrained optimization problem that is computationally inexpensive to solve. We discuss various properties of the algorithm and provide proof of convergence for two different optimization criteria We demonstrate the performance and the speed of the algorithm on linear classifiers learned from real-world datasets, including a medical dataset on detection of lung cancer from medical images. The ability to convert SVM's and other ""black-box"" classifiers into a set of human-understandable rules, is critical not only for physician acceptance, but also to reducing the regulatory barrier for medical-decision support systems based on such classifiers."
KDD	Consistent bipartite graph co-partitioning for star-structured high-order heterogeneous data co-clustering.	Bin Gao,Tie-Yan Liu,Xin Zheng,QianSheng Cheng,Wei-Ying Ma	2005	Heterogeneous data co-clustering has attracted more and more attention in recent years due to its high impact on various applications. While the co-clustering algorithms for two types of heterogeneous data (denoted by pair-wise co-clustering), such as documents and terms, have been well studied in the literature, the work on more types of heterogeneous data (denoted by high-order co-clustering) is still very limited. As an attempt in this direction, in this paper, we worked on a specific case of high-order co-clustering in which there is a central type of objects that connects the other types so as to form a star structure of the inter-relationships. Actually, this case could be a very good abstract for many real-world applications, such as the co-clustering of categories, documents and terms in text mining. In our philosophy, we treated such kind of problems as the fusion of multiple pair-wise co-clustering sub-problems with the constraint of the star structure. Accordingly, we proposed the concept of consistent bipartite graph co-partitioning, and developed an algorithm based on semi-definite programming (SDP) for efficient computation of the clustering results. Experiments on toy problems and real data both verified the effectiveness of our proposed method.
KDD	Price prediction and insurance for online auctions.	Rayid Ghani	2005	Online auctions are generating a new class of fine-grained data about online transactions. This data lends itself to a variety of applications and services that can be provided to both buyers and sellers in online marketplaces. We collect data from online auctions and use several classification algorithms to predict the probable-end prices of online auction items. This paper describes the feature extraction and selection process, and several machine learning formulations of the price prediction problem. As a prototype application, we developed Auction Price Insurance that uses the predicted end-price to offer price insurance to sellers in online auctions. We define Price Insurance as a service that offers insurance to auction sellers that guarantees a price for their goods, for an appropriate premium. If the item sells for less than the insured price, the seller is reimbursed for the difference. We show that our price prediction techniques are accurate enough to offer price insurance as a profitable business. While this paper deals specifically with online auctions, we believe that this is an interesting case study that applies to dynamic markets where the price of the goods is variable and is affected by both internal and external factors that change over time.
KDD	Dimension induced clustering.	Aristides Gionis,Alexander Hinneburg,Spiros Papadimitriou,Panayiotis Tsaparas	2005	It is commonly assumed that high-dimensional datasets contain points most of which are located in low-dimensional manifolds. Detection of low-dimensional clusters is an extremely useful task for performing operations such as clustering and classification, however, it is a challenging computational problem. In this paper we study the problem of finding subsets of points with low intrinsic dimensionality. Our main contribution is to extend the definition of fractal correlation dimension, which measures average volume growth rate, in order to estimate the intrinsic dimensionality of the data in local neighborhoods. We provide a careful analysis of several key examples in order to demonstrate the properties of our measure. Based on our proposed measure, we introduce a novel approach to discover clusters with low dimensionality. The resulting algorithms extend previous density based measures, which have been successfully used for clustering. We demonstrate the effectiveness of our algorithms for discovering low-dimensional m-flats embedded in high dimensional spaces, and for detecting low-rank sub-matrices.
KDD	Deriving marketing intelligence from online discussion.	Natalie S. Glance,Matthew Hurst,Kamal Nigam,Matthew Siegler,Robert Stockton,Takashi Tomokiyo	2005	Weblogs and message boards provide online forums for discussion that record the voice of the public. Woven into this mass of discussion is a wide range of opinion and commentary about consumer products. This presents an opportunity for companies to understand and respond to the consumer by analyzing this unsolicited feedback. Given the volume, format and content of the data, the appropriate approach to understand this data is to use large-scale web and text data mining technologies.This paper argues that applications for mining large volumes of textual data for marketing intelligence should provide two key elements: a suite of powerful mining and visualization technologies and an interactive analysis environment which allows for rapid generation and testing of hypotheses. This paper presents such a system that gathers and annotates online discussion relating to consumer products using a wide variety of state-of-the-art techniques, including crawling, wrapping, search, text classification and computational linguistics. Marketing intelligence is derived through an interactive analysis framework uniquely configured to leverage the connectivity and content of annotated online discussion.
KDD	Mining images on semantics via statistical learning.	Jianping Fan,Hangzai Luo,Mohand-Said Hacid	2005	In this paper, we have proposed a novel framework to enable hierarchical image classification via statistical learning. By integrating the concept hierarchy for semantic image concept organization, a hierarchical mixture model is proposed to enable multi-level modeling of semantic image concepts and hierarchical classifier combination. Thus, learning the classifiers for the semantic image concepts at the high level of the concept hierarchy can be effectively achieved by detecting the presences of the relevant base-level atomic image concepts. To effectively learn the base-level classifiers for the atomic image concepts at the first level of the concept hierarchy, we have proposed a novel adaptive EM algorithm to achieve more effective model selection and parameter estimation. In addition, a novel penalty term is proposed to effectively eliminate the misleading effects of the outlying unlabeled images on semi-supervised classifier training. Our experimental results in a specific image domain of outdoor photos are very attractive.
KDD	Mining tree queries in a graph.	Bart Goethals,Eveline Hoekx,Jan Van den Bussche	2005	We present an algorithm for mining tree-shaped patterns in a large graph. Novel about our class of patterns is that they can contain constants, and can contain existential nodes which are not counted when determining the number of occurrences of the pattern in the graph. Our algorithm has a number of provable optimality properties, which are based on the theory of conjunctive database queries. We propose a database-oriented implementation in SQL, and report upon some initial experimental results obtained with our implementation on graph data about food webs, about protein interactions, and about citation analysis.
KDD	Creating social networks to improve peer-to-peer networking.	Andrew Fast,David Jensen,Brian Neil Levine	2005	We use knowledge discovery techniques to guide the creation of efficient overlay networks for peer-to-peer file sharing. An overlay network specifies the logical connections among peers in a network and is distinct from the physical connections of the network. It determines the order in which peers will be queried when a user is searching for a specific file. To better understand the role of the network overlay structure in the performance of peer-to-peer file sharing protocols, we compare several methods for creating overlay networks. We analyze the networks using data from a campus network for peer-to-peer file sharing that recorded anonymized data on 6,528 users sharing 291,925 music files over an 81-day period. We propose a novel protocol for overlay creation based on a model of user preference identified by latent-variable clustering with hierarchical Dirichlet processes (HDPs). Our simulations and empirical studies show that the clusters of songs created by HDPs effectively model user behavior and can be used to create desirable network overlays that outperform alternative approaches.
KDD	Non-redundant clustering with conditional ensembles.	David Gondek,Thomas Hofmann	2005	"Data may often contain multiple plausible clusterings. In order to discover a clustering which is useful to the user, constrained clustering techniques have been proposed to guide the search. Typically, these techniques assume background knowledge in the form of explicit information about the desired clustering. In contrast, we consider the setting in which the background knowledge is instead about an undesired clustering. Such knowledge may be obtained from an existing classification or precedent algorithm. The problem is then to find a novel, ""orthogonal"" clustering in the data. We present a general algorithmic framework which makes use of cluster ensemble methods to solve this problem. One key advantage of this approach is that it takes a base clustering method which is used as a black box, allowing the practitioner to select the most appropriate clustering method for the domain. We present experimental results on synthetic and text data which establish the competitiveness of this framework."
KDD	The predictive power of online chatter.	Daniel Gruhl,Ramanathan V. Guha,Ravi Kumar,Jasmine Novak,Andrew Tomkins	2005	An increasing fraction of the global discourse is migrating online in the form of blogs, bulletin boards, web pages, wikis, editorials, and a dizzying array of new collaborative technologies. The migration has now proceeded to the point that topics reflecting certain individual products are sufficiently popular to allow targeted online tracking of the ebb and flow of chatter around these topics. Based on an analysis of around half a million sales rank values for 2,340 books over a period of four months, and correlating postings in blogs, media, and web pages, we are able to draw several interesting conclusions.First, carefully hand-crafted queries produce matching postings whose volume predicts sales ranks. Second, these queries can be automatically generated in many cases. And third, even though sales rank motion might be difficult to predict in general, algorithmic predictors can use online postings to successfully predict spikes in sales rank.
KDD	Wavelet synopsis for data streams: minimizing non-euclidean error.	Sudipto Guha,Boulos Harb	2005	We consider the wavelet synopsis construction problem for data streams where given n numbers we wish to estimate the data by constructing a synopsis, whose size, say B is much smaller than n. The B numbers are chosen to minimize a suitable error between the original data and the estimate derived from the synopsis.Several good one-pass wavelet construction streaming algorithms minimizing the l2 error exist. For other error measures, the problem is less understood. We provide the first one-pass small space streaming algorithms with provable error guarantees (additive approximation) for minimizing a variety of non-Euclidean error measures including all weighted lp (including l∞) and relative error lp metrics.In several previous works solutions (for weighted l2, l∞ and maximum relative error) where the B synopsis coefficients are restricted to be wavelet coefficients of the data were proposed. This restriction yields suboptimal solutions on even fairly simple examples. Other lines of research, such as probabilistic synopsis, imposed restrictions on how the synopsis was arrived at. To the best of our knowledge this paper is the first paper to address the general problem, without any restriction on how the synopsis is arrived at, as well as provide the first streaming algorithms with guaranteed performance for these classes of error measures.
KDD	Unweaving a web of documents.	Ramanathan V. Guha,Ravi Kumar,D. Sivakumar,Ravi Sundaram	2005	We develop an algorithmic framework to decompose a collection of time-stamped text documents into semantically coherent threads. Our formulation leads to a graph decomposition problem on directed acyclic graphs, for which we obtain three algorithms --- an exact algorithm that is based on minimum cost flow and two more efficient algorithms based on maximum matching and dynamic programming that solve specific versions of the graph decomposition problem. Applications of our algorithms include superior summarization of news search results, improved browsing paradigms for large collections of text-intensive corpora, and integration of time-stamped documents from a variety of sources. Experimental results based on over 250,000 news articles from a major newspaper over a period of four years demonstrate that our algorithms efficiently identify robust threads of varying lengths and time-spans.
KDD	Finding similar files in large document repositories.	George Forman,Kave Eshghi,Stephane Chiocchetti	2005	Hewlett-Packard has many millions of technical support documents in a variety of collections. As part of content management, such collections are periodically merged and groomed. In the process, it becomes important to identify and weed out support documents that are largely duplicates of newer versions. Doing so improves the quality of the collection, eliminates chaff from search results, and improves customer satisfaction.The technical challenge is that through workflow and human processes, the knowledge of which documents are related is often lost. We required a method that could identify similar documents based on their content alone, without relying on metadata, which may be corrupt or missing.We present an approach for finding similar files that scales up to large document repositories. It is based on chunking the byte stream to find unique signatures that may be shared in multiple files. An analysis of the file-chunk graph yields clusters of related files. An optional bipartite graph partitioning algorithm can be applied to greatly increase scalability.
KDD	Maximal boasting.	Cinda Heeren,Leonard Pitt	2005	"We introduce the boasting problem, wherein useful trends in historical ordinal data (rankings) are discovered. Claims of the form ""our object was ranked r or better in x of the last t time units,"" are formalized, and maximal claims (boasts) of this form are defined under two natural partial orders. For the first partial order, we give an efficient and optimal algorithm for finding all such maximal claims. For the second, we apply a classical result from computational geometry to achieve an algorithm whose running time is significantly more efficient than that of a naïve one. Finally, we connect this boasting problem to a novel variation of the problem of finding optimized confidence association rules as originally posed by Fukuda, et al. [2], and give an efficient algorithm for solving a simplification of the new problem."
KDD	Combining email models for false positive reduction.	Shlomo Hershkop,Salvatore J. Stolfo	2005	"Machine learning and data mining can be effectively used to model, classify and discover interesting information for a wide variety of data including email. The Email Mining Toolkit, EMT, has been designed to provide a wide range of analyses for arbitrary email sources. Depending upon the task, one can usually achieve very high accuracy, but with some amount of false positive tradeoff. Generally false positives are prohibitively expensive in the real world. In the case of spam detection, for example, even if one email is misclassified, this may be unacceptable if it is a very important email. Much work has been done to improve specific algorithms for the task of detecting unwanted messages, but less work has been report on leveraging multiple algorithms and correlating models in this particular domain of email analysis.EMT has been updated with new correlation functions allowing the analyst to integrate a number of EMT's user behavior models available in the core technology. We present results of combining classifier outputs for improving both accuracy and reducing false positives for the problem of spam detection. We apply these methods to a very large email data set and show results of different combination methods on these corpora. We introduce a new method to compare multiple and combined classifiers, and show how it differs from past work. The method analyzes the relative gain and maximum possible accuracy that can be achieved for certain combinations of classifiers to automatically choose the best combination."
KDD	Application of kernels to link analysis.	Takahiko Ito,Masashi Shimbo,Taku Kudo,Yuji Matsumoto	2005	"The application of kernel methods to link analysis is explored. In particular, Kandola et al.'s Neumann kernels are shown to subsume not only the co-citation and bibliographic coupling relatedness but also Kleinberg's HITS importance. These popular measures of relatedness and importance correspond to the Neumann kernels at the extremes of their parameter range, and hence these kernels can be interpreted as defining a spectrum of link analysis measures intermediate between co-citation/bibliographic coupling and HITS. We also show that the kernels based on the graph Laplacian, including the regularized Laplacian and diffusion kernels, provide relatedness measures that overcome some limitations of co-citation relatedness. The property of these kernel-based link analysis measures is examined with a network of bibliographic citations. Practical issues in applying these methods to real data are discussed, and possible solutions are proposed."
KDD	Privacy-preserving distributed k-means clustering over arbitrarily partitioned data.	Geetha Jagannathan,Rebecca N. Wright	2005	Advances in computer networking and database technologies have enabled the collection and storage of vast quantities of data. Data mining can extract valuable knowledge from this data, and organizations have realized that they can often obtain better results by pooling their data together. However, the collected data may contain sensitive or private information about the organizations or their customers, and privacy concerns are exacerbated if data is shared between multiple organizations.Distributed data mining is concerned with the computation of models from data that is distributed among multiple participants. Privacy-preserving distributed data mining seeks to allow for the cooperative computation of such models without the cooperating parties revealing any of their individual data items. Our paper makes two contributions in privacy-preserving data mining. First, we introduce the concept of arbitrarily partitioned data, which is a generalization of both horizontally and vertically partitioned data. Second, we provide an efficient privacy-preserving protocol for k-means clustering in the setting of arbitrarily partitioned data.
KDD	Nomograms for visualizing support vector machines.	Aleks Jakulin,Martin Mozina,Janez Demsar,Ivan Bratko,Blaz Zupan	2005	We propose a simple yet potentially very effective way of visualizing trained support vector machines. Nomograms are an established model visualization technique that can graphically encode the complete model on a single page. The dimensionality of the visualization does not depend on the number of attributes, but merely on the properties of the kernel. To represent the effect of each predictive feature on the log odds ratio scale as required for the nomograms, we employ logistic regression to convert the distance from the separating hyperplane into a probability. Case studies on selected data sets show that for a technique thought to be a black-box, nomograms can clearly expose its internal structure. By providing an easy-to-interpret visualization the analysts can gain insight and study the effects of predictive factors.
KDD	Fast discovery of unexpected patterns in data, relative to a Bayesian network.	Szymon Jaroszewicz,Tobias Scheffer	2005	"We consider a model in which background knowledge on a given domain of interest is available in terms of a Bayesian network, in addition to a large database. The mining problem is to discover unexpected patterns: our goal is to find the strongest discrepancies between network and database. This problem is intrinsically difficult because it requires inference in a Bayesian network and processing the entire, potentially very large, database. A sampling-based method that we introduce is efficient and yet provably finds the approximately most interesting unexpected patterns. We give a rigorous proof of the method's correctness. Experiments shed light on its efficiency and practicality for large-scale Bayesian networks and databases."
KDD	Generation of synthetic data sets for evaluating the accuracy of knowledge discovery systems.	Daniel R. Jeske,Behrokh Samadi,Pengyue J. Lin,Lan Ye,Sean Cox,Rui Xiao,Ted Younglove,Minh Ly,Douglas Holt,Ryan Rich	2005	Information Discovery and Analysis Systems (IDAS) are designed to correlate multiple sources of data and use data mining techniques to identify potential significant events. Application domains for IDAS are numerous and include the emerging area of homeland security.Developing test cases for an IDAS requires background data sets into which hypothetical future scenarios can be overlaid. The IDAS can then be measured in terms of false positive and false negative error rates. Obtaining the test data sets can be an obstacle due to both privacy issues and also the time and cost associated with collecting a diverse set of data sources.In this paper, we give an overview of the design and architecture of an IDAS Data Set Generator (IDSG) that enables a fast and comprehensive test of an IDAS. The IDSG generates data using statistical and rule-based algorithms and also semantic graphs that represent interdependencies between attributes. A credit card transaction application is used to illustrate the approach.
KDD	Simultaneous optimization of complex mining tasks with a knowledgeable cache.	Ruoming Jin,Kaushik Sinha,Gagan Agrawal	2005	With an increasing use of data mining tools and techniques, we envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for the following scenarios: 1) Sequence of Queries: A user may analyze one or more datasets by issuing a sequence of related complex mining queries, and 2) Multiple Simultaneous Queries: Several users may be analyzing a set of datasets concurrently, and may issue related complex queries.This paper presents a systematic mechanism to optimize for the above cases, targeting the class of mining queries involving frequent pattern mining on one or multiple datasets. We present a system architecture and propose new algorithms to simultaneously optimize multiple such queries and use a knowledgeable cache to store and utilize the past query results. We have implemented and evaluated our system with both real and synthetic datasets. Our experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries.
KDD	Discovering frequent topological structures from graph datasets.	Ruoming Jin,Chao Wang,Dmitrii Polshakov,Srinivasan Parthasarathy,Gagan Agrawal	2005	The problem of finding frequent patterns from graph-based datasets is an important one that finds applications in drug discovery, protein structure analysis, XML querying, and social network analysis among others. In this paper we propose a framework to mine frequent large-scale structures, formally defined as frequent topological structures, from graph datasets. Key elements of our framework include, fast algorithms for discovering frequent topological patterns based on the well known notion of a topological minor, algorithms for specifying and pushing constraints deep into the mining process for discovering constrained topological patterns, and mechanisms for specifying approximate matches when discovering frequent topological patterns in noisy datasets. We demonstrate the viability and scalability of the proposed algorithms on real and synthetic datasets and also discuss the use of the framework to discover meaningful topological structures from protein structure data.
KDD	A maximum entropy web recommendation system: combining collaborative and content features.	Xin Jin,Yanzan Zhou,Bamshad Mobasher	2005	"Web users display their preferences implicitly by navigating through a sequence of pages or by providing numeric ratings to some items. Web usage mining techniques are used to extract useful knowledge about user interests from such data. The discovered user models are then used for a variety of applications such as personalized recommendations. Web site content or semantic features of objects provide another source of knowledge for deciphering users' needs or interests. We propose a novel Web recommendation system in which collaborative features such as navigation or rating data as well as the content features accessed by the users are seamlessly integrated under the maximum entropy principle. Both the discovered user patterns and the semantic relationships among Web objects are represented as sets of constraints that are integrated to fit the model. In the case of content features, we use a new approach based on Latent Dirichlet Allocation (LDA) to discover the hidden semantic relationships among items and derive constraints used in the model. Experiments on real Web site usage data sets show that this approach can achieve better recommendation accuracy, when compared to systems using only usage information. The integration of semantic information also allows for better interpretation of the generated recommendations."
KDD	Data mining in the chemical industry.	Alex N. Kalos,Tim Rey	2005	In this paper we describe the experience of introducing data mining to a large chemical manufacturing company. The multi-national nature of doing business with multiple business units, presents a unique opportunity for the deployment of data mining. While each business unit has its own objectives and challenges, which may be at odds with those of other units, they also share many common interests and resources. In this environment, data mining can be used to identify potential value-creating opportunities, through large site integration of multiple assets and synergies from the use of common assets, such as site-wide manufacturing facilities, and world-wide supply-chain, purchasing and other shared services. However, issues arise, on one hand from overly complex systems, and on the other hand, from the danger of reaching sub-optimal solutions, if a big enough picture is not considered when executing projects. The company-wide initiative and use of Six Sigma at all levels of the company provided a fertile ground for making the case for data mining and facilitating its acceptance. The Six Sigma mindset of measuring the performance of processes and analyzing data promotes data-based decision making, therefore making data mining a natural extension of this methodology. We will describe the approach for launching a data mining capability within this framework, the strategy for securing upper management support, drawing from internal modeling, statistical, and other communities, and from external consultants and universities. Lessons learned from industrial case studies, enterprise-wide tool evaluation and peer benchmarking will be discussed.
KDD	Information retrieval based on collaborative filtering with latent interest semantic map.	Noriaki Kawamae,Katsumi Takahashi	2005	In this paper, we propose an information retrieval model called Latent Interest Semantic Map (LISM), which features retrieval composed of both Collaborative Filtering(CF) and Probabilistic Latent Semantic Analysis (PLSA). The motivation behind this study is that the relation between users and documents can be explained by the two different latent classes, where users belong probabilistically in one or more classes with the same interest groups, while documents also belong probabilistically in one or more class with the same topic groups. The novel aspect of LISM is that it simultaneously provides a user model and latent semantic analysis in one map. This benefit of LISM is to enable collaborative filtering in terms of user interest and document topic and thus solve the cold start problem.
KDD	Local sparsity control for naive Bayes with extreme misclassification costs.	Aleksander Kolcz	2005	In applications of data mining characterized by highly skewed misclassification costs certain types of errors become virtually unacceptable. This limits the utility of a classifier to a range in which such constraints can be met. Naive Bayes, which has proven to be very useful in text mining applications due to high scalability, can be particularly affected. Although its 0/1 loss tends to be small, its misclassifications are often made with apparently high confidence. Aside from efforts to better calibrate Naive Bayes scores, it has been shown that its accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance. Traditionally, sparsity is controlled globally, and the result for any particular document may vary. In this work we examine the merits of local sparsity control for Naive Bayes in the context of highly asymmetric misclassification costs. In experiments with three benchmark document collections we demonstrate clear advantages of document-level feature selection. In the extreme cost setting, multinomial Naive Bayes with local sparsity control is able to outperform even some of the recently proposed effective improvements to the Naive Bayes classifier. There are also indications that local feature selection may be preferable in different cost settings.
KDD	"Determining an author's native language by mining a text for errors."	Moshe Koppel,Jonathan Schler,Kfir Zigdon	2005	"In this paper, we show that stylistic text features can be exploited to determine an anonymous author's native language with high accuracy. Specifically, we first use automatic tools to ascertain frequencies of various stylistic idiosyncrasies in a text. These frequencies then serve as features for support vector machines that learn to classify texts according to author native language."
KDD	Density-based clustering of uncertain data.	Hans-Peter Kriegel,Martin Pfeifle	2005	In many different application areas, e.g. sensor databases, location based services or face recognition systems, distances between odjects have to be computed based on vague and uncertain data. Commonly, the distances between these uncertain object descriptions are expressed by one numerical distance value. Based on such single-valued distance functions standard data mining algorithms can work without any changes. In this paper, we propose to express the similarity between two fuzzy objects by distance probability functions. These fuzzy distance functions assign a probability value to each possible distance value. By integrating these fuzzy distance functions directly into data mining algorithms, the full information provided by these functions is exploited. In order to demonstrate the benefits of this general approach, we enhance the density-based clustering algorithm DBSCAN so that it can work directly on these fuzzy distance functions. In a detailed experimental evaluation based on artificial and real-world data sets, we show the characteristics and benefits of our new approach.
KDD	A multiple tree algorithm for the efficient association of asteroid observations.	Jeremy Kubica,Andrew W. Moore,Andrew Connolly,Robert Jedicke	2005	In this paper we examine the problem of efficiently finding sets of observations that conform to a given underlying motion model. While this problem is often phrased as a tracking problem, where it is called track initiation, it is useful in a variety of tasks where we want to find correspondences or patterns in spatial-temporal data. Unfortunately, this problem often suffers from a combinatorial explosion in the number of potential sets that must be evaluated. We consider the problem with respect to large-scale asteroid observation data, where the goal is to find associations among the observations that correspond to the same underlying asteroid. In this domain, it is vital that we can efficiently extract the underlying associations.We introduce a new methodology for track initiation that exhaustively considers all possible linkages. We then introduce an exact tree-based algorithm for tractably finding all compatible sets of points. Further, we extend this approach to use multiple trees, exploiting structure from several time steps at once. We compare this approach to a standard sequential approach and show how the use of multiple trees can provide a significant benefit.
KDD	Combining partitions by probabilistic label aggregation.	Tilman Lange,Joachim M. Buhmann	2005	Data clustering represents an important tool in exploratory data analysis. The lack of objective criteria render model selection as well as the identification of robust solutions particularly difficult. The use of a stability assessment and the combination of multiple clustering solutions represents an important ingredient to achieve the goal of finding useful partitions. In this work, we propose a novel way of combining multiple clustering solutions for both, hard and soft partitions: the approach is based on modeling the probability that two objects are grouped together. An efficient EM optimization strategy is employed in order to estimate the model parameters. Our proposal can also be extended in order to emphasize the signal more strongly by weighting individual base clustering solutions according to their consistency with the prediction for previously unseen objects. In addition to that, the probabilistic model supports an out-of-sample extension that (i) makes it possible to assign previously unseen objects to classes of the combined solution and (ii) renders the efficient aggregation of solutions possible. In this work, we also shed some light on the usefulness of such combination approaches. In the experimental result section, we demonstrate the competitive performance of our proposal in comparison with other recently proposed methods for combining multiple classifications of a finite data set.
KDD	Feature bagging for outlier detection.	Aleksandar Lazarevic,Vipin Kumar	2005	Outlier detection has recently become an important problem in many industrial and financial applications. In this paper, a novel feature bagging approach for detecting outliers in very large, high dimensional and noisy databases is proposed. It combines results from multiple outlier detection algorithms that are applied using different set of features. Every outlier detection algorithm uses a small subset of features that are randomly selected from the original feature set. As a result, each outlier detector identifies different outliers, and thus assigns to all data records outlier scores that correspond to their probability of being outliers. The outlier scores computed by the individual outlier detection algorithms are then combined in order to find the better quality outliers. Experiments performed on several synthetic and real life data sets show that the proposed methods for combining outputs from multiple outlier detection algorithms provide non-trivial improvements over the base algorithm.
KDD	Simple and effective visual models for gene expression cancer diagnostics.	Gregor Leban,Minca Mramor,Ivan Bratko,Blaz Zupan	2005	In the paper we show that diagnostic classes in cancer gene expression data sets, which most often include thousands of features (genes), may be effectively separated with simple two-dimensional plots such as scatterplot and radviz graph. The principal innovation proposed in the paper is a method called VizRank, which is able to score and identify the best among possibly millions of candidate projections for visualizations. Compared to recently much applied techniques in the field of cancer genomics that include neural networks, support vector machines and various ensemble-based approaches, VizRank is fast and finds visualization models that can be easily examined and interpreted by domain experts. Our experiments on a number of gene expression data sets show that VizRank was always able to find data visualizations with a small number of (two to seven) genes and excellent class separation. In addition to providing grounds for gene expression cancer diagnosis, VizRank and its visualizations also identify small sets of relevant genes, uncover interesting gene interactions and point to outliers and potential misclassifications in cancer data sets.
KDD	Graphs over time: densification laws, shrinking diameters and possible explanations.	Jure Leskovec,Jon M. Kleinberg,Christos Faloutsos	2005	"How do real graphs evolve over time? What are ""normal"" growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time.Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time, with the number of edges growing super-linearly in the number of nodes. Second, the average distance between nodes often shrinks over time, in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)).Existing graph generation models do not exhibit these types of behavior, even at a qualitative level. We provide a new graph generator, based on a ""forest fire"" spreading process, that has a simple, intuitive justification, requires very few parameters (like the ""flammability"" of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study."
KDD	Mining risk patterns in medical data.	Jiuyong Li,Ada Wai-Chee Fu,Hongxing He,Jie Chen,Huidong Jin,Damien McAullay,Graham J. Williams,Ross Sparks,Chris Kelman	2005	In this paper, we discuss a problem of finding risk patterns in medical data. We define risk patterns by a statistical metric, relative risk, which has been widely used in epidemiological research. We characterise the problem of mining risk patterns as an optimal rule discovery problem. We study an anti-monotone property for mining optimal risk pattern sets and present an algorithm to make use of the property in risk pattern discovery. The method has been applied to a real world data set to find patterns associated with an allergic event for ACE inhibitors. The algorithm has generated some useful results for medical researchers.
KDD	An integrated framework on mining logs files for computing system management.	Tao Li,Feng Liang,Sheng Ma,Wei Peng	2005	Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process that translates domain knowledge into operating rules and policies. This has been well known and experienced as a cumbersome, labor intensive, and error prone process. In addition, this process is difficult to keep up with the rapidly changing environments. In this paper, we will describe our research efforts on establishing an integrated framework for mining system log files for automatic management. In particular, we apply text mining techniques to categorize messages in log files into common situations, improve categorization accuracy by considering the temporal characteristics of log messages, develop temporal mining techniques to discover the relationships between different events, and utilize visualization tools to evaluate and validate the interesting temporal patterns for system management.
KDD	Automated detection of frontal systems from numerical model-generated data.	Xiang Li,Rahul Ramachandran,Sara J. Graves,Sunil Movva,Bilahari Akkiraju,David Emmitt,Steven Greco,Robert Atlas,Joseph Terry,Juan-Carlos Jusem	2005	Fronts are significant meteorological phenomena of interest. The extraction of frontal systems from observations and model data can greatly benefit many kinds of research and applications in atmospheric sciences. Due to the huge amount of observational and model data available nowadays, automated extraction of front systems is necessary. This paper presents an automated method to detect frontal systems from numerical model-generated data. In this method, a frontal system is characterized by a vector of features, comprised of parameters derived from the model wind field. K-means clustering is applied to the generated sample set of the feature vectors to partition the feature space and to identify clusters representing the fronts. The probability that a model grid belongs to a front is estimated based on its feature vector. The probability image is generated corresponding to the model grids. A hierarchical thresholding technique is applied to the probability image to identify the frontal systems and a Gaussian Bayes classifier is trained to determine the proper threshold value. This is followed by post processing to filter out false signatures. Experiment results from this method are in good agreement with the ones identified by the domain experts.
KDD	Co-clustering by block value decomposition.	Bo Long,Zhongfei (Mark) Zhang,Philip S. Yu	2005	Dyadic data matrices, such as co-occurrence matrix, rating matrix, and proximity matrix, arise frequently in various important applications. A fundamental problem in dyadic data analysis is to find the hidden block structure of the data matrix. In this paper, we present a new co-clustering framework, block value decomposition(BVD), for dyadic data, which factorizes the dyadic data matrix into three components, the row-coefficient matrix R, the block value matrix B, and the column-coefficient matrix C. Under this framework, we focus on a special yet very popular case -- non-negative dyadic data, and propose a specific novel co-clustering algorithm that iteratively computes the three decomposition matrices based on the multiplicative updating rules. Extensive experimental evaluations also demonstrate the effectiveness and potential of this framework as well as the specific algorithms for co-clustering, and in particular, for discovering the hidden block structure in the dyadic data.
KDD	Adversarial learning.	Daniel Lowd,Christopher Meek	2005	Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.
KDD	Estimating missed actual positives using independent classifiers.	Sandeep Mane,Jaideep Srivastava,San-Yih Hwang	2005	"Data mining is increasingly being applied in environments having very high rate of data generation like network intrusion detection [7], where routers generate about 300,000 -- 500,000 connections every minute. In such rare class data domains, the cost of missing a rare-class instance is much higher than that of other classes. However, the high cost for manual labeling of instances, the high rate at which data is collected as well as real-time response constraints do not always allow one to determine the actual classes for the collected unlabeled datasets. In our previous work [9], this problem of missed false negatives was explained in context of two different domains -- ""network intrusion detection"" and ""business opportunity classification"". In such cases, an estimate for the number of such missed high-cost, rare instances will aid in the evaluation of the performance of the modeling technique (e.g. classification) used. A capture-recapture method was used for estimating false negatives, using two or more learning methods (i.e. classifiers). This paper focuses on the dependence between the class labels assigned by such learners. We define the conditional independence for classifiers given a class label and show its relation to the conditional independence of the features sets (used by the classifiers) given a class label. The later is a computationally expensive problem and hence, a heuristic algorithm is proposed for obtaining conditionally independent (or less dependent) feature sets for the classifiers. Initial results of this algorithm on synthetic datasets are promising and further research is being pursued."
KDD	Discovering evolutionary theme patterns from text: an exploration of temporal text mining.	Qiaozhu Mei,ChengXiang Zhai	2005	Temporal Text Mining (TTM) is concerned with discovering temporal patterns in text information collected over time. Since most text information bears some time stamps, TTM has many applications in multiple domains, such as summarizing events in news articles and revealing research trends in scientific literature. In this paper, we study a particular TTM task -- discovering and summarizing the evolutionary patterns of themes in a text stream. We define this new text mining problem and present general probabilistic methods for solving this problem through (1) discovering latent themes from text; (2) constructing an evolution graph of themes; and (3) analyzing life cycles of themes. Evaluation of the proposed methods on two different domains (i.e., news articles and literature) shows that the proposed methods can discover interesting evolutionary theme patterns effectively.
KDD	A distributed learning framework for heterogeneous data sources.	Srujana Merugu,Joydeep Ghosh	2005	We present a probabilistic model-based framework for distributed learning that takes into account privacy restrictions and is applicable to scenarios where the different sites have diverse, possibly overlapping subsets of features. Our framework decouples data privacy issues from knowledge integration issues by requiring the individual sites to share only privacy-safe probabilistic models of the local data, which are then integrated to obtain a global probabilistic model based on the union of the features available at all the sites. We provide a mathematical formulation of the model integration problem using the maximum likelihood and maximum entropy principles and describe iterative algorithms that are guaranteed to converge to the optimal solution. For certain commonly occurring special cases involving hierarchically ordered feature sets or conditional independence, we obtain closed form solutions and use these to propose an efficient alternative scheme by recursive decomposition of the model integration problem. To address interpretability concerns, we also present a modified formulation where the global model is assumed to belong to a specified parametric family. Finally, to highlight the generality of our framework, we provide empirical results for various learning tasks such as clustering and classification on different kinds of datasets consisting of continuous vector, categorical and directional attributes. The results show that high quality global models can be obtained without much loss of privacy.
KDD	Efficient computations via scalable sparse kernel partial least squares and boosted latent features.	Michinari Momma	2005	"Kernel partial least squares (KPLS) has been known as a generic kernel regression method and proven to be competitive with other kernel regression methods such as support vector machines for regression (SVM) and kernel ridge regression. Kernel boosted latent features (KBLF) is a variant of KPLS for any differentiable convex loss functions. It provides a more flexible framework for various predictive modeling tasks such as classification with logistic loss and robust regression with L1 norm loss, etc. However, KPLS and KBLF solutions are dense and thus not suitable for large-scale computations. Sparsification of KPLS solutions has been studied for dual and primal forms. For dual sparsity, it requires solving a nonlinear optimization problem at every iteration step and its computational burden limits its applicability to general regression tasks.In this paper, we propose simple heuristics to approximate sparse solutions for KPLS and the framework is also applied for sparsifying KBLF solutions. The algorithm provides an interesting ""path"" from a maximum residual criterion based algorithm with orthogonality conditions to the dense KPLS/KBLF. With the orthogonality, it differentiates itself from many existing forward selection-type algorithms. The computational advantage is illustrated by benchmark datasets and comparison to SVM is done."
KDD	Optimizing time series discretization for knowledge discovery.	Fabian Mörchen,Alfred Ultsch	2005	Knowledge Discovery in time series usually requires symbolic time series. Many discretization methods that convert numeric time series to symbolic time series ignore the temporal order of values. This often leads to symbols that do not correspond to states of the process generating the time series and cannot be interpreted meaningfully. We propose a new method for meaningful unsupervised discretization of numeric time series called Persist. The algorithm is based on the Kullback-Leibler divergence between the marginal and the self-transition probability distributions of the discretization symbols. Its performance is evaluated on both artificial and real life data in comparison to the most common discretization methods. Persist achieves significantly higher accuracy than existing static methods and is robust against noise. It also outperforms Hidden Markov Models for all but very simple cases.
KDD	Key semantics extraction by dependency tree mining.	Satoshi Morinaga,Hiroki Arimura,Takahiro Ikeda,Yosuke Sakao,Susumu Akamine	2005	We propose a new text mining system which extracts characteristic contents from given documents. We define Key semantics as characteristic sub-structures of syntactic dependencies in the given documents, and consider the following three tasks in this paper: 1)Key semantics extraction: extracting characteristic syntactic dependency structures not only as ordered trees but also as unordered trees and free trees, 2)Redundancy reduction: from the result of extraction, deleting redundant dependency structures such as sub-structures or equivalent structures of the others, and 3)Phrase/sentence reconstruction: generating a phrase or sentence in a natural language corresponding to the extracted structure.Our system is a combination of natural language processing techniques and tree mining techniques. The system consists of the following five units: 1) syntactic dependency analysis unit, 2) input filters, 3) characteristic ordered subtree extraction unit, 4) output filters, and 5) phrase/sentence reconstruction unit. Although ordered trees are extracted in the third unit, the overall behavior of the system can be switched into the extraction of ordered trees, unordered trees, or free trees depending on which of the input filters is/are applied in the second step. The output filters delete redundant trees from the extraction result for efficient knowledge discovery. Finally, phrases or sentences corresponding to the extracted subtrees are reconstructed by utilizing the input documents.We demonstrate the validity of our system by showing experimental results using real data collected at a help desk and TDT pilot corpus.
KDD	Using retrieval measures to assess similarity in mining dynamic web clickstreams.	Olfa Nasraoui,Cesar Cardona,Carlos Rojas	2005	While scalable data mining methods are expected to cope with massive Web data, coping with evolving trends in noisy data in a continuous fashion, and without any unnecessary stoppages and reconfigurations is still an open challenge. This dynamic and single pass setting can be cast within the framework of mining evolving data streams. In this paper, we explore the task of mining mass user profiles by discovering evolving Web session clusters in a single pass with a recently proposed scalable immune based clustering approach (TECNO-STREAMS), and study the effect of the choice of different similarity measures on the mining process and on the interpretation of the mined patterns. We propose a simple similarity measure that has the advantage of explicitly coupling the precision and coverage criteria to the early learning stages, and furthermore requiring that the affinity of the data to the learned profiles or summaries be defined by the minimum of their coverage or precision, hence requiring that the learned profiles are simultaneously precise and complete, with no compromises.In our experiments, we study the task of mining evolving user profiles from Web clickstream data (web usage mining) in a single pass, and under different trend sequencing scenarios, showing that compared oto the cosine similarity measure, the proposed similarity measure explicitly based on precision and coverage allows the discovery of more correct profiles at the same precision or recall quality levels.
KDD	Detection of emerging space-time clusters.	Daniel B. Neill,Andrew W. Moore,Maheshkumar Sabhnani,Kenny Daniel	2005	"We propose a new class of spatio-temporal cluster detection methods designed for the rapid detection of emerging space-time clusters. We focus on the motivating application of prospective disease surveillance: detecting space-time clusters of disease cases resulting from an emerging disease outbreak. Automatic, real-time detection of outbreaks can enable rapid epidemiological response, potentially reducing rates of morbidity and mortality. Building on the prior work on spatial and space-time scan statistics, our methods combine time series analysis (to determine how many cases we expect to observe for a given spatial region in a given time interval) with new ""emerging cluster"" space-time scan statistics (to decide whether an observed increase in cases in a region is significant), enabling fast and accurate detection of emerging outbreaks. We evaluate these methods on two types of simulated outbreaks: aerosol release of inhalational anthrax (e.g. from a bioterrorist attack) and FLOO (""Fictional Linear Onset Outbreak""), injected into actual baseline data (Emergency Department records and over-the-counter drug sales data from Allegheny County). We demonstrate that our methods are successful in rapidly detecting both outbreak types while keeping the number of false positives low, and show that our new ""emerging cluster"" scan statistics consistently outperform the standard ""persistent cluster"" scan statistics approach."
KDD	Using relational knowledge discovery to prevent securities fraud.	Jennifer Neville,Özgür Simsek,David Jensen,John Komoroske,Kelly Palmer,Henry G. Goldberg	2005	"We describe an application of relational knowledge discovery to a key regulatory mission of the National Association of Securities Dealers (NASD). NASD is the world's largest private-sector securities regulator, with responsibility for preventing and discovering misconduct among securities brokers. Our goal was to help focus NASD's limited regulatory resources on the brokers who are most likely to engage in securities violations. Using statistical relational learning algorithms, we developed models that rank brokers with respect to the probability that they would commit a serious violation of securities regulations in the near future. Our models incorporate organizational relationships among brokers (e.g., past coworker), which domain experts consider important but have not been easily used before now. The learned models were subjected to an extensive evaluation using more than 18 months of data unseen by the model developers and comprising over two person weeks of effort by NASD staff. Model predictions were found to correlate highly with the subjective evaluations of experienced NASD examiners. Furthermore, in all performance measures, our models performed as well as or better than the handcrafted rules that are currently in use at NASD."
KDD	A hit-miss model for duplicate detection in the WHO drug safety database.	G. Niklas Norén,Roland Orre,Andrew Bate	2005	"The WHO Collaborating Centre for International Drug Monitoring in Uppsala, Sweden, maintains and analyses the world's largest database of reports on suspected adverse drug reaction incidents that occur after drugs are introduced on the market. As in other post-marketing drug safety data sets, the presence of duplicate records is an important data quality problem and the detection of duplicates in the WHO drug safety database remains a formidable challenge, especially since the reports are anonymised before submitted to the database. However, to our knowledge no work has been published on methods for duplicate detection in post-marketing drug safety data. In this paper, we propose a method for probabilistic duplicate detection based on the hit-miss model for statistical record linkage described by Copas & Hilton. We present two new generalisations of the standard hit-miss model: a hit-miss mixture model for errors in numerical record fields and a new method to handle correlated record fields. We demonstrate the effectiveness of the hit-miss model for duplicate detection in the WHO drug safety database both at identifying the most likely duplicate for a given record (94.7% accuracy) and at discriminating duplicates from random matches (63% recall with 71% precision). The proposed method allows for more efficient data cleaning in post-marketing drug safety data sets, and perhaps other applications throughout the KDD community."
KDD	Disease progression modeling from historical clinical databases.	Ronald K. Pearson,Robert J. Kingan,Alan Hochberg	2005	This paper considers the problem of modeling disease progression from historical clinical databases, with the ultimate objective of stratifying patients into groups with clearly distinguishable prognoses or suitability for different treatment strategies. To meet this objective, we describe a procedure that first fits clinical variables measured over time to a disease progression model. The resulting parameter estimates are then used as the basis for a stepwise clustering procedure to stratify patients into groups with distinct survival characteristics. As a practical illustration, we apply this procedure to survival prediction, using a liver transplant database from the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK).
KDD	On mining cross-graph quasi-cliques.	Jian Pei,Daxin Jiang,Aidong Zhang	2005	Joint mining of multiple data sets can often discover interesting, novel, and reliable patterns which cannot be obtained solely from any single source. For example, in cross-market customer segmentation, a group of customers who behave similarly in multiple markets should be considered as a more coherent and more reliable cluster than clusters found in a single market. As another example, in bioinformatics, by joint mining of gene expression data and protein interaction data, we can find clusters of genes which show coherent expression patterns and also produce interacting proteins. Such clusters may be potential pathways.In this paper, we investigate a novel data mining problem, mining cross-graph quasi-cliques, which is generalized from several interesting applications such as cross-market customer segmentation and joint mining of gene expression data and protein interaction data. We build a general model for mining cross-graph quasi-cliques, show why the complete set of cross-graph quasi-cliques cannot be found by previous data mining methods, and study the complexity of the problem. While the problem is difficult, we develop an efficient algorithm, Crochet, which exploits several interesting and effective techniques and heuristics to efficaciously mine cross-graph quasi-cliques. A systematic performance study is reported on both synthetic and real data sets. We demonstrate some interesting and meaningful cross-graph quasi-cliques in bioinformatics. The experimental results also show that algorithm Crochet is efficient and scalable.
KDD	Mining rare and frequent events in multi-camera surveillance video using self-organizing maps.	Valery A. Petrushin	2005	"This paper describes a method for unsupervised classification of events in multi-camera indoors surveillance video. This research is a part of the Multiple Sensor Indoor Surveillance (MSIS) project which uses 32 AXIS-2100 webcams that observe an office environment. The research was inspired by the following practical problem: how automatically classify and visualize a 24 hour long video captured by 32 cameras? Raw data are sequences of JPEG images captured by webcams at the rate 2-6 Hz. The following features are extracted from the image data: foreground pixels' spatial distribution and color histogram. The data are integrated by event by averaging motion and color features and creating a ""summary"" frame which accumulates all foreground pixels of frames of the event into one image. The self-organizing map (SOM) approach is applied to event data for clustering and visualization. One-level and two-level SOM clustering are used. A tool for browsing results allows exploring units of the SOM maps at different levels of hierarchy, clusters of units and distances between units in 3D space. A special technique has been developed to visualize rare events. The results are presented and discussed."
KDD	Improving discriminative sequential learning with rare--but--important associations.	Xuan Hieu Phan,Minh Le Nguyen,Tu Bao Ho,Susumu Horiguchi	2005	"Discriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or information extraction. Their key advantage is the ability to capture various non--independent and overlapping features of inputs. However, several unexpected pitfalls have a negative influence on the model's performance; these mainly come from an imbalance among classes/labels, irregular phenomena, and potential ambiguity in the training data. This paper presents a data--driven approach that can deal with such hard--to--predict data instances by discovering and emphasizing rare--but--important associations of statistics hidden in the training data. Mined associations are then incorporated into these models to deal with difficult examples. Experimental results of English phrase chunking and named entity recognition using CRFs show a significant improvement in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large dataset."
KDD	Robust boosting and its relation to bagging.	Saharon Rosset	2005	"Several authors have suggested viewing boosting as a gradient descent search for a good fit in function space. At each iteration observations are re-weighted using the gradient of the underlying loss function. We present an approach of weight decay for observation weights which is equivalent to ""robustifying"" the underlying loss function. At the extreme end of decay this approach converges to Bagging, which can be viewed as boosting with a linear underlying loss function. We illustrate the practical usefulness of weight decay for improving prediction performance and present an equivalence between one form of weight decay and ""Huberizing"" --- a statistical method for making loss functions more robust."
KDD	Short term performance forecasting in enterprise systems.	Rob Powers,Moisés Goldszmidt,Ira Cohen	2005	"We use data mining and machine learning techniques to predict upcoming periods of high utilization or poor performance in enterprise systems. The abundant data available and complexity of these systems defies human characterization or static models and makes the task suitable for data mining techniques. We formulate the problem as one of classification: given current and past information about the system's behavior, can we forecast whether the system will meet its performance targets over the next hour? Using real data gathered from several enterprise systems in Hewlett-Packard, we compare several approaches ranging from time series to Bayesian networks. Besides establishing the predictive power of these approaches our study analyzes three dimensions that are important for their application as a stand alone tool. First, it quantifies the gain in accuracy of multivariate prediction methods over simple statistical univariate methods. Second, it quantifies the variations in accuracy when using different classes of system and workload features. Third, it establishes that models induced using combined data from various systems generalize well and are applicable to new systems, enabling accurate predictions on systems with insufficient historical data. Together this analysis offers a promising outlook on the development of tools to automate assignment of resources to stabilize performance, (e.g., adding servers to a cluster) and allow opportunistic job scheduling (e.g., backups or virus scans)."
KDD	Query chains: learning to rank from implicit feedback.	Filip Radlinski,Thorsten Joachims	2005	This paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results. We observe that users searching the web often perform a sequence, or chain, of queries with a similar information need. Using query chains, we generate new types of preference judgments from search engine logs, thus taking advantage of user intelligence in reformulating queries. To validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments. We also implemented a real-world search engine to test our approach, using a modified ranking SVM to learn an improved ranking function from preference data. Our results demonstrate significant improvements in the ranking given by the search engine. The learned rankings outperform both a static ranking function, as well as one trained without considering query chains.
KDD	Incentive networks.	Prabhakar Raghavan	2005	"We propose a notion of incentive networks, modeling online settings in which multiple participants in a network help each other find information. Within this general setting, we study query incentive networks, a natural abstraction of question-answering systems with rewards for finding answers. We analyze strategic behavior in such networks and under a simple model of networks, show that the Nash equilibrium for participants' strategies exhibits an unexpected threshold phenomenon."
KDD	On the use of linear programming for unsupervised text classification.	Mark Sandler	2005	We propose a new algorithm for dimensionality reduction and unsupervised text classification. We use mixture models as underlying process of generating corpus and utilize a novel, L1-norm based approach introduced by Kleinberg and Sandler [19]. We show that our algorithm performs extremely well on large datasets, with peak accuracy approaching that of supervised learning based on Support Vector Machines (SVMs) with large training sets. The method is based on the same idea that underlies Latent Semantic Indexing (LSI). We find a good low-dimensional subspace of a feature space and project all documents into it. However our projection minimizes different error, and unlike LSI we build a basis, that in many cases corresponds to the actual topics. We present results of testing of our algorithm on the abstracts of arXiv - an electronic repository of scientific papers, and the 20 Newsgroup dataset - a small snapshot of 20 specific newsgroups.
KDD	A multinomial clustering model for fast simulation of computer architecture designs.	Kaushal Sanghai,Ting Su,Jennifer G. Dy,David R. Kaeli	2005	Computer architects utilize simulation tools to evaluate the merits of a new design feature. The time needed to adequately evaluate the tradeoffs associated with adding any new feature has become a critical issue. Recent work has found that by identifying execution phases present in common workloads used in simulation studies, we can apply clustering algorithms to significantly reduce the amount of time needed to complete the simulation. Our goal in this paper is to demonstrate the value of this approach when applied to the set of industry-standard benchmarks most commonly used in computer architecture studies. We also look to improve upon prior work by applying more appropriate clustering algorithms to identify phases, and to further reduce simulation time.We find that the phase clustering in computer architecture simulation has many similarities to text clustering. In prior work on clustering techniques to reduce simulation time, K-means clustering was used to identify representative program phases. In this paper we apply a mixture of multinomials to the clustering problem and show its advantages over using K-means on simulation data. We have implemented these two clustering algorithms and evaluate how well they can characterize program behavior. By adopting a mixture of multinomials model, we find that we can maintain simulation result fidelity, while greatly reducing overall simulation time. We report results for a range of applications taken from the SPEC2000 benchmark suite.
KDD	Predicting the product purchase patterns of corporate customers.	Bhavani Raskutti,Alan Herschtal	2005	This paper describes TIPPPS (Time Interleaved Product Purchase Prediction System), which analyses billing data of corporate customers in a large telecommunications company in order to predict high value upsell opportunities. The challenges presented by this prediction problem are significant. Firstly, the diversity of products used by corporate telecommunications customers is huge. This, coupled with low product take-up rates, makes this a problem of learning from a very high dimensional feature space with very few minority examples. Further, it is important to give priority specifically to the identification of those new customers who are of high value. These challenges are overcome by introducing a number of modifications to standard data pre-processing and machine learning algorithms, the most important of which are time-interleaving of data and value weighting. Time interleaving is the concatenation of examples from multiple time periods, thus increasing the number of training examples, and hence the number of minority examples. Value weighting assigns importance to minority examples in proportion to the dollar value of take-up, thus biasing the system to identify high value customers. These modifications create a novel algorithm that makes the prediction system practical and usable.Comparison with other techniques designed for similar problems shows that the expected average improvement in ranking accuracy achieved using these modifications is 3.7%. TIPPPS has been in operation for several months and has been successful in identifying many upsell opportunities that were not identified by using the previous manual system.
KDD	Sampling-based sequential subgroup mining.	Martin Scholz	2005	Subgroup discovery is a learning task that aims at finding interesting rules from classified examples. The search is guided by a utility function, trading off the coverage of rules against their statistical unusualness. One shortcoming of existing approaches is that they do not incorporate prior knowledge. To this end a novel generic sampling strategy is proposed. It allows to turn pattern mining into an iterative process. In each iteration the focus of subgroup discovery lies on those patterns that are unexpected with respect to prior knowledge and previously discovered patterns. The result of this technique is a small diverse set of understandable rules that characterise a specified property of interest. As another contribution this article derives a simple connection between subgroup discovery and classifier induction. For a popular utility function this connection allows to apply any standard rule induction algorithm to the task of subgroup discovery after a step of stratified resampling. The proposed techniques are empirically compared to state of the art subgroup discovery algorithms.
KDD	Probabilistic workflow mining.	Ricardo Silva,Jiji Zhang,James G. Shanahan	2005	In several organizations, it has become increasingly popular to document and log the steps that makeup a typical business process. In some situations, a normative workflow model of such processes is developed, and it becomes important to know if such a model is actually being followed by analyzing the available activity logs. In other scenarios, no model is available and, with the purpose of evaluating cases or creating new production policies, one is interested in learning a workflow representation of such activities. In either case, machine learning tools that can mine workflow models are of great interest and still relatively unexplored. We present here a probabilistic workflow model and a corresponding learning algorithm that runs in polynomial time. We illustrate the algorithm on example data derived from a real world workflow.
KDD	Modeling and predicting personal information dissemination behavior.	Xiaodan Song,Ching-Yung Lin,Belle L. Tseng,Ming-Ting Sun	2005	"In this paper, we propose a new way to automatically model and predict human behavior of receiving and disseminating information by analyzing the contact and content of personal communications. A personal profile, called CommunityNet, is established for each individual based on a novel algorithm incorporating contact, content, and time information simultaneously. It can be used for personal social capital management. Clusters of CommunityNets provide a view of informal networks for organization management. Our new algorithm is developed based on the combination of dynamic algorithms in the social network field and the semantic content classification methods in the natural language processing and machine learning literatures. We tested CommunityNets on the Enron Email corpus and report experimental results including filtering, prediction, and recommendation capabilities. We show that the personal behavior and intention are somewhat predictable based on these models. For instance, ""to whom a person is going to send a specific email"" can be predicted by one's personal social network and content analysis. Experimental results show the prediction accuracy of the proposed adaptive algorithm is 58% better than the social network-based predictions, and is 75% better than an aggregated model based on Latent Dirichlet Allocation with social network enhancement. Two online demo systems we developed that allow interactive exploration of CommunityNet are also discussed."
KDD	Evaluating similarity measures: a large-scale study in the orkut social network.	Ellen Spertus,Mehran Sahami,Orkut Buyukkokten	2005	"Online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering, which makes recommendations to users based on their collective past behavior. While many similarity measures have been proposed and individually evaluated, they have not been evaluated relative to each other in a large real-world environment. We present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the Orkut social network. We determine the usefulness of the different recommendations by actually measuring users' propensity to visit and join recommended communities. We also examine how the ordering of recommendations influenced user selection, as well as interesting social issues that arise in recommending communities within a real social network."
KDD	A hybrid unsupervised approach for document clustering.	Mihai Surdeanu,Jordi Turmo,Alicia Ageno	2005	We propose a hybrid, unsupervised document clustering approach that combines a hierarchical clustering algorithm with Expectation Maximization. We developed several heuristics to automatically select a subset of the clusters generated by the first algorithm as the initial points of the second one. Furthermore, our initialization algorithm generates not only an initial model for the iterative refinement algorithm but also an estimate of the model dimension, thus eliminating another important element of human supervision. We have evaluated the proposed system on five real-world document collections. The results show that our approach generates clustering solutions of higher quality than both its individual components.
KDD	Email data cleaning.	Jie Tang,Hang Li,Yunbo Cao,ZhaoHui Tang	2005	"Addressed in this paper is the issue of 'email data cleaning' for text mining. Many text mining applications need take emails as input. Email data is usually noisy and thus it is necessary to clean it before mining. Several products offer email cleaning features, however, the types of noises that can be eliminated are restricted. Despite the importance of the problem, email cleaning has received little attention in the research community. A thorough and systematic investigation on the issue is thus needed. In this paper, email cleaning is formalized as a problem of non-text filtering and text normalization. In this way, email cleaning becomes independent from any specific text mining processing. A cascaded approach is proposed, which cleans up an email in four passes including non-text filtering, paragraph normalization, sentence normalization, and word normalization. As far as we know, non-text filtering and paragraph normalization have not been investigated previously. Methods for performing the tasks on the basis of Support Vector Machines (SVM) have also been proposed in this paper. Features in the models have been defined. Experimental results indicate that the proposed SVM based methods can significantly outperform the baseline methods for email cleaning. The proposed method has been applied to term extraction, a typical text mining processing. Experimental results show that the accuracy of term extraction can be significantly improved by using the data cleaning method."
KDD	Mining comparable bilingual text corpora for cross-language information integration.	Tao Tao,ChengXiang Zhai	2005	Integrating information in multiple natural languages is a challenging task that often requires manually created linguistic resources such as a bilingual dictionary or examples of direct translations of text. In this paper, we propose a general cross-lingual text mining method that does not rely on any of these resources, but can exploit comparable bilingual text corpora to discover mappings between words and documents in different languages. Comparable text corpora are collections of text documents in different languages that are about similar topics; such text corpora are often naturally available (e.g., news articles in different languages published in the same time period). The main idea of our method is to exploit frequency correlations of words in different languages in the comparable corpora and discover mappings between words in different languages. Such mappings can then be used to further discover mappings between documents in different languages, achieving cross-lingual information integration. Evaluation of the proposed method on a 120MB Chinese-English comparable news collection shows that the proposed method is effective for mapping words and documents in English and Chinese. Since our method only relies on naturally available comparable corpora, it is generally applicable to any language pairs as long as we have comparable corpora.
KDD	Regression error characteristic surfaces.	Luís Torgo	2005	This paper presents a generalization of Regression Error Characteristic (REC) curves. REC curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of ROC curves to regression problems. REC curves provide useful information for analyzing the performance of models, particularly when compared to error statistics like for instance the Mean Squared Error. In this paper we present Regression Error Characteristic (REC) surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable, i.e. the joint cumulative distribution function of the errors and the target variable. This provides a more detailed analysis of the performance of models when compared to REC curves. This extra detail is particularly relevant in applications with non-uniform error costs, where it is important to study the performance of models for specific ranges of the target variable. In this paper we present the notion of REC surfaces, describe how to use them to compare the performance of models, and illustrate their use with an important practical class of applications: the prediction of rare extreme values.
KDD	Finding partial orders from unordered 0-1 data.	Antti Ukkonen,Mikael Fortelius,Heikki Mannila	2005	In applications such as paleontology and medical genetics the 0-1 data has an underlying unknown order (the ages of the fossil sites, the locations of markers in the genome). The order might be total or partial: for example, two sites in different parts of the globe might be ecologically incomparable, or the ordering of certain markers might be different in different subgroups of the data. We consider the following problem. Given a table over a set of 0-1 variables, find a partial order for the rows minimizing a score function and being as specific as possible. The score function can be, e.g., the number of changes from 1 to 0 in a column (for paleontology) or the likelihood of the marker sequence (for genomic data). Our solution for this task first constructs small totally ordered fragments of the partial order, then finds good orientations for the fragments, and finally uses a simple and efficient heuristic method for finding a partial order that corresponds well with the collection of fragments. We describe the method, discuss its properties, and give empirical results on paleontological data demonstrating the usefulness of the method. In the application the use of the method highlighted some previously unknown properties of the data and pointed out probable errors in the data.
KDD	Web object indexing using domain knowledge.	Muyuan Wang,Zhiwei Li,Lie Lu,Wei-Ying Ma,Naiyao Zhang	2005	"A web object is defined to represent any meaningful object embedded in web pages (e.g. images, music) or pointed to by hyperlinks (e.g. downloadable files). In many cases, users would like to search for information of a certain 'object', rather than a web page containing the query terms. To facilitate web object searching and organizing, in this paper, we propose a novel approach to web object indexing, by discovering its inherent structure information with existed domain knowledge. In our approach, first, Layered LSI spaces are built for a better representation of the hierarchically structured domain knowledge, in order to emphasize the specific semantics and term space in each layer of the domain knowledge. Meanwhile, the web object representation is constructed by hyperlink analysis, and further pruned to remove the noises. Then an optimal matching between the web object and the domain knowledge is performed, in order to pick out the structure attributes of the web object from the knowledge. Finally, the obtained structure attributes are used to re-organize and index the web objects. Our approach also indicates a new promising way to use trust-worthy Deep Web knowledge to help organize dispersive information of Surface Web."
KDD	Pattern-based similarity search for microarray data.	Haixun Wang,Jian Pei,Philip S. Yu	2005	One fundamental task in near-neighbor search as well as other similarity matching efforts is to find a distance function that can efficiently quantify the similarity between two objects in a meaningful way. In DNA microarray analysis, the expression levels of two closely related genes may rise and fall synchronously in response to a set of experimental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very similar. Unfortunately, none of the conventional distance metrics such as the Lp norm can model this similarity effectively. In this paper, we study the near-neighbor search problem based on this new type of similarity. We propose to measure the distance between two genes by subspace pattern similarity, i.e., whether they exhibit a synchronous pattern of rise and fall on a subset of dimensions. We then present an efficient algorithm for subspace near-neighbor search based on pattern similarity distance, and we perform tests on various data sets to show its effectiveness.
KDD	Formulating distance functions via the kernel trick.	Gang Wu,Edward Y. Chang,Navneet Panda	2005	"Tasks of data mining and information retrieval depend on a good distance function for measuring similarity between data instances. The most effective distance function must be formulated in a context-dependent (also application-, data-, and user-dependent) way. In this paper, we propose to learn a distance function by capturing the nonlinear relationships among contextual information provided by the application, data, or user. We show that through a process called the ""kernel trick,"" such nonlinear relationships can be learned efficiently in a projected space. Theoretically, we substantiate that our method is both sound and optimal. Empirically, using several datasets and applications, we demonstrate that our method is effective and useful."
KDD	CLICKS: an effective algorithm for mining subspace clusters in categorical datasets.	Mohammed Javeed Zaki,Markus Peters,Ira Assent,Thomas Seidl	2005	We present a novel algorithm called Clicks, that finds clusters in categorical datasets based on a search for k-partite maximal cliques. Unlike previous methods, Clicks mines subspace clusters. It uses a selective vertical method to guarantee complete search. Clicks outperforms previous approaches by over an order of magnitude and scales better than any of the existing method for high-dimensional datasets. These results are demonstrated in a comprehensive performance study on real and synthetic datasets.
KDD	Dynamic syslog mining for network failure monitoring.	Kenji Yamanishi,Yuko Maruyama	2005	Syslog monitoring technologies have recently received vast attentions in the areas of network management and network monitoring. They are used to address a wide range of important issues including network failure symptom detection and event correlation discovery. Syslogs are intrinsically dynamic in the sense that they form a time series and that their behavior may change over time. This paper proposes a new methodology of dynamic syslog mining in order to detect failure symptoms with higher confidence and to discover sequential alarm patterns among computer devices. The key ideas of dynamic syslog mining are 1) to represent syslog behavior using a mixture of Hidden Markov Models, 2) to adaptively learn the model using an on-line discounting learning algorithm in combination with dynamic selection of the optimal number of mixture components, and 3) to give anomaly scores using universal test statistics with a dynamically optimized threshold. Using real syslog data we demonstrate the validity of our methodology in the scenarios of failure symptom detection, emerging pattern identification, and correlation discovery.
KDD	Summarizing itemset patterns: a profile-based approach.	Xifeng Yan,Hong Cheng,Jiawei Han,Dong Xin	2005	Frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets, sequences, and graphs. However, the bottleneck of frequent-pattern mining is not at the efficiency but at the interpretability, due to the huge number of patterns generated by the mining process.In this paper, we examine how to summarize a collection of itemset patterns using only K representatives, a small number of patterns that a user can handle easily. The K representatives should not only cover most of the frequent patterns but also approximate their supports. A generative model is built to extract and profile these representatives, under which the supports of the patterns can be easily recovered without consulting the original dataset. Based on the restoration error, we propose a quality measure function to determine the optimal value of parameter K. Polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement. Empirical studies indicate that we can obtain compact summarization in real datasets.
KDD	Enhancing the lift under budget constraints: an application in the mutual fund industry.	Lian Yan,Michael Fassino,Patrick Baldasare	2005	A lift curve, with the true positive rate on the y-axis and the customer pull (or contact) rate on the x-axis, is often used to depict the model performance in many data mining applications, especially in the area of customer relationship management (CRM). Typically, these applications concern only the model accuracy at a relatively small pull or contact/intervention rate of the whole customer base, which is predetermined by a budget constraint for the project, e.g., how many customers can be contacted every month. In this paper, we address the important problem of enhancing the lift (true positive rate) at a specified pull rate. We propose two distinct algorithms, which are applicable to different scenarios. In particular, when the binary class label of the training set is extracted from a continuous variable, we can optimize a training objective which takes into account the specified pull rate rather than the class prior, based on the often ignored continuous variable. In those cases where only the binary class label is available during training, we propose a constrained optimization algorithm to maximize the true positive rate related to a specific decision threshold at which the specified pull rate is achieved. We applied both algorithms to our projects of predicting defection (decline in account value) of mutual fund accounts for two major U.S. mutual fund companies and achieved substantial enhancement of the lift at the specified pull rate.
KDD	A new scheme on privacy-preserving data classification.	Nan Zhang,Shengquan Wang,Wei Zhao	2005	We address privacy-preserving classification problem in a distributed system. Randomization has been the approach proposed to preserve privacy in such scenario. However, this approach is now proven to be insecure as it has been discovered that some privacy intrusion techniques can be used to reconstruct private information from the randomized data tuples. We introduce an algebraic-technique-based scheme. Compared to the randomization approach, our new scheme can build classifiers more accurately but disclose less private information. Furthermore, our new scheme can be readily integrated as a middleware with existing systems.
KDD	Mining closed relational graphs with connectivity constraints.	Xifeng Yan,Xianghong Jasmine Zhou,Jiawei Han	2005	Mining closed relational graphs with connectivity constraints.
KDD	Building connected neighborhood graphs for isometric data embedding.	Li Yang	2005	Neighborhood graph construction is usually the first step in algorithms for isometric data embedding and manifold learning that cope with the problem of projecting high dimensional data to a low space. This paper begins by explaining the algorithmic fundamentals of techniques for isometric data embedding and derives a general classification of these techniques. We will see that the nearest neighbor approaches commonly used to construct neighborhood graphs do not guarantee connectedness of the constructed neighborhood graphs and, consequently, may cause an algorithm fail to project data to a single low dimensional coordinate system. In this paper, we review three existing methods to construct k-edge-connected neighborhood graphs and propose a new method to construct k-connected neighborhood graphs. These methods are applicable to a wide range of data including data distributed among clusters. Their features are discussed and compared through experiments.
KDD	Learning to predict train wheel failures.	Chunsheng Yang,Sylvain Létourneau	2005	This paper describes a successful but challenging application of data mining in the railway industry. The objective is to optimize maintenance and operation of trains through prognostics of wheel failures. In addition to reducing maintenance costs, the proposed technology will help improve railway safety and augment throughput. Building on established techniques from data mining and machine learning, we present a methodology to learn models to predict train wheel failures from readily available operational and maintenance data. This methodology addresses various data mining tasks such as automatic labeling, feature extraction, model building, model fusion, and evaluation. After a detailed description of the methodology, we report results from large-scale experiments. These results clearly show the great potential of this innovative application of data mining in the railway industry.
KDD	A generalized framework for mining spatio-temporal patterns in scientific data.	Hui Yang,Srinivasan Parthasarathy,Sameep Mehta	2005	"In this paper, we present a general framework to discover spatial associations and spatio-temporal episodes for scientific datasets. In contrast to previous work in this area, features are modeled as geometric objects rather than points. We define multiple distance metrics that take into account objects' extent and thus are more robust in capturing the influence of an object on other objects in spatial neighborhood. We have developed algorithms to discover four different types of spatial object interaction (association) patterns. We also extend our approach to accommodate temporal information and propose a simple algorithm to derive spatio-temporal episodes. We show that such episodes can be used to reason about critical events. We evaluate our framework on real datasets to demonstrate its efficacy. The datasets originate from two different areas: Computational Molecular Dynamics and Computational Fluid Flow. We present results highlighting the importance of the identified patterns and episodes by using knowledge from the underlying domains. We also show that the proposed algorithms scale linearly with respect to the dataset size."
KDD	Streaming feature selection using alpha-investing.	Jing Zhou,Dean P. Foster,Robert A. Stine,Lyle H. Ungar	2005	In Streaming Feature Selection (SFS), new features are sequentially considered for addition to a predictive model. When the space of potential features is large, SFS offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model. We describe α-investing, an adaptive complexity penalty method for SFS which dynamically adjusts the threshold on the error reduction required for adding a new feature. α-investing gives false discovery rate-style guarantees against overfitting. It differs from standard penalty methods such as AIC, BIC or RIC, which always drastically over- or under-fit in the limit of infinite numbers of non-predictive features. Empirical results show that SFS is competitive with much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with over a million potential features.
KDD	Combining proactive and reactive predictions for data streams.	Ying Yang,Xindong Wu,Xingquan Zhu	2005	Mining data streams is important in both science and commerce. Two major challenges are (1) the data may grow without limit so that it is difficult to retain a long history; and (2) the underlying concept of the data may change over time. Different from common practice that keeps recent raw data, this paper uses a measure of conceptual equivalence to organize the data history into a history of concepts. Along the journey of concept change, it identifies new concepts as well as re-appearing ones, and learns transition patterns among concepts to help prediction. Different from conventional methodology that passively waits until the concept changes, this paper incorporates proactive and reactive predictions. In a proactive mode, it anticipates what the new concept will be if a future concept change takes place, and prepares prediction strategies in advance. If the anticipation turns out to be correct, a proper prediction model can be launched instantly upon the concept change. If not, it promptly resorts to a reactive mode: adapting a prediction model to the new data. A system RePro is proposed to implement these new ideas. Experiments compare the system with representative existing prediction methods on various benchmark data sets that represent diversified scenarios of concept change. Empirical evidence demonstrates that the proposed methodology is an effective and efficient solution to prediction for data streams.
KDD	Anonymity-preserving data collection.	Zhiqiang Yang,Sheng Zhong,Rebecca N. Wright	2005	"Protection of privacy has become an important problem in data mining. In particular, individuals have become increasingly unwilling to share their data, frequently resulting in individuals either refusing to share their data or providing incorrect data. In turn, such problems in data collection can affect the success of data mining, which relies on sufficient amounts of accurate data in order to produce meaningful results. Random perturbation and randomized response techniques can provide some level of privacy in data collection, but they have an associated cost in accuracy. Cryptographic privacy-preserving data mining methods provide good privacy and accuracy properties. However, in order to be efficient, those solutions must be tailored to specific mining tasks, thereby losing generality.In this paper, we propose efficient cryptographic techniques for online data collection in which data from a large number of respondents is collected anonymously, without the help of a trusted third party. That is, our solution allows the miner to collect the original data from each respondent, but in such a way that the miner cannot link a respondent's data to the respondent. An advantage of such a solution is that, because it does not change the actual data, its success does not depend on the underlying data mining problem. We provide proofs of the correctness and privacy of our solution, as well as experimental data that demonstrates its efficiency. We also extend our solution to tolerate certain kinds of malicious behavior of the participants."
KDD	"Cross-relational clustering with user's guidance."	Xiaoxin Yin,Jiawei Han,Philip S. Yu	2005	"Clustering is an essential data mining task with numerous applications. However, data in most real-life applications are high-dimensional in nature, and the related information often spreads across multiple relations. To ensure effective and efficient high-dimensional, cross-relational clustering, we propose a new approach, called CrossClus, which performs cross-relational clustering with user's guidance. We believe that user's guidance, even likely in very simple forms, could be essential for effective high-dimensional clustering since a user knows well the application requirements and data semantics. CrossClus is carried out as follows: A user specifies a clustering task and selects one or a small set of features pertinent to the task. CrossClus extracts the set of highly relevant features in multiple relations connected via linkages defined in the database schema, evaluates their effectiveness based on user's guidance, and identifies interesting clusters that fit user's needs. This method takes care of both quality in feature extraction and efficiency in clustering. Our comprehensive experiments demonstrate the effectiveness and scalability of this approach."
KDD	SVM selective sampling for ranking with application to data retrieval.	Hwanjo Yu	2005	"Learning ranking (or preference) functions has been a major issue in the machine learning community and has produced many applications in information retrieval. SVMs (Support Vector Machines) - a classification and regression methodology - have also shown excellent performance in learning ranking functions. They effectively learn ranking functions of high generalization based on the ""large-margin"" principle and also systematically support nonlinear ranking by the ""kernel trick"". In this paper, we propose an SVM selective sampling technique for learning ranking functions. SVM selective sampling (or active learning with SVM) has been studied in the context of classification. Such techniques reduce the labeling effort in learning classification functions by selecting only the most informative samples to be labeled. However, they are not extendable to learning ranking functions, as the labeled data in ranking is relative ordering, or partial orders of data. Our proposed sampling technique effectively learns an accurate SVM ranking function with fewer partial orders. We apply our sampling technique to the data retrieval application, which enables fuzzy search on relational databases by interacting with users for learning their preferences. Experimental results show a significant reduction of the labeling effort in inducing accurate ranking functions."
KDD	Pattern lattice traversal by selective jumps.	Osmar R. Zaïane,Mohammad El-Hajj	2005	Regardless of the frequent patterns to discover, either the full frequent patterns or the condensed ones, either closed or maximal, the strategy always includes the traversal of the lattice of candidate patterns. We study the existing depth versus breadth traversal approaches for generating candidate patterns and propose in this paper a new traversal approach that jumps in the search space among only promising nodes. Our leaping approach avoids nodes that would not participate in the answer set and reduce drastically the number of candidate patterns. We use this approach to efficiently pinpoint maximal patterns at the border of the frequent patterns in the lattice and collect enough information in the process to generate all subsequent patterns.
KDD	Reasoning about sets using redescription mining.	Mohammed Javeed Zaki,Naren Ramakrishnan	2005	Redescription mining is a newly introduced data mining problem that seeks to find subsets of data that afford multiple definitions. It can be viewed as a generalization of association rule mining, from finding implications to equivalences; as a form of conceptual clustering, where the goal is to identify clusters that afford dual characterizations; and as a form of constructive induction, to build features based on given descriptors that mutually reinforce each other. In this paper, we present the use of redescription mining as an important tool to reason about a collection of sets, especially their overlaps, similarities, and differences. We outline algorithms to mine all minimal (non-redundant) redescriptions underlying a dataset using notions of minimal generators of closed itemsets. We also show the use of these algorithms in an interactive context, supporting constraint-based exploration and querying. Specifically, we showcase a bioinformatics application that empowers the biologist to define a vocabulary of sets underlying a domain of genes and to reason about these sets, yielding significant biological insight.
KDD	Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Chicago, Illinois, USA, August 21-24, 2005	Robert Grossman,Roberto J. Bayardo,Kristin P. Bennett	2005	Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Chicago, Illinois, USA, August 21-24, 2005
PKDD	A Kernel Based Method for Discovering Market Segments in Beef Meat.	Jorge Díez,Juan José del Coz,Carlos Sañudo,Pere Albertí,Antonio Bahamonde	2005	A Kernel Based Method for Discovering Market Segments in Beef Meat.
PKDD	Cluster Aggregate Inequality and Multi-level Hierarchical Clustering.	Chris H. Q. Ding,Xiaofeng He	2005	Cluster Aggregate Inequality and Multi-level Hierarchical Clustering.
PKDD	Ensembles of Balanced Nested Dichotomies for Multi-class Problems.	Lin Dong,Eibe Frank,Stefan Kramer	2005	Ensembles of Balanced Nested Dichotomies for Multi-class Problems.
PKDD	Tree - Decision Trees for Tree Structured Data.	Björn Bringmann,Albrecht Zimmermann	2005	Tree - Decision Trees for Tree Structured Data.
PKDD	Generating Dynamic Higher-Order Markov Models in Web Usage Mining.	José Borges,Mark Levene	2005	Generating Dynamic Higher-Order Markov Models in Web Usage Mining.
PKDD	Community Mining from Multi-relational Networks.	Deng Cai,Zheng Shao,Xiaofei He,Xifeng Yan,Jiawei Han	2005	Community Mining from Multi-relational Networks.
PKDD	Machine Learning for Natural Language Processing (and Vice Versa?).	Claire Cardie	2005	Machine Learning for Natural Language Processing (and Vice Versa?).
PKDD	Evaluating the Correlation Between Objective Rule Interestingness Measures and Real Human Interest.	Deborah R. Carvalho,Alex Alves Freitas,Nelson F. F. Ebecken	2005	Evaluating the Correlation Between Objective Rule Interestingness Measures and Real Human Interest.
PKDD	-Anonymous Patterns.	Maurizio Atzori,Francesco Bonchi,Fosca Giannotti,Dino Pedreschi	2005	-Anonymous Patterns.
PKDD	Collaborative Filtering on Data Streams.	Jorge M. Barajas,Xue Li	2005	Collaborative Filtering on Data Streams.
PKDD	Data Analysis in the Life Sciences - Sparking Ideas -.	Michael R. Berthold	2005	Data Analysis in the Life Sciences - Sparking Ideas -.
PKDD	Interestingness is Not a Dichotomy: Introducing Softness in Constrained Pattern Mining.	Stefano Bistarelli,Francesco Bonchi	2005	Interestingness is Not a Dichotomy: Introducing Softness in Constrained Pattern Mining.
PKDD	The Relation of Closed Itemset Mining, Complete Pruning Strategies and Item Ordering in Apriori-Based FIM Algorithms.	Ferenc Bodon,Lars Schmidt-Thieme	2005	The Relation of Closed Itemset Mining, Complete Pruning Strategies and Item Ordering in Apriori-Based FIM Algorithms.
PKDD	Agglomerative Hierarchical Clustering with Constraints: Theoretical and Empirical Results.	Ian Davidson,S. S. Ravi	2005	Agglomerative Hierarchical Clustering with Constraints: Theoretical and Empirical Results.
PKDD	An Entropy-Based Approach for Generating Multi-dimensional Sequential Patterns.	Chang-Hwan Lee	2005	An Entropy-Based Approach for Generating Multi-dimensional Sequential Patterns.
PKDD	Protein Sequence Pattern Mining with Constraints.	Pedro Gabriel Ferreira,Paulo J. Azevedo	2005	Protein Sequence Pattern Mining with Constraints.
PKDD	Stress-Testing Hoeffding Trees.	Geoffrey Holmes,Richard Kirkby,Bernhard Pfahringer	2005	Stress-Testing Hoeffding Trees.
PKDD	Rank Measures for Ordering.	Jin Huang,Charles X. Ling	2005	Rank Measures for Ordering.
PKDD	Dynamic Ensemble Re-Construction for Better Ranking.	Jin Huang,Charles X. Ling	2005	Dynamic Ensemble Re-Construction for Better Ranking.
PKDD	Corpus-Based Neural Network Method for Explaining Unknown Words by WordNet Senses.	Bálint Gábor,Viktor Gyenes,András Lörincz	2005	Corpus-Based Neural Network Method for Explaining Unknown Words by WordNet Senses.
PKDD	Frequency-Based Separation of Climate Signals.	Alexander Ilin,Harri Valpola	2005	Frequency-Based Separation of Climate Signals.
PKDD	Segment and Combine Approach for Non-parametric Time-Series Classification.	Pierre Geurts,Louis Wehenkel	2005	Segment and Combine Approach for Non-parametric Time-Series Classification.
PKDD	Efficient Processing of Ranked Queries with Sweeping Selection.	Wen Jin,Martin Ester,Jiawei Han	2005	Efficient Processing of Ranked Queries with Sweeping Selection.
PKDD	Producing Accurate Interpretable Clusters from High-Dimensional Data.	Derek Greene,Padraig Cunningham	2005	Producing Accurate Interpretable Clusters from High-Dimensional Data.
PKDD	Feature Extraction from Mass Spectra for Classification of Pathological States.	Alexandros Kalousis,Julien Prados,Elton Rexhepaj,Melanie Hilario	2005	Feature Extraction from Mass Spectra for Classification of Pathological States.
PKDD	Recent Advances in Mining Time Series Data.	Eamonn J. Keogh	2005	Recent Advances in Mining Time Series Data.
PKDD	Data Streams and Data Synopses for Massive Data Sets.	Yossi Matias	2005	Data Streams and Data Synopses for Massive Data Sets.
PKDD	Word Sense Disambiguation for Exploiting Hierarchical Thesauri in Text Classification.	Dimitrios Mavroeidis,George Tsatsaronis,Michalis Vazirgiannis,Martin Theobald,Gerhard Weikum	2005	Word Sense Disambiguation for Exploiting Hierarchical Thesauri in Text Classification.
PKDD	Numbers in Multi-relational Data Mining.	Arno J. Knobbe,Eric K. Y. Ho	2005	Numbers in Multi-relational Data Mining.
PKDD	Testing Theories in Particle Physics Using Maximum Likelihood and Adaptive Bin Allocation.	Bruce Knuteson,Ricardo Vilalta	2005	Testing Theories in Particle Physics Using Maximum Likelihood and Adaptive Bin Allocation.
PKDD	Mining Model Trees from Spatial Data.	Donato Malerba,Michelangelo Ceci,Annalisa Appice	2005	Mining Model Trees from Spatial Data.
PKDD	An Incremental Algorithm for Mining Generators Representation.	Lijun Xu,Kanglin Xie	2005	An Incremental Algorithm for Mining Generators Representation.
PKDD	Focus the Mining Beacon: Lessons and Challenges from the World of E-Commerce.	Ron Kohavi	2005	Focus the Mining Beacon: Lessons and Challenges from the World of E-Commerce.
PKDD	Improved Naive Bayes for Extremely Skewed Misclassification Costs.	Aleksander Kolcz,Abdur Chowdhury	2005	Improved Naive Bayes for Extremely Skewed Misclassification Costs.
PKDD	Clustering and Prediction of Mobile User Routes from Cellular Data.	Kari Laasonen	2005	Clustering and Prediction of Mobile User Routes from Cellular Data.
PKDD	A Comparison Between Block CEM and Two-Way CEM Algorithms to Cluster a Contingency Table.	Mohamed Nadif,Gérard Govaert	2005	A Comparison Between Block CEM and Two-Way CEM Algorithms to Cluster a Contingency Table.
PKDD	Elastic Partial Matching of Time Series.	Longin Jan Latecki,Vasilis Megalooikonomou,Qiang Wang,Rolf Lakämper,Chotirat (Ann) Ratanamahatana,Eamonn J. Keogh	2005	Elastic Partial Matching of Time Series.
PKDD	An Adaptive Nearest Neighbor Classification Algorithm for Data Streams.	Yan-Nei Law,Carlo Zaniolo	2005	An Adaptive Nearest Neighbor Classification Algorithm for Data Streams.
PKDD	An Imbalanced Data Rule Learner.	Canh Hao Nguyen,Tu Bao Ho	2005	An Imbalanced Data Rule Learner.
PKDD	Support Vector Random Fields for Spatial Classification.	Chi-Hoon Lee,Russell Greiner,Mark Schmidt	2005	Support Vector Random Fields for Spatial Classification.
PKDD	Improvements in the Data Partitioning Approach for Frequent Itemsets Mining.	Son N. Nguyen,Maria E. Orlowska	2005	Improvements in the Data Partitioning Approach for Frequent Itemsets Mining.
PKDD	Realistic, Mathematically Tractable Graph Generation and Evolution, Using Kronecker Multiplication.	Jure Leskovec,Deepayan Chakrabarti,Jon M. Kleinberg,Christos Faloutsos	2005	Realistic, Mathematically Tractable Graph Generation and Evolution, Using Kronecker Multiplication.
PKDD	On-Line Adaptive Filtering of Web Pages.	Richard Nock,Babak Esfandiari	2005	On-Line Adaptive Filtering of Web Pages.
PKDD	A Correspondence Between Maximal Complete Bipartite Subgraphs and Closed Patterns.	Jinyan Li,Haiquan Li,Donny Soh,Limsoon Wong	2005	A Correspondence Between Maximal Complete Bipartite Subgraphs and Closed Patterns.
PKDD	Visual Terrain Analysis of High-Dimensional Datasets.	Wenyuan Li,Kok-Leong Ong,Wee Keong Ng	2005	Visual Terrain Analysis of High-Dimensional Datasets.
PKDD	Improving Generalization by Data Categorization.	Ling Li,Amrit Pratap,Hsuan-Tien Lin,Yaser S. Abu-Mostafa	2005	Improving Generalization by Data Categorization.
PKDD	Mining Paraphrases from Self-anchored Web Sentence Fragments.	Marius Pasca	2005	Mining Paraphrases from Self-anchored Web Sentence Fragments.
PKDD	An Auto-stopped Hierarchical Clustering Algorithm for Analyzing 3D Model Database.	Tian-yang Lv,Yu-hui Xing,Shaobin Huang,Zhengxuan Wang,Wanli Zuo	2005	An Auto-stopped Hierarchical Clustering Algorithm for Analyzing 3D Model Database.
PKDD	MSP: Mining Sequential Patterns Among Several Dimensions.	Marc Plantevit,Yeow Wei Choong,Anne Laurent,Dominique Laurent,Maguelonne Teisseire	2005	MSP: Mining Sequential Patterns Among Several Dimensions.
PKDD	A Bi-clustering Framework for Categorical Data.	Ruggero G. Pensa,Céline Robardet,Jean-François Boulicaut	2005	A Bi-clustering Framework for Categorical Data.
PKDD	Privacy-Preserving Collaborative Filtering on Vertically Partitioned Data.	Huseyin Polat,Wenliang Du	2005	Privacy-Preserving Collaborative Filtering on Vertically Partitioned Data.
PKDD	Speeding Up Logistic Model Tree Induction.	Marc Sumner,Eibe Frank,Mark A. Hall	2005	Speeding Up Logistic Model Tree Induction.
PKDD	Statistical Relational Learning: An Inductive Logic Programming Perspective.	Luc De Raedt	2005	Statistical Relational Learning: An Inductive Logic Programming Perspective.
PKDD	Weka4WS: A WSRF-Enabled Weka Toolkit for Distributed Data Mining on Grids.	Domenico Talia,Paolo Trunfio,Oreste Verta	2005	Weka4WS: A WSRF-Enabled Weka Toolkit for Distributed Data Mining on Grids.
PKDD	Using Inductive Logic Programming for Predicting Protein-Protein Interactions from Multiple Genomic Data.	Tuan Nam Tran,Kenji Satou,Tu Bao Ho	2005	Using Inductive Logic Programming for Predicting Protein-Protein Interactions from Multiple Genomic Data.
PKDD	A Systematic Comparison of Feature-Rich Probabilistic Classifiers for NER Tasks.	Binyamin Rosenfeld,Moshe Fresko,Ronen Feldman	2005	A Systematic Comparison of Feature-Rich Probabilistic Classifiers for NER Tasks.
PKDD	Knowledge Discovery from User Preferences in Conversational Recommendation.	Maria Salamó,James Reilly,Lorraine McGinty,Barry Smyth	2005	Knowledge Discovery from User Preferences in Conversational Recommendation.
PKDD	Indexed Bit Map (IBM) for Mining Frequent Sequences.	Lionel Savary,Karine Zeitouni	2005	Indexed Bit Map (IBM) for Mining Frequent Sequences.
PKDD	Unsupervised Discretization Using Tree-Based Density Estimation.	Gabi Schmidberger,Eibe Frank	2005	Unsupervised Discretization Using Tree-Based Density Estimation.
PKDD	ISOLLE: Locally Linear Embedding with Geodesic Distance.	Claudio Varini,Andreas Degenhard,Tim W. Nattkemper	2005	ISOLLE: Locally Linear Embedding with Geodesic Distance.
PKDD	Weighted Average Pointwise Mutual Information for Feature Selection in Text Categorization.	Karl-Michael Schneider	2005	Weighted Average Pointwise Mutual Information for Feature Selection in Text Categorization.
PKDD	STochFS: A Framework for Combining Feature Selection Outcomes Through a Stochastic Process.	Jerffeson Teixeira de Souza,Nathalie Japkowicz,Stan Matwin	2005	STochFS: A Framework for Combining Feature Selection Outcomes Through a Stochastic Process.
PKDD	Active Sampling for Knowledge Discovery from Biomedical Data.	Sriharsha Veeramachaneni,Francesca Demichelis,Emanuele Olivetti,Paolo Avesani	2005	Active Sampling for Knowledge Discovery from Biomedical Data.
PKDD	Non-stationary Environment Compensation Using Sequential EM Algorithm for Robust Speech Recognition.	Haifeng Shen,Jun Guo,Gang Liu,Qunxia Li	2005	Non-stationary Environment Compensation Using Sequential EM Algorithm for Robust Speech Recognition.
PKDD	A Multi-metric Index for Euclidean and Periodic Matching.	Michail Vlachos,Zografoula Vagena,Vittorio Castelli,Philip S. Yu	2005	A Multi-metric Index for Euclidean and Periodic Matching.
PKDD	Hybrid Cost-Sensitive Decision Tree.	Shengli Sheng,Charles X. Ling	2005	Hybrid Cost-Sensitive Decision Tree.
PKDD	Fast Burst Correlation of Financial Data.	Michail Vlachos,Kun-Lung Wu,Shyh-Kwei Chen,Philip S. Yu	2005	Fast Burst Correlation of Financial Data.
PKDD	A Random Method for Quantifying Changing Distributions in Data Streams.	Haixun Wang,Jian Pei	2005	A Random Method for Quantifying Changing Distributions in Data Streams.
PKDD	Characterization of Novel HIV Drug Resistance Mutations Using Clustering, Multidimensional Scaling and SVM-Based Feature Ranking.	Tobias Sing,Valentina Svicher,Niko Beerenwinkel,Francesca Ceccherini-Silberstein,Martin Däumer,Rolf Kaiser,Hauke Walter,Klaus Korn,Daniel Hoffmann,Mark Oette,Jürgen K. Rockstroh,Gerd Fätkenheuer,Carlo-Federico Perno,Thomas Lengauer	2005	Characterization of Novel HIV Drug Resistance Mutations Using Clustering, Multidimensional Scaling and SVM-Based Feature Ranking.
PKDD	Object Identification with Attribute-Mediated Dependences.	Parag Singla,Pedro Domingos	2005	Object Identification with Attribute-Mediated Dependences.
PKDD	Deriving Class Association Rules Based on Levelwise Subspace Clustering.	Takashi Washio,Koutarou Nakanishi,Hiroshi Motoda	2005	Deriving Class Association Rules Based on Levelwise Subspace Clustering.
PKDD	A Propositional Approach to Textual Case Indexing.	Nirmalie Wiratunga,Robert Lothian,Sutanu Chakraborti,Ivan Koychev	2005	A Propositional Approach to Textual Case Indexing.
PKDD	A Quantitative Comparison of the Subgraph Miners MoFa, gSpan, FFSM, and Gaston.	Marc Wörlein,Thorsten Meinl,Ingrid Fischer,Michael Philippsen	2005	A Quantitative Comparison of the Subgraph Miners MoFa, gSpan, FFSM, and Gaston.
PKDD	Efficient Classification from Multiple Heterogeneous Databases.	Xiaoxin Yin,Jiawei Han	2005	Efficient Classification from Multiple Heterogeneous Databases.
PKDD	A Probabilistic Clustering-Projection Model for Discrete Data.	Shipeng Yu,Kai Yu,Volker Tresp,Hans-Peter Kriegel	2005	A Probabilistic Clustering-Projection Model for Discrete Data.
PKDD	Hybrid Technique for Artificial Neural Network Architecture and Weight Optimization.	Cleber Zanchettin,Teresa Bernarda Ludermir	2005	Hybrid Technique for Artificial Neural Network Architecture and Weight Optimization.
PKDD	Knowledge Discovery in Databases: PKDD 2005, 9th European Conference on Principles and Practice of Knowledge Discovery in Databases, Porto, Portugal, October 3-7, 2005, Proceedings	Alípio Jorge,Luís Torgo,Pavel Brazdil,Rui Camacho,João Gama	2005	Knowledge Discovery in Databases: PKDD 2005, 9th European Conference on Principles and Practice of Knowledge Discovery in Databases, Porto, Portugal, October 3-7, 2005, Proceedings
SDM	Finding Young Stellar Populations in Elliptical Galaxies from Independent Components of Optical Spectra.	Ata Kabán,Louisa Nolan,Somak Raychaudhury	2005	Finding Young Stellar Populations in Elliptical Galaxies from Independent Components of Optical Spectra.
SDM	Symmetric Statistical Translation Models for Automatic Image Annotation.	Feng Kang,Rong Jin	2005	Symmetric Statistical Translation Models for Automatic Image Annotation.
SDM	On Error Correlation and Accuracy of Nearest Neighbor Ensemble Classifiers.	Carlotta Domeniconi,Bojun Yan	2005	On Error Correlation and Accuracy of Nearest Neighbor Ensemble Classifiers.
SDM	On Abnormality Detection in Spuriously Populated Data Streams.	Charu C. Aggarwal	2005	On Abnormality Detection in Spuriously Populated Data Streams.
SDM	Sparse Fisher Discriminant Analysis for Computer Aided Detection.	Mehmet Dundar,Glenn Fung,Jinbo Bi,Sathyakama Sandilya,R. Bharat Rao	2005	Sparse Fisher Discriminant Analysis for Computer Aided Detection.
SDM	Online Analysis of Community Evolution in Data Streams.	Charu C. Aggarwal,Philip S. Yu	2005	Online Analysis of Community Evolution in Data Streams.
SDM	On Variable Constraints in Privacy Preserving Data Mining.	Charu C. Aggarwal,Philip S. Yu	2005	On Variable Constraints in Privacy Preserving Data Mining.
SDM	Making Data Mining Models Useful to Model Non-paying Customers of Exchange Carriers.	Wei Fan,Janek Mathuria,Chang-tien Lu	2005	Making Data Mining Models Useful to Model Non-paying Customers of Exchange Carriers.
SDM	Correlation Clustering for Learning Mixtures of Canonical Correlation Models.	Xiaoli Zhang Fern,Carla E. Brodley,Mark A. Friedl	2005	Correlation Clustering for Learning Mixtures of Canonical Correlation Models.
SDM	Cluster Validity Analysis of Alternative Results from Multi-Objective Optimization.	Reda Alhajj	2005	Cluster Validity Analysis of Alternative Results from Multi-Objective Optimization.
SDM	Expanding the Training Data Space Using Emerging Patterns and Genetic Methods.	Hamad Alhammady,Kotagiri Ramamohanarao	2005	Expanding the Training Data Space Using Emerging Patterns and Genetic Methods.
SDM	Hierarchical Document Classification Using Automatically Generated Hierarchy.	Tao Li	2005	Hierarchical Document Classification Using Automatically Generated Hierarchy.
SDM	On Clustering Binary Data.	Tao Li	2005	On Clustering Binary Data.
SDM	Striking Two Birds With One Stone: Simultaneous Mining of Positive and Negative Spatial Patterns.	Bavani Arunasalam,Sanjay Chawla,Pei Sun	2005	Striking Two Birds With One Stone: Simultaneous Mining of Positive and Negative Spatial Patterns.
SDM	CLSI: A Flexible Approximation Scheme from Clustered Term-Document Matrices.	Efstratios Gallopoulos,Dimitrios Zeimpekis	2005	CLSI: A Flexible Approximation Scheme from Clustered Term-Document Matrices.
SDM	Decision Tree Induction in High Dimensional, Hierarchically Distributed Databases.	Amir Bar-Or,Ran Wolff,Assaf Schuster,Daniel Keren	2005	Decision Tree Induction in High Dimensional, Hierarchically Distributed Databases.
SDM	Mining Non-Derivable Association Rules.	Bart Goethals,Juho Muhonen,Hannu Toivonen	2005	Mining Non-Derivable Association Rules.
SDM	Statictical Models for Unequally Spaced Time Series.	Alina Beygelzimer,Emre Erdogan,Sheng Ma,Irina Rish	2005	Statictical Models for Unequally Spaced Time Series.
SDM	An Algorithm for Well Structured Subspace Clusters.	Haiyun Bian,Raj Bhatnagar	2005	An Algorithm for Well Structured Subspace Clusters.
SDM	Clustering with Model-level Constraints.	David Gondek,Shivakumar Vaithyanathan,Ashutosh Garg	2005	Clustering with Model-level Constraints.
SDM	SPID4.7: Discretization Using Successive Pseudo Deletion at Maximum Information Gain Boundary Points.	Himika Biswas,Somnath Pal	2005	SPID4.7: Discretization Using Successive Pseudo Deletion at Maximum Information Gain Boundary Points.
SDM	Mining Unconnected Patterns in Workflows.	Gianluigi Greco,Antonella Guzzo,Giuseppe Manco,Domenico Saccà	2005	General patterns of execution that have been frequently scheduled by a workflow management system provide the administrator with previously unknown, and potentially useful information, e.g., about the existence of unexpected causalities between subprocesses of a given workflow. This paper investigates the problem of mining unconnected patterns on the basis of some execution traces, i.e., of detecting sets of activities exhibiting no explicit dependency relationships that are frequently executed together. The problem is faced in the paper by proposing and analyzing two algorithms. One algorithm takes into account information about the structure of the control-flow graph only, while the other is a smart refinement where the knowledge of the frequencies of edges and activities in the traces at hand is also accounted for, by means of a sophisticated graphical analysis. Both algorithms have been implemented and integrated into a system prototype, which may profitably support the enactment phase of the workflow. The correctness of the two algorithms is formally proven, and several experiments are reported to evidence the ability of the graphical analysis to significantly improve the performances, by dramatically pruning the search space of candidate patterns.
SDM	Matrix Condition Number Prediction with SVM Regression and Feature Selection.	Shuting Xu,Jun Zhang	2005	Matrix Condition Number Prediction with SVM Regression and Feature Selection.
SDM	A Random Walks Perspective on Maximizing Satisfaction and Profit.	Matthew Brand	2005	A Random Walks Perspective on Maximizing Satisfaction and Profit.
SDM	On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering.	Chris H. Q. Ding,Xiaofeng He	2005	On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering.
SDM	Markov Models for Identification of Significant Episodes.	Robert Gwadera,Mikhail J. Atallah,Wojciech Szpankowski	2005	Markov Models for Identification of Significant Episodes.
SDM	Lazy Learning for Classification Based on Query Projections.	Yiqiu Han,Wai Lam	2005	Lazy Learning for Classification Based on Query Projections.
SDM	Depth-First Non-Derivable Itemset Mining.	Toon Calders,Bart Goethals	2005	Depth-First Non-Derivable Itemset Mining.
SDM	Summarizing Sequential Data with Closed Partial Orders.	Gemma Casas-Garriga	2005	Summarizing Sequential Data with Closed Partial Orders.
SDM	2-Dimensional Singular Value Decomposition for 2D Maps and Images.	Chris H. Q. Ding,Jieping Ye	2005	2-Dimensional Singular Value Decomposition for 2D Maps and Images.
SDM	Hybrid Data Reduction with Fuzzy Rough Set Theory for Classification.	Qinghua Hu	2005	Hybrid Data Reduction with Fuzzy Rough Set Theory for Classification.
SDM	ClosedPROWL: Efficient Mining of Closed Frequent Continuities by Projected Window List Technology.	Kuo-Yu Huang,Chia-Hui Chang,Kuo-Zui Lin	2005	ClosedPROWL: Efficient Mining of Closed Frequent Continuities by Projected Window List Technology.
SDM	Discarding Insignificant Rules during Impact Rule Discovery in Large, Dense Databases.	Shiying Huang,Geoffrey I. Webb	2005	Discarding Insignificant Rules during Impact Rule Discovery in Large, Dense Databases.
SDM	SeqIndex: Indexing Sequences by Sequential Pattern Analysis.	Hong Cheng,Xifeng Yan,Jiawei Han	2005	SeqIndex: Indexing Sequences by Sequential Pattern Analysis.
SDM	Knowledge Discovery from Heterogeneous Dynamic Systems using Change-Point Correlations.	Tsuyoshi Idé,Keisuke Inoue	2005	Knowledge Discovery from Heterogeneous Dynamic Systems using Change-Point Correlations.
SDM	Loadstar: A Load Shedding Scheme for Classifying Data Streams.	Yun Chi,Philip S. Yu,Haixun Wang,Richard R. Muntz	2005	Loadstar: A Load Shedding Scheme for Classifying Data Streams.
SDM	Summarizing and Mining Skewed Data Streams.	Graham Cormode,S. Muthukrishnan	2005	Summarizing and Mining Skewed Data Streams.
SDM	Exploiting Relationships for Domain-Independent Data Cleaning.	Dmitri V. Kalashnikov,Sharad Mehrotra,Zhaoqi Chen	2005	Exploiting Relationships for Domain-Independent Data Cleaning.
SDM	Clustering with Constraints: Feasibility Issues and the k-Means Algorithm.	Ian Davidson,S. S. Ravi	2005	Clustering with Constraints: Feasibility Issues and the k-Means Algorithm.
SDM	Three Myths about Dynamic Time Warping Data Mining.	Chotirat (Ann) Ratanamahatana,Eamonn J. Keogh	2005	Three Myths about Dynamic Time Warping Data Mining.
SDM	Gaussian Processes for Active Data Mining of Spatial Aggregates.	Naren Ramakrishnan,Chris Bailey-Kellogg,Satish Tadepalli,Varun Pandey	2005	Gaussian Processes for Active Data Mining of Spatial Aggregates.
SDM	PCA and kernel PCA using polynomial filtering: a case study on face recognition.	Effrosini Kokiopoulou,Yousef Saad	2005	PCA and kernel PCA using polynomial filtering: a case study on face recognition.
SDM	Time-series Bitmaps: a Practical Visualization Tool for Working with Large Time Series Databases.	Nitin Kumar,Venkata Nishanth Lolla,Eamonn J. Keogh,Stefano Lonardi,Chotirat (Ann) Ratanamahatana	2005	Time-series Bitmaps: a Practical Visualization Tool for Working with Large Time Series Databases.
SDM	Pushing Feature Selection Ahead Of Join.	Rong She,Ke Wang,Yabo Xu,Philip S. Yu	2005	Pushing Feature Selection Ahead Of Join.
SDM	Model-based Clustering With Probabilistic Constraints.	Martin H. C. Law,Alexander P. Topchy,Anil K. Jain	2005	Model-based Clustering With Probabilistic Constraints.
SDM	Variational Learning for Noisy-OR Component Analysis.	Tomás Singliar,Milos Hauskrecht	2005	Variational Learning for Noisy-OR Component Analysis.
SDM	Slope One Predictors for Online Rating-Based Collaborative Filtering.	Daniel Lemire,Anna Maclachlan	2005	Slope One Predictors for Online Rating-Based Collaborative Filtering.
SDM	Iterative Mining for Rules with Constrained Antecedents.	Zheng Sun,Philip S. Yu,Xiang-Yang Li	2005	Iterative Mining for Rules with Constrained Antecedents.
SDM	Mining Frequent Itemsets from Data Streams with a Time-Sensitive Sliding Window.	Chih-Hsiang Lin,Ding-Ying Chiu,Yi-Hung Wu,Arbee L. P. Chen	2005	Mining Frequent Itemsets from Data Streams with a Time-Sensitive Sliding Window.
SDM	Efficient Allocation of Marketing Resources using Dynamic Programming.	Giuliano Tirenni,Abderrahim Labbi,André Elisseeff,Cesar Berrospi	2005	Efficient Allocation of Marketing Resources using Dynamic Programming.
SDM	CBS: A New Classification Method by Using Sequential Patterns.	Vincent Shin-Mu Tseng,Chao-Hui Lee	2005	CBS: A New Classification Method by Using Sequential Patterns.
SDM	Computational Developments of Psi-learning.	Sijin Liu,Xiaotong Shen,Wing Hung Wong	2005	Computational Developments of Psi-learning.
SDM	On Periodicity Detection and Structural Periodic Similarity.	Michail Vlachos,Philip S. Yu,Vittorio Castelli	2005	On Periodicity Detection and Structural Periodic Similarity.
SDM	"Mining Behavior Graphs for ""Backtrace"" of Noncrashing Bugs."	Chao Liu,Xifeng Yan,Hwanjo Yu,Jiawei Han,Philip S. Yu	2005	"Mining Behavior Graphs for ""Backtrace"" of Noncrashing Bugs."
SDM	Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.	Al Mamunur Rashid,George Karypis,John Riedl	2005	Influence in Ratings-Based Recommender Systems: An Algorithm-Independent Approach.
SDM	HARMONY: Efficiently Mining the Best Rules for Classification.	Jianyong Wang,George Karypis	2005	HARMONY: Efficiently Mining the Best Rules for Classification.
SDM	Efficient Mining of Maximal Sequential Patterns Using Multiple Samples.	Congnan Luo,Soon Myoung Chung	2005	Efficient Mining of Maximal Sequential Patterns Using Multiple Samples.
SDM	Near-Neighbor Search in Pattern Distance Spaces.	Haixun Wang,Chang-Shing Perng,Philip S. Yu	2005	Near-Neighbor Search in Pattern Distance Spaces.
SDM	Building Decision Trees on Records Linked through Key References.	Ke Wang,Yabo Xu,Philip S. Yu,Rong She	2005	Building Decision Trees on Records Linked through Key References.
SDM	Dynamic Classification of Defect Structures in Molecular Dynamics Simulation Data.	Sameep Mehta,Steve Barr,Tat-Sang Choy,Hui Yang,Srinivasan Parthasarathy,Raghu Machiraju,John Wilkins	2005	Dynamic Classification of Defect Structures in Molecular Dynamics Simulation Data.
SDM	The Best Nurturers in Computer Science Research.	Bharath Kumar Mohan	2005	The Best Nurturers in Computer Science Research.
SDM	A Spectral Clustering Approach To Finding Communities in Graph.	Scott White,Padhraic Smyth	2005	A Spectral Clustering Approach To Finding Communities in Graph.
SDM	Mining Top-K Itemsets over a Sliding Window Based on Zipfian Distribution.	Raymond Chi-Wing Wong,Ada Wai-Chee Fu	2005	Mining Top-K Itemsets over a Sliding Window Based on Zipfian Distribution.
SDM	Learning to Refine Ontology for a New Web Site Using a Bayesian Approach.	Tak-Lam Wong,Wai Lam	2005	Learning to Refine Ontology for a New Web Site Using a Bayesian Approach.
SDM	Privacy Aware Market Basket Data Set Generation: A Feasible Approach for Inverse Frequent Set Mining.	Xintao Wu,Ying Wu,Yongge Wang,Yingjiu Li	2005	Privacy Aware Market Basket Data Set Generation: A Feasible Approach for Inverse Frequent Set Mining.
SDM	Kronecker Factorization for Speeding up Kernel Machines.	Gang Wu,Zhihua Zhang,Edward Y. Chang	2005	Kronecker Factorization for Speeding up Kernel Machines.
SDM	A Cutting Algorithm for the Minimum Sum-of-Squared Error Clustering.	Yu Xia,Jiming Peng	2005	A Cutting Algorithm for the Minimum Sum-of-Squared Error Clustering.
SDM	Exploiting Parameter Related Domain Knowledge for Learning in Graphical Models.	Radu Stefan Niculescu,Tom M. Mitchell,R. Bharat Rao	2005	Exploiting Parameter Related Domain Knowledge for Learning in Graphical Models.
SDM	Exploiting Geometry for Support Vector Machine Indexing.	Navneet Panda,Edward Y. Chang	2005	Exploiting Geometry for Support Vector Machine Indexing.
SDM	SUMSRM: A New Statistic for the Structural Break Detection in Time Series.	Kwok Pan Pang	2005	SUMSRM: A New Statistic for the Structural Break Detection in Time Series.
SDM	Privacy-Preserving Classification of Customer Data without Loss of Accuracy.	Zhiqiang Yang,Sheng Zhong,Rebecca N. Wright	2005	Privacy-Preserving Classification of Customer Data without Loss of Accuracy.
SDM	Surveying Data for Patchy Structure.	Ronald K. Pearson	2005	Surveying Data for Patchy Structure.
SDM	WFIM: Weighted Frequent Itemset Mining with a weight range and a minimum weight.	Unil Yun,John J. Leggett	2005	WFIM: Weighted Frequent Itemset Mining with a weight range and a minimum weight.
SDM	Cross Table Cubing: Mining Iceberg Cubes from Data Warehouses.	Jian Pei,Moonjung Cho,David Wai-Lok Cheung	2005	Cross Table Cubing: Mining Iceberg Cubes from Data Warehouses.
SDM	Correcting Sampling Bias in Structural Genomics through Iterative Selection of Underrepresented Targets.	Kang Peng,Slobodan Vucetic,Zoran Obradovic	2005	Correcting Sampling Bias in Structural Genomics through Iterative Selection of Underrepresented Targets.
SDM	Parallel Computation of RBF Kernels for Support Vector Classifiers.	Shibin Qiu,Terran Lane	2005	Parallel Computation of RBF Kernels for Support Vector Classifiers.
SDM	Topic-driven Clustering for Document Datasets.	Ying Zhao,George Karypis	2005	Topic-driven Clustering for Document Datasets.
